[10/30 23:32:42] detectron2 INFO: Rank of current process: 0. World size: 2
[10/30 23:35:25] detectron2 INFO: Rank of current process: 0. World size: 2
[10/30 23:37:36] detectron2 INFO: Rank of current process: 0. World size: 2
[10/30 23:38:02] detectron2 INFO: Rank of current process: 0. World size: 2
[10/31 00:06:46] detectron2 INFO: Rank of current process: 0. World size: 2
[10/31 00:10:52] detectron2 INFO: Rank of current process: 0. World size: 2
[10/31 00:23:40] detectron2 INFO: Rank of current process: 0. World size: 2
[10/31 00:55:00] detectron2 INFO: Rank of current process: 0. World size: 2
[10/31 01:02:30] detectron2 INFO: Rank of current process: 0. World size: 2
[10/31 01:02:31] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.3 (default, May 19 2020, 18:47:26) [GCC 7.3.0]
numpy                   1.19.1
detectron2              0.2.1 @/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 5.4
CUDA compiler           not available
detectron2 arch flags   /cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/_C.cpython-38-x86_64-linux-gnu.so; cannot find cuobjdump
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.6.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           True
GPU 0,1                 GeForce GTX 1080 Ti (arch=6.1)
CUDA_HOME               /opt/common/cuda/cuda-10.0.130/lib64/
Pillow                  7.2.0
torchvision             0.7.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision
torchvision arch flags  /cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                  0.1.2.post20201030
cv2                     4.4.0
----------------------  --------------------------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

[10/31 01:02:31] detectron2 INFO: Command line arguments: Namespace(config_file='configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml', dist_url='tcp://127.0.0.1:63010', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=['MODEL.MASK_ON', 'True', 'SOLVER.BASE_LR', '0.015', 'OUTPUT_DIR', 'temp'], resume=False)
[10/31 01:02:31] detectron2 INFO: Contents of args.config_file=configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml:
_BASE_: "../Base-RCNN-FPN.yaml"
MODEL:
  WEIGHTS: "https://download.pytorch.org/models/resnet18-5c106cde.pth"
  MASK_ON: True
  RESNETS:
    DEPTH: 18
    RES2_OUT_CHANNELS: 64
SOLVER:
  BASE_LR: 0.02
  CHECKPOINT_PERIOD: 7330
  # IMS_PER_BATCH: 24
  # STEPS: (40000, 53500)
  # MAX_ITER: 60000
  # WARMUP_FACTOR: 5e-4
  # WARMUP_ITERS: 2000
#OUTPUT_DIR: output/mask_r18_fpn
OUTPUT_DIR: "temp/"

[10/31 01:02:31] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  FILTER_EMPTY_ANNOTATIONS: True
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: ()
  PROPOSAL_FILES_TRAIN: ()
  TEST: ('coco_2017_val',)
  TRAIN: ('coco_2017_train',)
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: False
    SIZE: [0.9, 0.9]
    TYPE: relative_range
  FORMAT: BGR
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES: [[-90, 0, 90]]
    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES: [[32], [64], [128], [256], [512]]
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_fpn_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']
    NORM: 
    OUT_CHANNELS: 256
  KEYPOINT_ON: False
  LOAD_PROPOSALS: False
  MASK_ON: True
  META_ARCHITECTURE: GeneralizedRCNN
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: True
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN: [103.53, 116.28, 123.675]
  PIXEL_STD: [1.0, 1.0, 1.0]
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: False
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE: [False, False, False, False]
    DEPTH: 18
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']
    RES2_OUT_CHANNELS: 64
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.4, 0.5]
    NMS_THRESH_TEST: 0.5
    NORM: 
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))
    IOUS: (0.5, 0.6, 0.7)
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    CLS_AGNOSTIC_BBOX_REG: False
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: FastRCNNConvFCHead
    NORM: 
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    IOU_LABELS: [0, 1]
    IOU_THRESHOLDS: [0.5]
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: True
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: False
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: 
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    BOUNDARY_THRESH: -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.3, 0.7]
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: https://download.pytorch.org/models/resnet18-5c106cde.pth
OUTPUT_DIR: temp
SEED: -1
SOLVER:
  AMP:
    ENABLED: False
  BASE_LR: 0.015
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 7330
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: False
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 16
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 90000
  MOMENTUM: 0.9
  NESTEROV: False
  REFERENCE_WORLD_SIZE: 0
  STEPS: (60000, 80000)
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: False
    FLIP: True
    MAX_SIZE: 4000
    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 0
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: False
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0
[10/31 01:02:31] detectron2 INFO: Full config saved to temp/config.yaml
[10/31 01:02:31] d2.utils.env INFO: Using a generated random seed 31168116
[10/31 01:02:31] d2.engine.defaults INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten()
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[10/31 01:02:52] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 21.18 seconds.
[10/31 01:02:54] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/31 01:03:01] d2.data.build INFO: Removed 1021 images with no usable annotations. 117266 images left.
[10/31 01:03:06] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[10/31 01:03:06] d2.data.common INFO: Serializing 117266 elements to byte tensors and concatenating them all ...
[10/31 01:03:10] d2.data.common INFO: Serialized dataset takes 451.21 MiB
[10/31 01:03:10] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[10/31 01:03:10] d2.data.build INFO: Using training sampler TrainingSampler
[10/31 01:03:13] fvcore.common.checkpoint INFO: Loading checkpoint from https://download.pytorch.org/models/resnet18-5c106cde.pth
[10/31 01:03:24] fvcore.common.file_io INFO: URL https://download.pytorch.org/models/resnet18-5c106cde.pth cached in /cfarhomes/shishira/.torch/fvcore_cache/models/resnet18-5c106cde.pth
[10/31 01:03:24] fvcore.common.checkpoint INFO: Some model parameters or buffers are not found in the checkpoint:
  [34mbackbone.bottom_up.res4.1.conv1.norm.{weight, bias}[0m
  [34mroi_heads.mask_head.mask_fcn1.{bias, weight}[0m
  [34mproposal_generator.anchor_generator.cell_anchors.{2, 1, 4, 0, 3}[0m
  [34mbackbone.bottom_up.res4.1.conv1.weight[0m
  [34mbackbone.bottom_up.res4.0.shortcut.weight[0m
  [34mbackbone.fpn_output4.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn2.{weight, bias}[0m
  [34mroi_heads.mask_head.deconv.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.0.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.fpn_output2.{bias, weight}[0m
  [34mbackbone.fpn_output3.{bias, weight}[0m
  [34mproposal_generator.rpn_head.conv.{bias, weight}[0m
  [34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
  [34mbackbone.fpn_lateral4.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn4.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.1.conv1.weight[0m
  [34mroi_heads.mask_head.predictor.{bias, weight}[0m
  [34mbackbone.fpn_lateral5.{weight, bias}[0m
  [34mbackbone.bottom_up.stem.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.conv2.weight[0m
  [34mroi_heads.box_head.fc1.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.shortcut.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.conv1.weight[0m
  [34mbackbone.bottom_up.res2.0.conv2.weight[0m
  [34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.shortcut.norm.{bias, weight}[0m
  [34mroi_heads.box_head.fc2.{bias, weight}[0m
  [34mroi_heads.box_predictor.bbox_pred.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.fpn_lateral3.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.0.conv1.weight[0m
  [34mbackbone.fpn_lateral2.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.stem.conv1.weight[0m
  [34mbackbone.bottom_up.res5.1.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.conv1.weight[0m
  [34mbackbone.bottom_up.res3.0.conv2.norm.{bias, weight}[0m
  [34mroi_heads.box_predictor.cls_score.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.1.conv2.weight[0m
  [34mbackbone.bottom_up.res3.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res3.1.conv2.weight[0m
  [34mbackbone.fpn_output5.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.shortcut.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.1.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.1.conv2.weight[0m
  [34mbackbone.bottom_up.res5.1.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.1.conv1.weight[0m
  [34mbackbone.bottom_up.res2.1.conv2.weight[0m
  [34mbackbone.bottom_up.res3.1.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.conv2.weight[0m
  [34mbackbone.bottom_up.res2.1.conv1.weight[0m
  [34mroi_heads.mask_head.mask_fcn3.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.conv2.weight[0m
  [34mbackbone.bottom_up.res4.0.conv1.weight[0m
  [34mbackbone.bottom_up.res5.0.shortcut.weight[0m
[10/31 01:03:24] fvcore.common.checkpoint INFO: The checkpoint state_dict contains keys that are not used by the model:
  [35mconv1.weight[0m
  [35mbn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv1.weight[0m
  [35mlayer1.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv2.weight[0m
  [35mlayer1.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv1.weight[0m
  [35mlayer1.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv2.weight[0m
  [35mlayer1.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv1.weight[0m
  [35mlayer2.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv2.weight[0m
  [35mlayer2.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.downsample.0.weight[0m
  [35mlayer2.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv1.weight[0m
  [35mlayer2.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv2.weight[0m
  [35mlayer2.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv1.weight[0m
  [35mlayer3.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv2.weight[0m
  [35mlayer3.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.downsample.0.weight[0m
  [35mlayer3.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv1.weight[0m
  [35mlayer3.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv2.weight[0m
  [35mlayer3.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv1.weight[0m
  [35mlayer4.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv2.weight[0m
  [35mlayer4.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.downsample.0.weight[0m
  [35mlayer4.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv1.weight[0m
  [35mlayer4.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv2.weight[0m
  [35mlayer4.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mfc.{weight, bias}[0m
[10/31 01:03:25] d2.engine.train_loop INFO: Starting training from iteration 0
[10/31 01:03:59] d2.utils.events INFO:  eta: 22:15:09  iter: 19  total_loss: 10.01  loss_cls: 6.884  loss_box_reg: 0.1077  loss_mask: 0.7218  loss_rpn_cls: 0.8046  loss_rpn_loc: 1.468  time: 0.8996  data_time: 0.9113  lr: 0.00029971  max_mem: 6441M
[10/31 01:04:18] d2.utils.events INFO:  eta: 22:17:15  iter: 39  total_loss: 2.101  loss_cls: 0.7321  loss_box_reg: 0.1093  loss_mask: 0.6903  loss_rpn_cls: 0.4088  loss_rpn_loc: 0.1958  time: 0.9194  data_time: 0.1702  lr: 0.00059941  max_mem: 6441M
[10/31 01:04:37] d2.utils.events INFO:  eta: 22:47:47  iter: 59  total_loss: 2.057  loss_cls: 0.607  loss_box_reg: 0.1644  loss_mask: 0.6787  loss_rpn_cls: 0.4088  loss_rpn_loc: 0.1742  time: 0.9339  data_time: 0.1627  lr: 0.00089911  max_mem: 6441M
[10/31 01:04:56] d2.utils.events INFO:  eta: 22:56:59  iter: 79  total_loss: 2.314  loss_cls: 0.7817  loss_box_reg: 0.1894  loss_mask: 0.6756  loss_rpn_cls: 0.4295  loss_rpn_loc: 0.1683  time: 0.9354  data_time: 0.1226  lr: 0.0011988  max_mem: 6441M
[10/31 01:05:15] d2.utils.events INFO:  eta: 23:02:10  iter: 99  total_loss: 2.019  loss_cls: 0.5802  loss_box_reg: 0.1798  loss_mask: 0.6741  loss_rpn_cls: 0.3753  loss_rpn_loc: 0.1464  time: 0.9407  data_time: 0.1351  lr: 0.0014985  max_mem: 6441M
[10/31 01:05:34] d2.utils.events INFO:  eta: 23:16:59  iter: 119  total_loss: 1.901  loss_cls: 0.5382  loss_box_reg: 0.188  loss_mask: 0.6675  loss_rpn_cls: 0.3815  loss_rpn_loc: 0.1478  time: 0.9446  data_time: 0.1290  lr: 0.0017982  max_mem: 6441M
[10/31 01:05:54] d2.utils.events INFO:  eta: 23:19:24  iter: 139  total_loss: 1.82  loss_cls: 0.4744  loss_box_reg: 0.1798  loss_mask: 0.6577  loss_rpn_cls: 0.3749  loss_rpn_loc: 0.1633  time: 0.9470  data_time: 0.1569  lr: 0.0020979  max_mem: 6441M
[10/31 01:06:14] d2.utils.events INFO:  eta: 23:26:02  iter: 159  total_loss: 1.806  loss_cls: 0.4799  loss_box_reg: 0.1969  loss_mask: 0.66  loss_rpn_cls: 0.3371  loss_rpn_loc: 0.1212  time: 0.9532  data_time: 0.1689  lr: 0.0023976  max_mem: 6441M
[10/31 01:06:33] d2.utils.events INFO:  eta: 23:27:23  iter: 179  total_loss: 1.828  loss_cls: 0.4966  loss_box_reg: 0.1969  loss_mask: 0.6639  loss_rpn_cls: 0.3092  loss_rpn_loc: 0.1348  time: 0.9550  data_time: 0.1531  lr: 0.0026973  max_mem: 6441M
[10/31 01:06:43] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 134, in train
    self.run_step()
  File "/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 423, in run_step
    self._trainer.run_step()
  File "/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 228, in run_step
    loss_dict = self.model(data)
  File "/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 511, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 160, in forward
    proposals, proposal_losses = self.proposal_generator(images, features, gt_instances)
  File "/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/modeling/proposal_generator/rpn.py", line 448, in forward
    proposals = self.predict_proposals(
  File "/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/modeling/proposal_generator/rpn.py", line 474, in predict_proposals
    return find_top_rpn_proposals(
  File "/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/modeling/proposal_generator/proposal_utils.py", line 91, in find_top_rpn_proposals
    raise FloatingPointError(
FloatingPointError: Predicted boxes or scores contain Inf/NaN. Training has diverged.
[10/31 01:06:43] d2.engine.hooks INFO: Overall training speed: 188 iterations in 0:02:59 (0.9565 s / it)
[10/31 01:06:43] d2.engine.hooks INFO: Total training time: 0:03:00 (0:00:00 on hooks)
[10/31 01:06:43] d2.utils.events INFO:  eta: 23:27:48  iter: 190  total_loss: 1.924  loss_cls: 0.5629  loss_box_reg: 0.2081  loss_mask: 0.6667  loss_rpn_cls: 0.3394  loss_rpn_loc: 0.1448  time: 0.9535  data_time: 0.1449  lr: 0.0028472  max_mem: 6441M
[10/31 01:15:07] detectron2 INFO: Rank of current process: 0. World size: 2
[10/31 01:15:08] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.3 (default, May 19 2020, 18:47:26) [GCC 7.3.0]
numpy                   1.19.1
detectron2              0.2.1 @/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 5.4
CUDA compiler           not available
detectron2 arch flags   /cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/_C.cpython-38-x86_64-linux-gnu.so; cannot find cuobjdump
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.6.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           True
GPU 0,1                 GeForce GTX 1080 Ti (arch=6.1)
CUDA_HOME               /opt/common/cuda/cuda-10.0.130/lib64/
Pillow                  7.2.0
torchvision             0.7.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision
torchvision arch flags  /cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                  0.1.2.post20201030
cv2                     4.4.0
----------------------  --------------------------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

[10/31 01:15:08] detectron2 INFO: Command line arguments: Namespace(config_file='configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml', dist_url='tcp://127.0.0.1:63010', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=['SOLVER.BASE_LR', '0.02', 'SOLVER.WARMUP_ITERS', '5000', 'SOLVER.WARMUP_FACTOR', '2e-4', 'OUTPUT_DIR', 'temp'], resume=False)
[10/31 01:15:08] detectron2 INFO: Contents of args.config_file=configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml:
_BASE_: "../Base-RCNN-FPN.yaml"
MODEL:
  WEIGHTS: "https://download.pytorch.org/models/resnet18-5c106cde.pth"
  MASK_ON: True
  RESNETS:
    DEPTH: 18
    RES2_OUT_CHANNELS: 64
SOLVER:
  BASE_LR: 0.02
  CHECKPOINT_PERIOD: 7330
  # IMS_PER_BATCH: 24
  # STEPS: (40000, 53500)
  # MAX_ITER: 60000
  # WARMUP_FACTOR: 5e-4
  # WARMUP_ITERS: 2000
#OUTPUT_DIR: output/mask_r18_fpn
OUTPUT_DIR: "temp/"

[10/31 01:15:08] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  FILTER_EMPTY_ANNOTATIONS: True
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: ()
  PROPOSAL_FILES_TRAIN: ()
  TEST: ('coco_2017_val',)
  TRAIN: ('coco_2017_train',)
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: False
    SIZE: [0.9, 0.9]
    TYPE: relative_range
  FORMAT: BGR
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES: [[-90, 0, 90]]
    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES: [[32], [64], [128], [256], [512]]
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_fpn_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']
    NORM: 
    OUT_CHANNELS: 256
  KEYPOINT_ON: False
  LOAD_PROPOSALS: False
  MASK_ON: True
  META_ARCHITECTURE: GeneralizedRCNN
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: True
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN: [103.53, 116.28, 123.675]
  PIXEL_STD: [1.0, 1.0, 1.0]
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: False
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE: [False, False, False, False]
    DEPTH: 18
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']
    RES2_OUT_CHANNELS: 64
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.4, 0.5]
    NMS_THRESH_TEST: 0.5
    NORM: 
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))
    IOUS: (0.5, 0.6, 0.7)
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    CLS_AGNOSTIC_BBOX_REG: False
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: FastRCNNConvFCHead
    NORM: 
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    IOU_LABELS: [0, 1]
    IOU_THRESHOLDS: [0.5]
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: True
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: False
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: 
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    BOUNDARY_THRESH: -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.3, 0.7]
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: https://download.pytorch.org/models/resnet18-5c106cde.pth
OUTPUT_DIR: temp
SEED: -1
SOLVER:
  AMP:
    ENABLED: False
  BASE_LR: 0.02
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 7330
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: False
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 16
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 90000
  MOMENTUM: 0.9
  NESTEROV: False
  REFERENCE_WORLD_SIZE: 0
  STEPS: (60000, 80000)
  WARMUP_FACTOR: 0.0002
  WARMUP_ITERS: 5000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: False
    FLIP: True
    MAX_SIZE: 4000
    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 0
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: False
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0
[10/31 01:15:08] detectron2 INFO: Full config saved to temp/config.yaml
[10/31 01:15:08] d2.utils.env INFO: Using a generated random seed 8900969
[10/31 01:15:09] d2.engine.defaults INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten()
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[10/31 01:15:30] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 21.19 seconds.
[10/31 01:15:31] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/31 01:15:39] d2.data.build INFO: Removed 1021 images with no usable annotations. 117266 images left.
[10/31 01:15:44] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[10/31 01:15:44] d2.data.common INFO: Serializing 117266 elements to byte tensors and concatenating them all ...
[10/31 01:15:49] d2.data.common INFO: Serialized dataset takes 451.21 MiB
[10/31 01:15:49] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[10/31 01:15:49] d2.data.build INFO: Using training sampler TrainingSampler
[10/31 01:15:52] fvcore.common.checkpoint INFO: Loading checkpoint from https://download.pytorch.org/models/resnet18-5c106cde.pth
[10/31 01:15:52] fvcore.common.file_io INFO: URL https://download.pytorch.org/models/resnet18-5c106cde.pth cached in /cfarhomes/shishira/.torch/fvcore_cache/models/resnet18-5c106cde.pth
[10/31 01:15:52] fvcore.common.checkpoint INFO: Some model parameters or buffers are not found in the checkpoint:
  [34mbackbone.bottom_up.res5.0.shortcut.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.1.conv2.norm.{weight, bias}[0m
  [34mbackbone.fpn_lateral2.{weight, bias}[0m
  [34mproposal_generator.rpn_head.conv.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.conv2.weight[0m
  [34mbackbone.bottom_up.res2.1.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.shortcut.weight[0m
  [34mproposal_generator.rpn_head.objectness_logits.{weight, bias}[0m
  [34mroi_heads.box_predictor.cls_score.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn1.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.1.conv1.weight[0m
  [34mbackbone.bottom_up.res4.1.conv2.weight[0m
  [34mbackbone.bottom_up.res2.1.conv2.weight[0m
  [34mroi_heads.mask_head.mask_fcn3.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.conv1.weight[0m
  [34mbackbone.bottom_up.res4.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.1.conv1.norm.{bias, weight}[0m
  [34mbackbone.fpn_output4.{bias, weight}[0m
  [34mproposal_generator.anchor_generator.cell_anchors.{1, 4, 3, 2, 0}[0m
  [34mbackbone.bottom_up.res5.0.conv1.weight[0m
  [34mbackbone.bottom_up.res3.0.conv1.norm.{bias, weight}[0m
  [34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
  [34mbackbone.fpn_output3.{bias, weight}[0m
  [34mroi_heads.mask_head.predictor.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv1.weight[0m
  [34mroi_heads.box_head.fc2.{bias, weight}[0m
  [34mbackbone.fpn_lateral4.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.1.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv2.norm.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn2.{bias, weight}[0m
  [34mroi_heads.mask_head.deconv.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.1.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.shortcut.norm.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn4.{bias, weight}[0m
  [34mroi_heads.box_head.fc1.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.1.conv2.weight[0m
  [34mbackbone.fpn_lateral3.{weight, bias}[0m
  [34mbackbone.fpn_output5.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.shortcut.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.1.conv1.weight[0m
  [34mbackbone.fpn_output2.{bias, weight}[0m
  [34mroi_heads.box_predictor.bbox_pred.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv2.weight[0m
  [34mbackbone.fpn_lateral5.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res4.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.1.conv1.weight[0m
  [34mbackbone.bottom_up.res5.0.conv2.weight[0m
  [34mbackbone.bottom_up.stem.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.1.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res5.1.conv2.weight[0m
  [34mbackbone.bottom_up.res3.0.conv2.weight[0m
  [34mbackbone.bottom_up.res2.1.conv1.weight[0m
  [34mbackbone.bottom_up.res5.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.conv1.weight[0m
  [34mbackbone.bottom_up.stem.conv1.weight[0m
[10/31 01:15:52] fvcore.common.checkpoint INFO: The checkpoint state_dict contains keys that are not used by the model:
  [35mconv1.weight[0m
  [35mbn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv1.weight[0m
  [35mlayer1.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv2.weight[0m
  [35mlayer1.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv1.weight[0m
  [35mlayer1.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv2.weight[0m
  [35mlayer1.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv1.weight[0m
  [35mlayer2.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv2.weight[0m
  [35mlayer2.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.downsample.0.weight[0m
  [35mlayer2.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv1.weight[0m
  [35mlayer2.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv2.weight[0m
  [35mlayer2.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv1.weight[0m
  [35mlayer3.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv2.weight[0m
  [35mlayer3.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.downsample.0.weight[0m
  [35mlayer3.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv1.weight[0m
  [35mlayer3.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv2.weight[0m
  [35mlayer3.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv1.weight[0m
  [35mlayer4.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv2.weight[0m
  [35mlayer4.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.downsample.0.weight[0m
  [35mlayer4.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv1.weight[0m
  [35mlayer4.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv2.weight[0m
  [35mlayer4.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mfc.{weight, bias}[0m
[10/31 01:15:53] d2.engine.train_loop INFO: Starting training from iteration 0
[10/31 01:16:29] d2.utils.events INFO:  eta: 1 day, 1:29:35  iter: 19  total_loss: 11.5  loss_cls: 6.472  loss_box_reg: 0.1035  loss_mask: 0.7414  loss_rpn_cls: 0.9463  loss_rpn_loc: 2.048  time: 1.0223  data_time: 0.8966  lr: 7.9985e-05  max_mem: 6433M
[10/31 01:16:48] d2.utils.events INFO:  eta: 23:58:04  iter: 39  total_loss: 3.293  loss_cls: 1.563  loss_box_reg: 0.1091  loss_mask: 0.6778  loss_rpn_cls: 0.4759  loss_rpn_loc: 0.476  time: 0.9992  data_time: 0.2115  lr: 0.00015997  max_mem: 6433M
[10/31 01:17:07] d2.utils.events INFO:  eta: 23:13:37  iter: 59  total_loss: 2.546  loss_cls: 0.8916  loss_box_reg: 0.1431  loss_mask: 0.6718  loss_rpn_cls: 0.4552  loss_rpn_loc: 0.2701  time: 0.9680  data_time: 0.1598  lr: 0.00023995  max_mem: 6433M
[10/31 01:17:26] d2.utils.events INFO:  eta: 23:20:53  iter: 79  total_loss: 2.034  loss_cls: 0.5362  loss_box_reg: 0.1792  loss_mask: 0.6763  loss_rpn_cls: 0.4236  loss_rpn_loc: 0.2268  time: 0.9694  data_time: 0.1989  lr: 0.00031994  max_mem: 6433M
[10/31 01:17:46] d2.utils.events INFO:  eta: 23:32:00  iter: 99  total_loss: 1.938  loss_cls: 0.4967  loss_box_reg: 0.1862  loss_mask: 0.6733  loss_rpn_cls: 0.3948  loss_rpn_loc: 0.161  time: 0.9766  data_time: 0.1669  lr: 0.00039992  max_mem: 6433M
[10/31 01:18:06] d2.utils.events INFO:  eta: 23:43:24  iter: 119  total_loss: 1.914  loss_cls: 0.5006  loss_box_reg: 0.195  loss_mask: 0.6674  loss_rpn_cls: 0.3828  loss_rpn_loc: 0.1642  time: 0.9828  data_time: 0.2298  lr: 0.0004799  max_mem: 6433M
[10/31 01:18:27] d2.utils.events INFO:  eta: 23:44:35  iter: 139  total_loss: 1.818  loss_cls: 0.4782  loss_box_reg: 0.1909  loss_mask: 0.6682  loss_rpn_cls: 0.366  loss_rpn_loc: 0.1471  time: 0.9877  data_time: 0.2377  lr: 0.00055989  max_mem: 6434M
[10/31 01:18:46] d2.utils.events INFO:  eta: 23:42:43  iter: 159  total_loss: 1.876  loss_cls: 0.4855  loss_box_reg: 0.1954  loss_mask: 0.6653  loss_rpn_cls: 0.3835  loss_rpn_loc: 0.1719  time: 0.9842  data_time: 0.1630  lr: 0.00063987  max_mem: 6434M
[10/31 01:19:06] d2.utils.events INFO:  eta: 23:43:57  iter: 179  total_loss: 1.833  loss_cls: 0.4766  loss_box_reg: 0.2004  loss_mask: 0.6648  loss_rpn_cls: 0.3561  loss_rpn_loc: 0.1662  time: 0.9864  data_time: 0.1688  lr: 0.00071986  max_mem: 6434M
[10/31 01:19:25] d2.utils.events INFO:  eta: 23:43:14  iter: 199  total_loss: 1.794  loss_cls: 0.4427  loss_box_reg: 0.1846  loss_mask: 0.6631  loss_rpn_cls: 0.3708  loss_rpn_loc: 0.1564  time: 0.9844  data_time: 0.1362  lr: 0.00079984  max_mem: 6434M
[10/31 01:19:45] d2.utils.events INFO:  eta: 23:42:55  iter: 219  total_loss: 1.833  loss_cls: 0.4288  loss_box_reg: 0.176  loss_mask: 0.6554  loss_rpn_cls: 0.3606  loss_rpn_loc: 0.1559  time: 0.9837  data_time: 0.1976  lr: 0.00087982  max_mem: 6435M
[10/31 01:20:04] d2.utils.events INFO:  eta: 23:37:48  iter: 239  total_loss: 1.617  loss_cls: 0.3989  loss_box_reg: 0.1662  loss_mask: 0.6567  loss_rpn_cls: 0.2846  loss_rpn_loc: 0.1183  time: 0.9789  data_time: 0.1408  lr: 0.00095981  max_mem: 6435M
[10/31 01:20:23] d2.utils.events INFO:  eta: 23:32:59  iter: 259  total_loss: 1.813  loss_cls: 0.444  loss_box_reg: 0.1859  loss_mask: 0.6622  loss_rpn_cls: 0.3489  loss_rpn_loc: 0.1461  time: 0.9777  data_time: 0.1569  lr: 0.0010398  max_mem: 6435M
[10/31 01:20:42] d2.utils.events INFO:  eta: 23:35:25  iter: 279  total_loss: 1.741  loss_cls: 0.4354  loss_box_reg: 0.1884  loss_mask: 0.6579  loss_rpn_cls: 0.3247  loss_rpn_loc: 0.1442  time: 0.9765  data_time: 0.1372  lr: 0.0011198  max_mem: 6435M
[10/31 01:21:01] d2.utils.events INFO:  eta: 23:35:07  iter: 299  total_loss: 1.72  loss_cls: 0.4507  loss_box_reg: 0.1899  loss_mask: 0.6534  loss_rpn_cls: 0.3129  loss_rpn_loc: 0.1273  time: 0.9755  data_time: 0.1729  lr: 0.0011998  max_mem: 6435M
[10/31 01:21:21] d2.utils.events INFO:  eta: 23:36:33  iter: 319  total_loss: 1.733  loss_cls: 0.4507  loss_box_reg: 0.1763  loss_mask: 0.6503  loss_rpn_cls: 0.3054  loss_rpn_loc: 0.1278  time: 0.9766  data_time: 0.1456  lr: 0.0012797  max_mem: 6435M
[10/31 01:21:40] d2.utils.events INFO:  eta: 23:35:23  iter: 339  total_loss: 1.7  loss_cls: 0.4472  loss_box_reg: 0.1836  loss_mask: 0.6422  loss_rpn_cls: 0.3145  loss_rpn_loc: 0.1362  time: 0.9744  data_time: 0.1457  lr: 0.0013597  max_mem: 6435M
[10/31 01:21:59] d2.utils.events INFO:  eta: 23:32:38  iter: 359  total_loss: 1.693  loss_cls: 0.4279  loss_box_reg: 0.179  loss_mask: 0.6468  loss_rpn_cls: 0.3093  loss_rpn_loc: 0.1157  time: 0.9725  data_time: 0.1199  lr: 0.0014397  max_mem: 6435M
[10/31 01:22:19] d2.utils.events INFO:  eta: 23:34:45  iter: 379  total_loss: 1.804  loss_cls: 0.4384  loss_box_reg: 0.1957  loss_mask: 0.6431  loss_rpn_cls: 0.326  loss_rpn_loc: 0.1601  time: 0.9743  data_time: 0.2082  lr: 0.0015197  max_mem: 6435M
[10/31 01:22:39] d2.utils.events INFO:  eta: 23:35:43  iter: 399  total_loss: 1.763  loss_cls: 0.4516  loss_box_reg: 0.1991  loss_mask: 0.6395  loss_rpn_cls: 0.3134  loss_rpn_loc: 0.1602  time: 0.9741  data_time: 0.1234  lr: 0.0015997  max_mem: 6435M
[10/31 01:22:59] d2.utils.events INFO:  eta: 23:35:58  iter: 419  total_loss: 1.705  loss_cls: 0.4371  loss_box_reg: 0.2008  loss_mask: 0.6354  loss_rpn_cls: 0.3242  loss_rpn_loc: 0.1633  time: 0.9753  data_time: 0.1717  lr: 0.0016797  max_mem: 6435M
[10/31 01:23:18] d2.utils.events INFO:  eta: 23:35:05  iter: 439  total_loss: 1.75  loss_cls: 0.413  loss_box_reg: 0.1707  loss_mask: 0.6305  loss_rpn_cls: 0.3399  loss_rpn_loc: 0.1644  time: 0.9740  data_time: 0.1320  lr: 0.0017596  max_mem: 6435M
[10/31 01:23:37] d2.utils.events INFO:  eta: 23:36:10  iter: 459  total_loss: 1.723  loss_cls: 0.4487  loss_box_reg: 0.2055  loss_mask: 0.6427  loss_rpn_cls: 0.2923  loss_rpn_loc: 0.1282  time: 0.9744  data_time: 0.1703  lr: 0.0018396  max_mem: 6435M
[10/31 01:23:57] d2.utils.events INFO:  eta: 23:35:51  iter: 479  total_loss: 1.663  loss_cls: 0.4323  loss_box_reg: 0.1883  loss_mask: 0.633  loss_rpn_cls: 0.2872  loss_rpn_loc: 0.1179  time: 0.9744  data_time: 0.1833  lr: 0.0019196  max_mem: 6435M
[10/31 01:24:17] d2.utils.events INFO:  eta: 23:36:23  iter: 499  total_loss: 1.756  loss_cls: 0.4373  loss_box_reg: 0.1861  loss_mask: 0.6334  loss_rpn_cls: 0.3001  loss_rpn_loc: 0.1306  time: 0.9751  data_time: 0.2008  lr: 0.0019996  max_mem: 6435M
[10/31 01:24:38] d2.utils.events INFO:  eta: 23:36:13  iter: 519  total_loss: 1.763  loss_cls: 0.4265  loss_box_reg: 0.1879  loss_mask: 0.6247  loss_rpn_cls: 0.3347  loss_rpn_loc: 0.1552  time: 0.9783  data_time: 0.2575  lr: 0.0020796  max_mem: 6435M
[10/31 01:24:58] d2.utils.events INFO:  eta: 23:38:00  iter: 539  total_loss: 1.731  loss_cls: 0.4717  loss_box_reg: 0.2091  loss_mask: 0.6211  loss_rpn_cls: 0.2912  loss_rpn_loc: 0.1576  time: 0.9796  data_time: 0.1970  lr: 0.0021596  max_mem: 6435M
[10/31 01:25:18] d2.utils.events INFO:  eta: 23:38:25  iter: 559  total_loss: 1.76  loss_cls: 0.433  loss_box_reg: 0.1958  loss_mask: 0.6212  loss_rpn_cls: 0.3245  loss_rpn_loc: 0.1718  time: 0.9798  data_time: 0.1394  lr: 0.0022396  max_mem: 6435M
[10/31 01:25:38] d2.utils.events INFO:  eta: 23:39:50  iter: 579  total_loss: 1.767  loss_cls: 0.4665  loss_box_reg: 0.205  loss_mask: 0.6257  loss_rpn_cls: 0.3101  loss_rpn_loc: 0.1532  time: 0.9797  data_time: 0.1858  lr: 0.0023195  max_mem: 6435M
[10/31 01:25:57] d2.utils.events INFO:  eta: 23:37:47  iter: 599  total_loss: 1.702  loss_cls: 0.4737  loss_box_reg: 0.2013  loss_mask: 0.6226  loss_rpn_cls: 0.2783  loss_rpn_loc: 0.1327  time: 0.9791  data_time: 0.1752  lr: 0.0023995  max_mem: 6435M
[10/31 01:26:16] d2.utils.events INFO:  eta: 23:36:44  iter: 619  total_loss: 1.757  loss_cls: 0.4398  loss_box_reg: 0.2001  loss_mask: 0.6124  loss_rpn_cls: 0.3489  loss_rpn_loc: 0.1514  time: 0.9784  data_time: 0.1264  lr: 0.0024795  max_mem: 6435M
[10/31 01:26:36] d2.utils.events INFO:  eta: 23:36:25  iter: 639  total_loss: 1.718  loss_cls: 0.4165  loss_box_reg: 0.1921  loss_mask: 0.6144  loss_rpn_cls: 0.3052  loss_rpn_loc: 0.1426  time: 0.9794  data_time: 0.2018  lr: 0.0025595  max_mem: 6435M
[10/31 01:26:56] d2.utils.events INFO:  eta: 23:35:56  iter: 659  total_loss: 1.656  loss_cls: 0.4266  loss_box_reg: 0.1924  loss_mask: 0.6134  loss_rpn_cls: 0.2822  loss_rpn_loc: 0.1492  time: 0.9790  data_time: 0.1927  lr: 0.0026395  max_mem: 6435M
[10/31 01:27:15] d2.utils.events INFO:  eta: 23:35:37  iter: 679  total_loss: 1.676  loss_cls: 0.4396  loss_box_reg: 0.1934  loss_mask: 0.6076  loss_rpn_cls: 0.2885  loss_rpn_loc: 0.154  time: 0.9781  data_time: 0.1318  lr: 0.0027195  max_mem: 6435M
[10/31 01:27:33] d2.utils.events INFO:  eta: 23:33:42  iter: 699  total_loss: 1.643  loss_cls: 0.4032  loss_box_reg: 0.1824  loss_mask: 0.6111  loss_rpn_cls: 0.2807  loss_rpn_loc: 0.1548  time: 0.9770  data_time: 0.1335  lr: 0.0027994  max_mem: 6435M
[10/31 01:27:52] d2.utils.events INFO:  eta: 23:32:21  iter: 719  total_loss: 1.624  loss_cls: 0.4248  loss_box_reg: 0.1916  loss_mask: 0.6053  loss_rpn_cls: 0.2687  loss_rpn_loc: 0.1412  time: 0.9762  data_time: 0.1275  lr: 0.0028794  max_mem: 6435M
[10/31 01:28:11] d2.utils.events INFO:  eta: 23:31:30  iter: 739  total_loss: 1.561  loss_cls: 0.4029  loss_box_reg: 0.1933  loss_mask: 0.6024  loss_rpn_cls: 0.2649  loss_rpn_loc: 0.1342  time: 0.9753  data_time: 0.1376  lr: 0.0029594  max_mem: 6435M
[10/31 01:28:31] d2.utils.events INFO:  eta: 23:31:43  iter: 759  total_loss: 1.712  loss_cls: 0.45  loss_box_reg: 0.2148  loss_mask: 0.6196  loss_rpn_cls: 0.3134  loss_rpn_loc: 0.1488  time: 0.9759  data_time: 0.2271  lr: 0.0030394  max_mem: 6435M
[10/31 01:28:51] d2.utils.events INFO:  eta: 23:30:52  iter: 779  total_loss: 1.641  loss_cls: 0.4081  loss_box_reg: 0.1869  loss_mask: 0.6107  loss_rpn_cls: 0.288  loss_rpn_loc: 0.1394  time: 0.9756  data_time: 0.1375  lr: 0.0031194  max_mem: 6435M
[10/31 01:29:11] d2.utils.events INFO:  eta: 23:31:32  iter: 799  total_loss: 1.809  loss_cls: 0.4895  loss_box_reg: 0.2055  loss_mask: 0.6101  loss_rpn_cls: 0.3131  loss_rpn_loc: 0.1703  time: 0.9762  data_time: 0.1920  lr: 0.0031994  max_mem: 6435M
[10/31 01:29:30] d2.utils.events INFO:  eta: 23:31:37  iter: 819  total_loss: 1.714  loss_cls: 0.454  loss_box_reg: 0.2042  loss_mask: 0.6075  loss_rpn_cls: 0.2779  loss_rpn_loc: 0.1438  time: 0.9760  data_time: 0.1262  lr: 0.0032793  max_mem: 6435M
[10/31 01:29:51] d2.utils.events INFO:  eta: 23:33:13  iter: 839  total_loss: 1.645  loss_cls: 0.4509  loss_box_reg: 0.1947  loss_mask: 0.6063  loss_rpn_cls: 0.2602  loss_rpn_loc: 0.1572  time: 0.9780  data_time: 0.2075  lr: 0.0033593  max_mem: 6435M
[10/31 01:30:11] d2.utils.events INFO:  eta: 23:32:46  iter: 859  total_loss: 1.724  loss_cls: 0.4943  loss_box_reg: 0.2073  loss_mask: 0.6057  loss_rpn_cls: 0.2959  loss_rpn_loc: 0.1584  time: 0.9777  data_time: 0.1417  lr: 0.0034393  max_mem: 6435M
[10/31 01:30:30] d2.utils.events INFO:  eta: 23:32:14  iter: 879  total_loss: 1.622  loss_cls: 0.4057  loss_box_reg: 0.1856  loss_mask: 0.6027  loss_rpn_cls: 0.2854  loss_rpn_loc: 0.1582  time: 0.9774  data_time: 0.1315  lr: 0.0035193  max_mem: 6435M
[10/31 01:30:49] d2.utils.events INFO:  eta: 23:31:55  iter: 899  total_loss: 1.659  loss_cls: 0.4535  loss_box_reg: 0.197  loss_mask: 0.598  loss_rpn_cls: 0.2617  loss_rpn_loc: 0.1517  time: 0.9771  data_time: 0.1178  lr: 0.0035993  max_mem: 6435M
[10/31 01:31:08] d2.utils.events INFO:  eta: 23:30:13  iter: 919  total_loss: 1.678  loss_cls: 0.4479  loss_box_reg: 0.2056  loss_mask: 0.609  loss_rpn_cls: 0.2752  loss_rpn_loc: 0.1533  time: 0.9766  data_time: 0.1412  lr: 0.0036793  max_mem: 6435M
[10/31 01:31:27] d2.utils.events INFO:  eta: 23:29:43  iter: 939  total_loss: 1.602  loss_cls: 0.4116  loss_box_reg: 0.2014  loss_mask: 0.5998  loss_rpn_cls: 0.2628  loss_rpn_loc: 0.132  time: 0.9755  data_time: 0.1205  lr: 0.0037592  max_mem: 6435M
[10/31 01:31:47] d2.utils.events INFO:  eta: 23:29:24  iter: 959  total_loss: 1.585  loss_cls: 0.4027  loss_box_reg: 0.2021  loss_mask: 0.6005  loss_rpn_cls: 0.2807  loss_rpn_loc: 0.1338  time: 0.9765  data_time: 0.2163  lr: 0.0038392  max_mem: 6435M
[10/31 01:32:07] d2.utils.events INFO:  eta: 23:29:16  iter: 979  total_loss: 1.659  loss_cls: 0.4415  loss_box_reg: 0.2006  loss_mask: 0.6077  loss_rpn_cls: 0.2761  loss_rpn_loc: 0.1374  time: 0.9764  data_time: 0.1574  lr: 0.0039192  max_mem: 6435M
[10/31 01:32:26] d2.utils.events INFO:  eta: 23:28:51  iter: 999  total_loss: 1.696  loss_cls: 0.4513  loss_box_reg: 0.2192  loss_mask: 0.5964  loss_rpn_cls: 0.2731  loss_rpn_loc: 0.16  time: 0.9759  data_time: 0.1402  lr: 0.0039992  max_mem: 6435M
[10/31 01:32:46] d2.utils.events INFO:  eta: 23:28:27  iter: 1019  total_loss: 1.785  loss_cls: 0.4707  loss_box_reg: 0.2191  loss_mask: 0.5989  loss_rpn_cls: 0.2947  loss_rpn_loc: 0.167  time: 0.9761  data_time: 0.1552  lr: 0.0040792  max_mem: 6435M
[10/31 01:33:05] d2.utils.events INFO:  eta: 23:28:19  iter: 1039  total_loss: 1.781  loss_cls: 0.445  loss_box_reg: 0.2168  loss_mask: 0.5971  loss_rpn_cls: 0.3342  loss_rpn_loc: 0.1464  time: 0.9759  data_time: 0.1419  lr: 0.0041592  max_mem: 6435M
[10/31 01:33:25] d2.utils.events INFO:  eta: 23:30:29  iter: 1059  total_loss: 1.776  loss_cls: 0.4751  loss_box_reg: 0.2176  loss_mask: 0.6075  loss_rpn_cls: 0.322  loss_rpn_loc: 0.1491  time: 0.9764  data_time: 0.1758  lr: 0.0042392  max_mem: 6435M
[10/31 01:33:44] d2.utils.events INFO:  eta: 23:31:34  iter: 1079  total_loss: 1.741  loss_cls: 0.4601  loss_box_reg: 0.1927  loss_mask: 0.6035  loss_rpn_cls: 0.3247  loss_rpn_loc: 0.1603  time: 0.9759  data_time: 0.1189  lr: 0.0043191  max_mem: 6435M
[10/31 01:34:04] d2.utils.events INFO:  eta: 23:29:51  iter: 1099  total_loss: 1.686  loss_cls: 0.4401  loss_box_reg: 0.1891  loss_mask: 0.6133  loss_rpn_cls: 0.3261  loss_rpn_loc: 0.1456  time: 0.9758  data_time: 0.1577  lr: 0.0043991  max_mem: 6435M
[10/31 01:34:24] d2.utils.events INFO:  eta: 23:28:58  iter: 1119  total_loss: 1.652  loss_cls: 0.4268  loss_box_reg: 0.1829  loss_mask: 0.5969  loss_rpn_cls: 0.2719  loss_rpn_loc: 0.1418  time: 0.9763  data_time: 0.1679  lr: 0.0044791  max_mem: 6435M
[10/31 01:34:43] d2.utils.events INFO:  eta: 23:27:26  iter: 1139  total_loss: 1.679  loss_cls: 0.4326  loss_box_reg: 0.1985  loss_mask: 0.6016  loss_rpn_cls: 0.2762  loss_rpn_loc: 0.1471  time: 0.9760  data_time: 0.1550  lr: 0.0045591  max_mem: 6435M
[10/31 01:35:03] d2.utils.events INFO:  eta: 23:30:29  iter: 1159  total_loss: 1.681  loss_cls: 0.4567  loss_box_reg: 0.191  loss_mask: 0.6015  loss_rpn_cls: 0.2902  loss_rpn_loc: 0.1317  time: 0.9764  data_time: 0.1809  lr: 0.0046391  max_mem: 6435M
[10/31 01:35:22] d2.utils.events INFO:  eta: 23:27:52  iter: 1179  total_loss: 1.725  loss_cls: 0.4759  loss_box_reg: 0.219  loss_mask: 0.5981  loss_rpn_cls: 0.2885  loss_rpn_loc: 0.1441  time: 0.9759  data_time: 0.1451  lr: 0.0047191  max_mem: 6435M
[10/31 01:35:42] d2.utils.events INFO:  eta: 23:28:09  iter: 1199  total_loss: 1.755  loss_cls: 0.4511  loss_box_reg: 0.2098  loss_mask: 0.5981  loss_rpn_cls: 0.3238  loss_rpn_loc: 0.1658  time: 0.9764  data_time: 0.1559  lr: 0.004799  max_mem: 6435M
[10/31 01:36:02] d2.utils.events INFO:  eta: 23:29:21  iter: 1219  total_loss: 1.72  loss_cls: 0.4642  loss_box_reg: 0.2121  loss_mask: 0.5998  loss_rpn_cls: 0.3075  loss_rpn_loc: 0.1459  time: 0.9767  data_time: 0.1795  lr: 0.004879  max_mem: 6435M
[10/31 01:36:21] d2.utils.events INFO:  eta: 23:29:57  iter: 1239  total_loss: 1.668  loss_cls: 0.4324  loss_box_reg: 0.1848  loss_mask: 0.5924  loss_rpn_cls: 0.2978  loss_rpn_loc: 0.1709  time: 0.9764  data_time: 0.1261  lr: 0.004959  max_mem: 6435M
[10/31 01:36:41] d2.utils.events INFO:  eta: 23:30:26  iter: 1259  total_loss: 1.673  loss_cls: 0.4301  loss_box_reg: 0.195  loss_mask: 0.5918  loss_rpn_cls: 0.2949  loss_rpn_loc: 0.1652  time: 0.9765  data_time: 0.1651  lr: 0.005039  max_mem: 6435M
[10/31 01:37:00] d2.utils.events INFO:  eta: 23:30:11  iter: 1279  total_loss: 1.713  loss_cls: 0.4451  loss_box_reg: 0.1839  loss_mask: 0.605  loss_rpn_cls: 0.2924  loss_rpn_loc: 0.1504  time: 0.9759  data_time: 0.1254  lr: 0.005119  max_mem: 6435M
[10/31 01:37:20] d2.utils.events INFO:  eta: 23:30:47  iter: 1299  total_loss: 1.708  loss_cls: 0.4649  loss_box_reg: 0.2139  loss_mask: 0.6001  loss_rpn_cls: 0.3145  loss_rpn_loc: 0.1384  time: 0.9764  data_time: 0.1463  lr: 0.005199  max_mem: 6435M
[10/31 01:37:40] d2.utils.events INFO:  eta: 23:30:17  iter: 1319  total_loss: 1.697  loss_cls: 0.4307  loss_box_reg: 0.201  loss_mask: 0.6064  loss_rpn_cls: 0.2928  loss_rpn_loc: 0.1484  time: 0.9763  data_time: 0.1296  lr: 0.0052789  max_mem: 6435M
[10/31 01:37:59] d2.utils.events INFO:  eta: 23:31:12  iter: 1339  total_loss: 1.629  loss_cls: 0.4678  loss_box_reg: 0.1898  loss_mask: 0.5915  loss_rpn_cls: 0.2481  loss_rpn_loc: 0.1311  time: 0.9760  data_time: 0.1470  lr: 0.0053589  max_mem: 6435M
[10/31 01:38:19] d2.utils.events INFO:  eta: 23:32:42  iter: 1359  total_loss: 1.68  loss_cls: 0.4475  loss_box_reg: 0.2095  loss_mask: 0.5935  loss_rpn_cls: 0.2701  loss_rpn_loc: 0.1614  time: 0.9766  data_time: 0.1627  lr: 0.0054389  max_mem: 6435M
[10/31 01:38:40] d2.utils.events INFO:  eta: 23:32:23  iter: 1379  total_loss: 1.612  loss_cls: 0.4151  loss_box_reg: 0.1878  loss_mask: 0.5932  loss_rpn_cls: 0.251  loss_rpn_loc: 0.1518  time: 0.9776  data_time: 0.2470  lr: 0.0055189  max_mem: 6435M
[10/31 01:39:00] d2.utils.events INFO:  eta: 23:32:10  iter: 1399  total_loss: 1.629  loss_cls: 0.4337  loss_box_reg: 0.2038  loss_mask: 0.5844  loss_rpn_cls: 0.2377  loss_rpn_loc: 0.1349  time: 0.9776  data_time: 0.1451  lr: 0.0055989  max_mem: 6435M
[10/31 01:39:20] d2.utils.events INFO:  eta: 23:30:08  iter: 1419  total_loss: 1.606  loss_cls: 0.4222  loss_box_reg: 0.2039  loss_mask: 0.5838  loss_rpn_cls: 0.251  loss_rpn_loc: 0.1295  time: 0.9778  data_time: 0.1852  lr: 0.0056789  max_mem: 6435M
[10/31 01:39:40] d2.utils.events INFO:  eta: 23:30:57  iter: 1439  total_loss: 1.647  loss_cls: 0.4316  loss_box_reg: 0.2131  loss_mask: 0.5849  loss_rpn_cls: 0.273  loss_rpn_loc: 0.1599  time: 0.9782  data_time: 0.1415  lr: 0.0057588  max_mem: 6435M
[10/31 01:39:59] d2.utils.events INFO:  eta: 23:30:56  iter: 1459  total_loss: 1.773  loss_cls: 0.4719  loss_box_reg: 0.2131  loss_mask: 0.5961  loss_rpn_cls: 0.2844  loss_rpn_loc: 0.1558  time: 0.9779  data_time: 0.1025  lr: 0.0058388  max_mem: 6435M
[10/31 01:40:19] d2.utils.events INFO:  eta: 23:30:48  iter: 1479  total_loss: 1.712  loss_cls: 0.4575  loss_box_reg: 0.217  loss_mask: 0.5817  loss_rpn_cls: 0.2867  loss_rpn_loc: 0.1602  time: 0.9780  data_time: 0.1288  lr: 0.0059188  max_mem: 6435M
[10/31 01:40:38] d2.utils.events INFO:  eta: 23:31:05  iter: 1499  total_loss: 1.708  loss_cls: 0.4432  loss_box_reg: 0.2131  loss_mask: 0.5864  loss_rpn_cls: 0.257  loss_rpn_loc: 0.1572  time: 0.9779  data_time: 0.1365  lr: 0.0059988  max_mem: 6435M
[10/31 01:40:58] d2.utils.events INFO:  eta: 23:30:46  iter: 1519  total_loss: 1.625  loss_cls: 0.418  loss_box_reg: 0.1843  loss_mask: 0.5835  loss_rpn_cls: 0.2633  loss_rpn_loc: 0.1658  time: 0.9778  data_time: 0.1205  lr: 0.0060788  max_mem: 6435M
[10/31 01:41:18] d2.utils.events INFO:  eta: 23:29:39  iter: 1539  total_loss: 1.589  loss_cls: 0.4374  loss_box_reg: 0.2111  loss_mask: 0.5834  loss_rpn_cls: 0.2311  loss_rpn_loc: 0.1275  time: 0.9782  data_time: 0.1980  lr: 0.0061588  max_mem: 6435M
[10/31 01:41:38] d2.utils.events INFO:  eta: 23:29:03  iter: 1559  total_loss: 1.546  loss_cls: 0.4325  loss_box_reg: 0.1974  loss_mask: 0.5836  loss_rpn_cls: 0.223  loss_rpn_loc: 0.126  time: 0.9783  data_time: 0.1418  lr: 0.0062388  max_mem: 6435M
[10/31 01:41:58] d2.utils.events INFO:  eta: 23:27:35  iter: 1579  total_loss: 1.509  loss_cls: 0.4172  loss_box_reg: 0.2043  loss_mask: 0.5718  loss_rpn_cls: 0.2264  loss_rpn_loc: 0.1351  time: 0.9787  data_time: 0.1950  lr: 0.0063187  max_mem: 6435M
[10/31 01:42:18] d2.utils.events INFO:  eta: 23:29:58  iter: 1599  total_loss: 1.717  loss_cls: 0.4491  loss_box_reg: 0.2193  loss_mask: 0.5802  loss_rpn_cls: 0.2702  loss_rpn_loc: 0.1694  time: 0.9793  data_time: 0.1805  lr: 0.0063987  max_mem: 6435M
[10/31 01:42:39] d2.utils.events INFO:  eta: 23:30:21  iter: 1619  total_loss: 1.58  loss_cls: 0.4401  loss_box_reg: 0.2043  loss_mask: 0.566  loss_rpn_cls: 0.2448  loss_rpn_loc: 0.1313  time: 0.9796  data_time: 0.1898  lr: 0.0064787  max_mem: 6435M
[10/31 01:42:59] d2.utils.events INFO:  eta: 23:30:02  iter: 1639  total_loss: 1.869  loss_cls: 0.4903  loss_box_reg: 0.1973  loss_mask: 0.6106  loss_rpn_cls: 0.38  loss_rpn_loc: 0.1779  time: 0.9798  data_time: 0.1627  lr: 0.0065587  max_mem: 6435M
[10/31 01:42:59] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 134, in train
    self.run_step()
  File "/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 423, in run_step
    self._trainer.run_step()
  File "/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 228, in run_step
    loss_dict = self.model(data)
  File "/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 511, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 160, in forward
    proposals, proposal_losses = self.proposal_generator(images, features, gt_instances)
  File "/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/modeling/proposal_generator/rpn.py", line 448, in forward
    proposals = self.predict_proposals(
  File "/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/modeling/proposal_generator/rpn.py", line 474, in predict_proposals
    return find_top_rpn_proposals(
  File "/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/modeling/proposal_generator/proposal_utils.py", line 91, in find_top_rpn_proposals
    raise FloatingPointError(
FloatingPointError: Predicted boxes or scores contain Inf/NaN. Training has diverged.
[10/31 01:42:59] d2.engine.hooks INFO: Overall training speed: 1638 iterations in 0:26:45 (0.9801 s / it)
[10/31 01:42:59] d2.engine.hooks INFO: Total training time: 0:26:48 (0:00:03 on hooks)
[10/31 01:42:59] d2.utils.events INFO:  eta: 23:30:01  iter: 1640  total_loss: 1.869  loss_cls: 0.4903  loss_box_reg: 0.1973  loss_mask: 0.6106  loss_rpn_cls: 0.38  loss_rpn_loc: 0.1779  time: 0.9798  data_time: 0.1627  lr: 0.0065587  max_mem: 6435M
[10/31 04:54:46] detectron2 INFO: Rank of current process: 0. World size: 2
[10/31 04:54:48] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.3 (default, May 19 2020, 18:47:26) [GCC 7.3.0]
numpy                   1.19.1
detectron2              0.2.1 @/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 5.4
CUDA compiler           not available
detectron2 arch flags   /cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/_C.cpython-38-x86_64-linux-gnu.so; cannot find cuobjdump
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.6.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           True
GPU 0,1                 GeForce GTX 1080 Ti (arch=6.1)
CUDA_HOME               /opt/common/cuda/cuda-10.0.130/lib64/
Pillow                  7.2.0
torchvision             0.7.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision
torchvision arch flags  /cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                  0.1.2.post20201030
cv2                     4.4.0
----------------------  --------------------------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

[10/31 04:54:48] detectron2 INFO: Command line arguments: Namespace(config_file='configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml', dist_url='tcp://127.0.0.1:63010', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=['MODEL.MASK_ON', 'True', 'SOLVER.BASE_LR', '0.015', 'OUTPUT_DIR', 'temp'], resume=False)
[10/31 04:54:48] detectron2 INFO: Contents of args.config_file=configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml:
_BASE_: "../Base-RCNN-FPN.yaml"
MODEL:
  WEIGHTS: "https://download.pytorch.org/models/resnet18-5c106cde.pth"
  MASK_ON: True
  RESNETS:
    DEPTH: 18
    RES2_OUT_CHANNELS: 64
# DATASETS:
#   TRAIN: ("coco_2017_val",)
#   TEST: ("coco_2017_val",) #("coco_2017_test-dev",)
SOLVER:
  BASE_LR: 0.02
  CHECKPOINT_PERIOD: 7330
  # IMS_PER_BATCH: 24
  # STEPS: (40000, 53500)
  # MAX_ITER: 60000
  # WARMUP_FACTOR: 5e-4
  # WARMUP_ITERS: 2000
OUTPUT_DIR: output/faster_r18_fpn

[10/31 04:54:48] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  FILTER_EMPTY_ANNOTATIONS: True
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: ()
  PROPOSAL_FILES_TRAIN: ()
  TEST: ('coco_2017_val',)
  TRAIN: ('coco_2017_train',)
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: False
    SIZE: [0.9, 0.9]
    TYPE: relative_range
  FORMAT: BGR
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES: [[-90, 0, 90]]
    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES: [[32], [64], [128], [256], [512]]
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_fpn_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']
    NORM: 
    OUT_CHANNELS: 256
  KEYPOINT_ON: False
  LOAD_PROPOSALS: False
  MASK_ON: True
  META_ARCHITECTURE: GeneralizedRCNN
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: True
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN: [103.53, 116.28, 123.675]
  PIXEL_STD: [1.0, 1.0, 1.0]
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: False
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE: [False, False, False, False]
    DEPTH: 18
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']
    RES2_OUT_CHANNELS: 64
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.4, 0.5]
    NMS_THRESH_TEST: 0.5
    NORM: 
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))
    IOUS: (0.5, 0.6, 0.7)
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    CLS_AGNOSTIC_BBOX_REG: False
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: FastRCNNConvFCHead
    NORM: 
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    IOU_LABELS: [0, 1]
    IOU_THRESHOLDS: [0.5]
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: True
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: False
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: 
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    BOUNDARY_THRESH: -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.3, 0.7]
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: https://download.pytorch.org/models/resnet18-5c106cde.pth
OUTPUT_DIR: temp
SEED: -1
SOLVER:
  AMP:
    ENABLED: False
  BASE_LR: 0.015
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 7330
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: False
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 16
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 90000
  MOMENTUM: 0.9
  NESTEROV: False
  REFERENCE_WORLD_SIZE: 0
  STEPS: (60000, 80000)
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: False
    FLIP: True
    MAX_SIZE: 4000
    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 0
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: False
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0
[10/31 04:54:48] detectron2 INFO: Full config saved to temp/config.yaml
[10/31 04:54:48] d2.utils.env INFO: Using a generated random seed 48608085
[10/31 04:54:49] d2.engine.defaults INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten()
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[10/31 04:55:10] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 21.20 seconds.
[10/31 04:55:11] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/31 04:55:18] d2.data.build INFO: Removed 1021 images with no usable annotations. 117266 images left.
[10/31 04:55:24] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[10/31 04:55:24] d2.data.common INFO: Serializing 117266 elements to byte tensors and concatenating them all ...
[10/31 04:55:28] d2.data.common INFO: Serialized dataset takes 451.21 MiB
[10/31 04:55:28] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[10/31 04:55:28] d2.data.build INFO: Using training sampler TrainingSampler
[10/31 04:55:31] fvcore.common.checkpoint INFO: Loading checkpoint from https://download.pytorch.org/models/resnet18-5c106cde.pth
[10/31 04:55:31] fvcore.common.file_io INFO: URL https://download.pytorch.org/models/resnet18-5c106cde.pth cached in /cfarhomes/shishira/.torch/fvcore_cache/models/resnet18-5c106cde.pth
[10/31 04:55:31] fvcore.common.checkpoint INFO: Some model parameters or buffers are not found in the checkpoint:
  [34mbackbone.bottom_up.res4.0.shortcut.norm.{bias, weight}[0m
  [34mbackbone.fpn_lateral3.{weight, bias}[0m
  [34mproposal_generator.anchor_generator.cell_anchors.{3, 0, 2, 4, 1}[0m
  [34mproposal_generator.rpn_head.conv.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.fpn_lateral5.{bias, weight}[0m
  [34mroi_heads.box_predictor.cls_score.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.conv1.weight[0m
  [34mbackbone.fpn_output3.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.1.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.1.conv2.weight[0m
  [34mbackbone.bottom_up.res5.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res2.0.conv2.norm.{bias, weight}[0m
  [34mroi_heads.mask_head.predictor.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.0.conv2.weight[0m
  [34mbackbone.bottom_up.res3.1.conv2.weight[0m
  [34mroi_heads.box_head.fc2.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.1.conv2.weight[0m
  [34mbackbone.bottom_up.res5.1.conv2.weight[0m
  [34mbackbone.bottom_up.res5.0.conv1.weight[0m
  [34mbackbone.bottom_up.res2.1.conv1.weight[0m
  [34mbackbone.bottom_up.res5.0.conv2.norm.{weight, bias}[0m
  [34mbackbone.fpn_output4.{bias, weight}[0m
  [34mbackbone.fpn_output5.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.shortcut.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.1.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.conv1.weight[0m
  [34mbackbone.fpn_lateral2.{weight, bias}[0m
  [34mroi_heads.box_predictor.bbox_pred.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.1.conv1.norm.{weight, bias}[0m
  [34mroi_heads.mask_head.mask_fcn4.{bias, weight}[0m
  [34mroi_heads.mask_head.deconv.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.conv2.weight[0m
  [34mbackbone.bottom_up.res2.1.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.shortcut.weight[0m
  [34mroi_heads.mask_head.mask_fcn2.{weight, bias}[0m
  [34mroi_heads.mask_head.mask_fcn1.{bias, weight}[0m
  [34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
  [34mbackbone.bottom_up.stem.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.1.conv1.norm.{weight, bias}[0m
  [34mroi_heads.box_head.fc1.{weight, bias}[0m
  [34mroi_heads.mask_head.mask_fcn3.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.1.conv1.weight[0m
  [34mbackbone.bottom_up.res3.0.shortcut.norm.{bias, weight}[0m
  [34mbackbone.fpn_lateral4.{weight, bias}[0m
  [34mbackbone.fpn_output2.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv1.weight[0m
  [34mbackbone.bottom_up.res5.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.1.conv1.weight[0m
  [34mbackbone.bottom_up.stem.conv1.weight[0m
  [34mbackbone.bottom_up.res3.1.conv1.weight[0m
  [34mbackbone.bottom_up.res4.0.conv2.norm.{bias, weight}[0m
  [34mproposal_generator.rpn_head.anchor_deltas.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res5.0.conv2.weight[0m
  [34mbackbone.bottom_up.res4.0.conv2.weight[0m
[10/31 04:55:31] fvcore.common.checkpoint INFO: The checkpoint state_dict contains keys that are not used by the model:
  [35mconv1.weight[0m
  [35mbn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv1.weight[0m
  [35mlayer1.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv2.weight[0m
  [35mlayer1.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv1.weight[0m
  [35mlayer1.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv2.weight[0m
  [35mlayer1.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv1.weight[0m
  [35mlayer2.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv2.weight[0m
  [35mlayer2.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.downsample.0.weight[0m
  [35mlayer2.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv1.weight[0m
  [35mlayer2.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv2.weight[0m
  [35mlayer2.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv1.weight[0m
  [35mlayer3.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv2.weight[0m
  [35mlayer3.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.downsample.0.weight[0m
  [35mlayer3.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv1.weight[0m
  [35mlayer3.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv2.weight[0m
  [35mlayer3.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv1.weight[0m
  [35mlayer4.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv2.weight[0m
  [35mlayer4.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.downsample.0.weight[0m
  [35mlayer4.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv1.weight[0m
  [35mlayer4.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv2.weight[0m
  [35mlayer4.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mfc.{weight, bias}[0m
[10/31 04:55:32] d2.engine.train_loop INFO: Starting training from iteration 0
[10/31 04:56:04] d2.utils.events INFO:  eta: 22:23:33  iter: 19  total_loss: 7.472  loss_cls: 4.233  loss_box_reg: 0.07112  loss_mask: 0.7139  loss_rpn_cls: 0.7587  loss_rpn_loc: 0.951  time: 0.9121  data_time: 0.8079  lr: 0.00029971  max_mem: 6438M
[10/31 04:56:21] d2.utils.events INFO:  eta: 20:59:33  iter: 39  total_loss: 10.79  loss_cls: 8.262  loss_box_reg: 0.1216  loss_mask: 0.7077  loss_rpn_cls: 0.6188  loss_rpn_loc: 0.3959  time: 0.8741  data_time: 0.1190  lr: 0.00059941  max_mem: 6438M
[10/31 04:56:30] d2.engine.hooks INFO: Overall training speed: 47 iterations in 0:00:41 (0.8820 s / it)
[10/31 04:56:30] d2.engine.hooks INFO: Total training time: 0:00:41 (0:00:00 on hooks)
[10/31 04:56:30] d2.utils.events INFO:  eta: 21:40:23  iter: 49  total_loss: 7.545  loss_cls: 5.231  loss_box_reg: 0.1491  loss_mask: 0.7147  loss_rpn_cls: 0.4933  loss_rpn_loc: 0.2183  time: 0.8717  data_time: 0.1139  lr: 0.00073428  max_mem: 6438M
[10/31 13:10:54] detectron2 INFO: Rank of current process: 0. World size: 2
[10/31 13:10:56] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.3 (default, May 19 2020, 18:47:26) [GCC 7.3.0]
numpy                   1.19.1
detectron2              0.2.1 @/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 5.4
CUDA compiler           not available
detectron2 arch flags   /cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/_C.cpython-38-x86_64-linux-gnu.so; cannot find cuobjdump
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.6.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           True
GPU 0,1                 GeForce GTX 1080 Ti (arch=6.1)
CUDA_HOME               /opt/common/cuda/cuda-10.0.130/lib64/
Pillow                  7.2.0
torchvision             0.7.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision
torchvision arch flags  /cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                  0.1.2.post20201030
cv2                     4.4.0
----------------------  --------------------------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

[10/31 13:10:56] detectron2 INFO: Command line arguments: Namespace(config_file='configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml', dist_url='tcp://127.0.0.1:63010', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=['MODEL.MASK_ON', 'True', 'SOLVER.BASE_LR', '0.015', 'OUTPUT_DIR', 'temp'], resume=False)
[10/31 13:10:56] detectron2 INFO: Contents of args.config_file=configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml:
_BASE_: "../Base-RCNN-FPN.yaml"
MODEL:
  WEIGHTS: "https://download.pytorch.org/models/resnet18-5c106cde.pth"
  MASK_ON: True
  RESNETS:
    DEPTH: 18
    RES2_OUT_CHANNELS: 64
# DATASETS:
#   TRAIN: ("coco_2017_val",)
#   TEST: ("coco_2017_val",) #("coco_2017_test-dev",)
SOLVER:
  BASE_LR: 0.02
  CHECKPOINT_PERIOD: 7330
  # IMS_PER_BATCH: 24
  # STEPS: (40000, 53500)
  # MAX_ITER: 60000
  # WARMUP_FACTOR: 5e-4
  # WARMUP_ITERS: 2000
OUTPUT_DIR: output/faster_r18_fpn

[10/31 13:10:56] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  FILTER_EMPTY_ANNOTATIONS: True
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: ()
  PROPOSAL_FILES_TRAIN: ()
  TEST: ('coco_2017_val',)
  TRAIN: ('coco_2017_train',)
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: False
    SIZE: [0.9, 0.9]
    TYPE: relative_range
  FORMAT: BGR
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES: [[-90, 0, 90]]
    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES: [[32], [64], [128], [256], [512]]
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_fpn_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']
    NORM: 
    OUT_CHANNELS: 256
  KEYPOINT_ON: False
  LOAD_PROPOSALS: False
  MASK_ON: True
  META_ARCHITECTURE: GeneralizedRCNN
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: True
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN: [103.53, 116.28, 123.675]
  PIXEL_STD: [1.0, 1.0, 1.0]
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: False
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE: [False, False, False, False]
    DEPTH: 18
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']
    RES2_OUT_CHANNELS: 64
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.4, 0.5]
    NMS_THRESH_TEST: 0.5
    NORM: 
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))
    IOUS: (0.5, 0.6, 0.7)
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    CLS_AGNOSTIC_BBOX_REG: False
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: FastRCNNConvFCHead
    NORM: 
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    IOU_LABELS: [0, 1]
    IOU_THRESHOLDS: [0.5]
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: True
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: False
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: 
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    BOUNDARY_THRESH: -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.3, 0.7]
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: https://download.pytorch.org/models/resnet18-5c106cde.pth
OUTPUT_DIR: temp
SEED: -1
SOLVER:
  AMP:
    ENABLED: False
  BASE_LR: 0.015
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 7330
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: False
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 16
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 90000
  MOMENTUM: 0.9
  NESTEROV: False
  REFERENCE_WORLD_SIZE: 0
  STEPS: (60000, 80000)
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: False
    FLIP: True
    MAX_SIZE: 4000
    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 0
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: False
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0
[10/31 13:10:56] detectron2 INFO: Full config saved to temp/config.yaml
[10/31 13:10:56] d2.utils.env INFO: Using a generated random seed 56782826
[10/31 13:10:57] d2.engine.defaults INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten()
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[10/31 13:11:18] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 21.40 seconds.
[10/31 13:11:19] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/31 13:11:27] d2.data.build INFO: Removed 1021 images with no usable annotations. 117266 images left.
[10/31 13:11:33] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[10/31 13:11:33] d2.data.common INFO: Serializing 117266 elements to byte tensors and concatenating them all ...
[10/31 13:11:37] d2.data.common INFO: Serialized dataset takes 451.21 MiB
[10/31 13:11:37] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[10/31 13:11:37] d2.data.build INFO: Using training sampler TrainingSampler
[10/31 13:11:40] fvcore.common.checkpoint INFO: Loading checkpoint from https://download.pytorch.org/models/resnet18-5c106cde.pth
[10/31 13:11:40] fvcore.common.file_io INFO: URL https://download.pytorch.org/models/resnet18-5c106cde.pth cached in /cfarhomes/shishira/.torch/fvcore_cache/models/resnet18-5c106cde.pth
[10/31 13:11:40] fvcore.common.checkpoint INFO: Some model parameters or buffers are not found in the checkpoint:
  [34mbackbone.bottom_up.res3.0.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.conv2.weight[0m
  [34mroi_heads.mask_head.mask_fcn4.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.1.conv2.weight[0m
  [34mproposal_generator.rpn_head.conv.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.1.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.stem.conv1.weight[0m
  [34mbackbone.bottom_up.res3.0.shortcut.norm.{weight, bias}[0m
  [34mproposal_generator.anchor_generator.cell_anchors.{2, 1, 4, 0, 3}[0m
  [34mbackbone.bottom_up.res3.0.conv1.norm.{weight, bias}[0m
  [34mbackbone.fpn_output3.{weight, bias}[0m
  [34mbackbone.fpn_lateral3.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.1.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res2.1.conv2.norm.{bias, weight}[0m
  [34mroi_heads.box_head.fc1.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.1.conv2.weight[0m
  [34mbackbone.bottom_up.res2.0.conv2.norm.{bias, weight}[0m
  [34mbackbone.fpn_lateral4.{weight, bias}[0m
  [34mbackbone.fpn_lateral5.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.1.conv1.norm.{bias, weight}[0m
  [34mbackbone.fpn_output4.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res4.1.conv2.weight[0m
  [34mbackbone.bottom_up.res3.0.conv2.weight[0m
  [34mbackbone.bottom_up.res5.0.shortcut.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.conv2.norm.{weight, bias}[0m
  [34mroi_heads.mask_head.mask_fcn2.{bias, weight}[0m
  [34mroi_heads.mask_head.predictor.{bias, weight}[0m
  [34mroi_heads.mask_head.deconv.{weight, bias}[0m
  [34mbackbone.fpn_lateral2.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.conv1.weight[0m
  [34mroi_heads.mask_head.mask_fcn1.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.1.conv1.norm.{bias, weight}[0m
  [34mroi_heads.box_predictor.cls_score.{bias, weight}[0m
  [34mproposal_generator.rpn_head.objectness_logits.{weight, bias}[0m
  [34mbackbone.fpn_output2.{weight, bias}[0m
  [34mbackbone.bottom_up.stem.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.1.conv1.weight[0m
  [34mbackbone.bottom_up.res4.0.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.conv1.weight[0m
  [34mbackbone.bottom_up.res4.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.conv1.weight[0m
  [34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
  [34mroi_heads.box_head.fc2.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.1.conv2.weight[0m
  [34mbackbone.bottom_up.res3.1.conv2.norm.{bias, weight}[0m
  [34mroi_heads.box_predictor.bbox_pred.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn3.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.conv2.weight[0m
  [34mbackbone.bottom_up.res2.0.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.shortcut.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.0.conv1.weight[0m
  [34mbackbone.bottom_up.res4.1.conv1.norm.{weight, bias}[0m
  [34mbackbone.fpn_output5.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.1.conv1.weight[0m
  [34mbackbone.bottom_up.res2.0.conv2.weight[0m
  [34mbackbone.bottom_up.res4.1.conv1.weight[0m
  [34mbackbone.bottom_up.res2.1.conv1.weight[0m
  [34mbackbone.bottom_up.res5.0.shortcut.weight[0m
[10/31 13:11:40] fvcore.common.checkpoint INFO: The checkpoint state_dict contains keys that are not used by the model:
  [35mconv1.weight[0m
  [35mbn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv1.weight[0m
  [35mlayer1.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv2.weight[0m
  [35mlayer1.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv1.weight[0m
  [35mlayer1.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv2.weight[0m
  [35mlayer1.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv1.weight[0m
  [35mlayer2.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv2.weight[0m
  [35mlayer2.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.downsample.0.weight[0m
  [35mlayer2.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv1.weight[0m
  [35mlayer2.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv2.weight[0m
  [35mlayer2.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv1.weight[0m
  [35mlayer3.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv2.weight[0m
  [35mlayer3.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.downsample.0.weight[0m
  [35mlayer3.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv1.weight[0m
  [35mlayer3.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv2.weight[0m
  [35mlayer3.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv1.weight[0m
  [35mlayer4.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv2.weight[0m
  [35mlayer4.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.downsample.0.weight[0m
  [35mlayer4.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv1.weight[0m
  [35mlayer4.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv2.weight[0m
  [35mlayer4.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mfc.{weight, bias}[0m
[10/31 13:11:41] d2.engine.train_loop INFO: Starting training from iteration 0
[10/31 13:12:15] d2.utils.events INFO:  eta: 21:33:06  iter: 19  total_loss: 10.9  loss_cls: 7.42  loss_box_reg: 0.131  loss_mask: 0.7378  loss_rpn_cls: 0.7706  loss_rpn_loc: 0.9816  time: 1.0156  data_time: 0.9608  lr: 0.00029971  max_mem: 6443M
[10/31 13:12:33] d2.engine.hooks INFO: Overall training speed: 37 iterations in 0:00:35 (0.9566 s / it)
[10/31 13:12:33] d2.engine.hooks INFO: Total training time: 0:00:35 (0:00:00 on hooks)
[10/31 13:12:33] d2.utils.events INFO:  eta: 21:02:26  iter: 39  total_loss: 3.037  loss_cls: 1.612  loss_box_reg: 0.03022  loss_mask: 0.688  loss_rpn_cls: 0.4701  loss_rpn_loc: 0.3127  time: 0.9490  data_time: 0.2326  lr: 0.00058443  max_mem: 6443M
[10/31 13:20:37] detectron2 INFO: Rank of current process: 0. World size: 2
[10/31 13:20:38] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.3 (default, May 19 2020, 18:47:26) [GCC 7.3.0]
numpy                   1.19.1
detectron2              0.2.1 @/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 5.4
CUDA compiler           not available
detectron2 arch flags   /cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/_C.cpython-38-x86_64-linux-gnu.so; cannot find cuobjdump
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.6.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           True
GPU 0,1                 GeForce GTX 1080 Ti (arch=6.1)
CUDA_HOME               /opt/common/cuda/cuda-10.0.130/lib64/
Pillow                  7.2.0
torchvision             0.7.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision
torchvision arch flags  /cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                  0.1.2.post20201030
cv2                     4.4.0
----------------------  --------------------------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

[10/31 13:20:38] detectron2 INFO: Command line arguments: Namespace(config_file='configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml', dist_url='tcp://127.0.0.1:63010', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=['MODEL.MASK_ON', 'True', 'SOLVER.BASE_LR', '0.015', 'OUTPUT_DIR', 'temp'], resume=False)
[10/31 13:20:38] detectron2 INFO: Contents of args.config_file=configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml:
_BASE_: "../Base-RCNN-FPN.yaml"
MODEL:
  WEIGHTS: "https://download.pytorch.org/models/resnet18-5c106cde.pth"
  MASK_ON: True
  RESNETS:
    DEPTH: 18
    RES2_OUT_CHANNELS: 64
# DATASETS:
#   TRAIN: ("coco_2017_val",)
#   TEST: ("coco_2017_val",) #("coco_2017_test-dev",)
SOLVER:
  BASE_LR: 0.02
  CHECKPOINT_PERIOD: 7330
  # IMS_PER_BATCH: 24
  # STEPS: (40000, 53500)
  # MAX_ITER: 60000
  # WARMUP_FACTOR: 5e-4
  # WARMUP_ITERS: 2000
OUTPUT_DIR: output/faster_r18_fpn

[10/31 13:20:38] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  FILTER_EMPTY_ANNOTATIONS: True
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: ()
  PROPOSAL_FILES_TRAIN: ()
  TEST: ('coco_2017_val',)
  TRAIN: ('coco_2017_train',)
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: False
    SIZE: [0.9, 0.9]
    TYPE: relative_range
  FORMAT: BGR
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES: [[-90, 0, 90]]
    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES: [[32], [64], [128], [256], [512]]
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_fpn_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']
    NORM: 
    OUT_CHANNELS: 256
  KEYPOINT_ON: False
  LOAD_PROPOSALS: False
  MASK_ON: True
  META_ARCHITECTURE: GeneralizedRCNN
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: True
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN: [103.53, 116.28, 123.675]
  PIXEL_STD: [1.0, 1.0, 1.0]
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: False
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE: [False, False, False, False]
    DEPTH: 18
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']
    RES2_OUT_CHANNELS: 64
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.4, 0.5]
    NMS_THRESH_TEST: 0.5
    NORM: 
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))
    IOUS: (0.5, 0.6, 0.7)
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    CLS_AGNOSTIC_BBOX_REG: False
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: FastRCNNConvFCHead
    NORM: 
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    IOU_LABELS: [0, 1]
    IOU_THRESHOLDS: [0.5]
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: True
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: False
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: 
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    BOUNDARY_THRESH: -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.3, 0.7]
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: https://download.pytorch.org/models/resnet18-5c106cde.pth
OUTPUT_DIR: temp
SEED: -1
SOLVER:
  AMP:
    ENABLED: False
  BASE_LR: 0.015
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 7330
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: False
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 16
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 90000
  MOMENTUM: 0.9
  NESTEROV: False
  REFERENCE_WORLD_SIZE: 0
  STEPS: (60000, 80000)
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: False
    FLIP: True
    MAX_SIZE: 4000
    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 0
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: False
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0
[10/31 13:20:38] detectron2 INFO: Full config saved to temp/config.yaml
[10/31 13:20:38] d2.utils.env INFO: Using a generated random seed 38925851
[10/31 13:20:39] d2.engine.defaults INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten()
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[10/31 13:21:02] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 22.80 seconds.
[10/31 13:21:03] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/31 13:21:10] d2.data.build INFO: Removed 1021 images with no usable annotations. 117266 images left.
[10/31 13:21:16] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[10/31 13:21:16] d2.data.common INFO: Serializing 117266 elements to byte tensors and concatenating them all ...
[10/31 13:21:20] d2.data.common INFO: Serialized dataset takes 451.21 MiB
[10/31 13:21:20] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[10/31 13:21:20] d2.data.build INFO: Using training sampler TrainingSampler
[10/31 13:21:23] fvcore.common.checkpoint INFO: Loading checkpoint from https://download.pytorch.org/models/resnet18-5c106cde.pth
[10/31 13:21:23] fvcore.common.file_io INFO: URL https://download.pytorch.org/models/resnet18-5c106cde.pth cached in /cfarhomes/shishira/.torch/fvcore_cache/models/resnet18-5c106cde.pth
[10/31 13:21:24] fvcore.common.checkpoint INFO: Some model parameters or buffers are not found in the checkpoint:
  [34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
  [34mproposal_generator.rpn_head.conv.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.1.conv2.norm.{weight, bias}[0m
  [34mroi_heads.box_predictor.bbox_pred.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res5.0.shortcut.norm.{weight, bias}[0m
  [34mroi_heads.mask_head.mask_fcn2.{bias, weight}[0m
  [34mbackbone.fpn_lateral2.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.1.conv2.weight[0m
  [34mbackbone.bottom_up.res3.0.conv1.weight[0m
  [34mbackbone.bottom_up.res4.1.conv1.norm.{weight, bias}[0m
  [34mroi_heads.box_predictor.cls_score.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.1.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv2.norm.{weight, bias}[0m
  [34mbackbone.fpn_lateral4.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.conv2.norm.{bias, weight}[0m
  [34mproposal_generator.anchor_generator.cell_anchors.{0, 1, 3, 2, 4}[0m
  [34mbackbone.fpn_output2.{weight, bias}[0m
  [34mbackbone.fpn_lateral3.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.1.conv2.weight[0m
  [34mbackbone.bottom_up.res4.0.conv1.norm.{weight, bias}[0m
  [34mbackbone.fpn_output4.{bias, weight}[0m
  [34mroi_heads.box_head.fc2.{bias, weight}[0m
  [34mbackbone.fpn_lateral5.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res4.0.conv1.weight[0m
  [34mroi_heads.mask_head.mask_fcn1.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn3.{bias, weight}[0m
  [34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn4.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.1.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.1.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.1.conv2.weight[0m
  [34mbackbone.bottom_up.res2.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.shortcut.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.1.conv1.weight[0m
  [34mbackbone.bottom_up.res2.1.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.1.conv2.weight[0m
  [34mbackbone.bottom_up.res5.0.conv1.weight[0m
  [34mbackbone.bottom_up.res4.0.shortcut.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.1.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.1.conv1.weight[0m
  [34mroi_heads.mask_head.deconv.{bias, weight}[0m
  [34mbackbone.bottom_up.stem.conv1.weight[0m
  [34mbackbone.fpn_output5.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.1.conv2.norm.{weight, bias}[0m
  [34mroi_heads.mask_head.predictor.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.conv2.weight[0m
  [34mbackbone.bottom_up.stem.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.conv2.weight[0m
  [34mbackbone.fpn_output3.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.0.conv2.weight[0m
  [34mbackbone.bottom_up.res5.0.conv2.norm.{weight, bias}[0m
  [34mroi_heads.box_head.fc1.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.0.conv1.weight[0m
  [34mbackbone.bottom_up.res2.1.conv1.weight[0m
  [34mbackbone.bottom_up.res5.1.conv1.weight[0m
  [34mbackbone.bottom_up.res5.0.conv2.weight[0m
  [34mbackbone.bottom_up.res5.0.shortcut.weight[0m
[10/31 13:21:24] fvcore.common.checkpoint INFO: The checkpoint state_dict contains keys that are not used by the model:
  [35mconv1.weight[0m
  [35mbn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv1.weight[0m
  [35mlayer1.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv2.weight[0m
  [35mlayer1.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv1.weight[0m
  [35mlayer1.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv2.weight[0m
  [35mlayer1.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv1.weight[0m
  [35mlayer2.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv2.weight[0m
  [35mlayer2.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.downsample.0.weight[0m
  [35mlayer2.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv1.weight[0m
  [35mlayer2.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv2.weight[0m
  [35mlayer2.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv1.weight[0m
  [35mlayer3.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv2.weight[0m
  [35mlayer3.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.downsample.0.weight[0m
  [35mlayer3.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv1.weight[0m
  [35mlayer3.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv2.weight[0m
  [35mlayer3.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv1.weight[0m
  [35mlayer4.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv2.weight[0m
  [35mlayer4.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.downsample.0.weight[0m
  [35mlayer4.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv1.weight[0m
  [35mlayer4.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv2.weight[0m
  [35mlayer4.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mfc.{weight, bias}[0m
[10/31 13:42:37] detectron2 INFO: Rank of current process: 0. World size: 2
[10/31 13:42:38] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.3 (default, May 19 2020, 18:47:26) [GCC 7.3.0]
numpy                   1.19.1
detectron2              0.2.1 @/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 5.4
CUDA compiler           not available
detectron2 arch flags   /cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/_C.cpython-38-x86_64-linux-gnu.so; cannot find cuobjdump
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.6.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           True
GPU 0,1                 GeForce GTX 1080 Ti (arch=6.1)
CUDA_HOME               /opt/common/cuda/cuda-10.0.130/lib64/
Pillow                  7.2.0
torchvision             0.7.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision
torchvision arch flags  /cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                  0.1.2.post20201030
cv2                     4.4.0
----------------------  --------------------------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

[10/31 13:42:38] detectron2 INFO: Command line arguments: Namespace(config_file='configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml', dist_url='tcp://127.0.0.1:63010', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=['MODEL.MASK_ON', 'True', 'SOLVER.BASE_LR', '0.015', 'OUTPUT_DIR', 'temp'], resume=False)
[10/31 13:42:38] detectron2 INFO: Contents of args.config_file=configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml:
_BASE_: "../Base-RCNN-FPN.yaml"
MODEL:
  WEIGHTS: "https://download.pytorch.org/models/resnet18-5c106cde.pth"
  MASK_ON: True
  RESNETS:
    DEPTH: 18
    RES2_OUT_CHANNELS: 64
# DATASETS:
#   TRAIN: ("coco_2017_val",)
#   TEST: ("coco_2017_val",) #("coco_2017_test-dev",)
SOLVER:
  BASE_LR: 0.02
  CHECKPOINT_PERIOD: 7330
  # IMS_PER_BATCH: 24
  # STEPS: (40000, 53500)
  # MAX_ITER: 60000
  # WARMUP_FACTOR: 5e-4
  # WARMUP_ITERS: 2000
OUTPUT_DIR: output/faster_r18_fpn

[10/31 13:42:38] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  FILTER_EMPTY_ANNOTATIONS: True
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: ()
  PROPOSAL_FILES_TRAIN: ()
  TEST: ('coco_2017_val',)
  TRAIN: ('coco_2017_train',)
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: False
    SIZE: [0.9, 0.9]
    TYPE: relative_range
  FORMAT: BGR
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES: [[-90, 0, 90]]
    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES: [[32], [64], [128], [256], [512]]
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_fpn_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']
    NORM: 
    OUT_CHANNELS: 256
  KEYPOINT_ON: False
  LOAD_PROPOSALS: False
  MASK_ON: True
  META_ARCHITECTURE: GeneralizedRCNN
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: True
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN: [103.53, 116.28, 123.675]
  PIXEL_STD: [1.0, 1.0, 1.0]
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: False
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE: [False, False, False, False]
    DEPTH: 18
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']
    RES2_OUT_CHANNELS: 64
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.4, 0.5]
    NMS_THRESH_TEST: 0.5
    NORM: 
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))
    IOUS: (0.5, 0.6, 0.7)
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    CLS_AGNOSTIC_BBOX_REG: False
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: FastRCNNConvFCHead
    NORM: 
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    IOU_LABELS: [0, 1]
    IOU_THRESHOLDS: [0.5]
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: True
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: False
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: 
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    BOUNDARY_THRESH: -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.3, 0.7]
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: https://download.pytorch.org/models/resnet18-5c106cde.pth
OUTPUT_DIR: temp
SEED: -1
SOLVER:
  AMP:
    ENABLED: False
  BASE_LR: 0.015
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 7330
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: False
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 16
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 90000
  MOMENTUM: 0.9
  NESTEROV: False
  REFERENCE_WORLD_SIZE: 0
  STEPS: (60000, 80000)
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: False
    FLIP: True
    MAX_SIZE: 4000
    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 0
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: False
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0
[10/31 13:42:38] detectron2 INFO: Full config saved to temp/config.yaml
[10/31 13:42:38] d2.utils.env INFO: Using a generated random seed 38776617
[10/31 13:42:39] d2.engine.defaults INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten()
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[10/31 13:43:01] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 22.23 seconds.
[10/31 13:43:02] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/31 13:43:09] d2.data.build INFO: Removed 1021 images with no usable annotations. 117266 images left.
[10/31 13:43:15] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[10/31 13:43:15] d2.data.common INFO: Serializing 117266 elements to byte tensors and concatenating them all ...
[10/31 13:43:20] d2.data.common INFO: Serialized dataset takes 451.21 MiB
[10/31 13:43:20] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[10/31 13:43:20] d2.data.build INFO: Using training sampler TrainingSampler
[10/31 13:43:23] fvcore.common.checkpoint INFO: Loading checkpoint from https://download.pytorch.org/models/resnet18-5c106cde.pth
[10/31 13:43:23] fvcore.common.file_io INFO: URL https://download.pytorch.org/models/resnet18-5c106cde.pth cached in /cfarhomes/shishira/.torch/fvcore_cache/models/resnet18-5c106cde.pth
[10/31 13:43:23] fvcore.common.checkpoint INFO: Some model parameters or buffers are not found in the checkpoint:
  [34mbackbone.bottom_up.res3.1.conv2.weight[0m
  [34mbackbone.bottom_up.res4.0.conv1.norm.{weight, bias}[0m
  [34mproposal_generator.anchor_generator.cell_anchors.{1, 3, 4, 0, 2}[0m
  [34mbackbone.bottom_up.res4.1.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.conv1.weight[0m
  [34mbackbone.fpn_lateral3.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.1.conv1.norm.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn2.{weight, bias}[0m
  [34mproposal_generator.rpn_head.conv.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res4.1.conv1.weight[0m
  [34mbackbone.bottom_up.res4.0.conv1.weight[0m
  [34mbackbone.bottom_up.res5.1.conv2.norm.{weight, bias}[0m
  [34mbackbone.fpn_lateral4.{weight, bias}[0m
  [34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.conv1.weight[0m
  [34mbackbone.fpn_output2.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.1.conv2.weight[0m
  [34mproposal_generator.rpn_head.anchor_deltas.{weight, bias}[0m
  [34mroi_heads.box_head.fc1.{bias, weight}[0m
  [34mroi_heads.box_predictor.bbox_pred.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.shortcut.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.conv1.norm.{weight, bias}[0m
  [34mbackbone.fpn_lateral2.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn4.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.1.conv2.weight[0m
  [34mbackbone.bottom_up.res3.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res4.0.conv2.weight[0m
  [34mroi_heads.box_head.fc2.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.conv1.norm.{bias, weight}[0m
  [34mroi_heads.box_predictor.cls_score.{bias, weight}[0m
  [34mbackbone.fpn_output3.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.0.conv1.weight[0m
  [34mbackbone.bottom_up.res2.1.conv1.norm.{bias, weight}[0m
  [34mbackbone.fpn_lateral5.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv2.weight[0m
  [34mbackbone.bottom_up.res2.1.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.shortcut.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.1.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.1.conv1.weight[0m
  [34mbackbone.bottom_up.res5.1.conv1.weight[0m
  [34mroi_heads.mask_head.predictor.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.conv2.weight[0m
  [34mbackbone.fpn_output5.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.1.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.1.conv2.weight[0m
  [34mroi_heads.mask_head.mask_fcn3.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.stem.conv1.weight[0m
  [34mbackbone.bottom_up.res4.0.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.conv2.norm.{weight, bias}[0m
  [34mroi_heads.mask_head.deconv.{bias, weight}[0m
  [34mbackbone.fpn_output4.{weight, bias}[0m
  [34mbackbone.bottom_up.stem.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.1.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.conv2.norm.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn1.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.1.conv1.weight[0m
  [34mbackbone.bottom_up.res4.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res3.0.shortcut.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.conv2.weight[0m
[10/31 13:43:23] fvcore.common.checkpoint INFO: The checkpoint state_dict contains keys that are not used by the model:
  [35mconv1.weight[0m
  [35mbn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv1.weight[0m
  [35mlayer1.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv2.weight[0m
  [35mlayer1.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv1.weight[0m
  [35mlayer1.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv2.weight[0m
  [35mlayer1.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv1.weight[0m
  [35mlayer2.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv2.weight[0m
  [35mlayer2.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.downsample.0.weight[0m
  [35mlayer2.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv1.weight[0m
  [35mlayer2.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv2.weight[0m
  [35mlayer2.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv1.weight[0m
  [35mlayer3.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv2.weight[0m
  [35mlayer3.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.downsample.0.weight[0m
  [35mlayer3.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv1.weight[0m
  [35mlayer3.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv2.weight[0m
  [35mlayer3.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv1.weight[0m
  [35mlayer4.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv2.weight[0m
  [35mlayer4.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.downsample.0.weight[0m
  [35mlayer4.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv1.weight[0m
  [35mlayer4.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv2.weight[0m
  [35mlayer4.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mfc.{weight, bias}[0m
[10/31 13:45:42] detectron2 INFO: Rank of current process: 0. World size: 2
[10/31 13:45:43] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.3 (default, May 19 2020, 18:47:26) [GCC 7.3.0]
numpy                   1.19.1
detectron2              0.2.1 @/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 5.4
CUDA compiler           not available
detectron2 arch flags   /cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/_C.cpython-38-x86_64-linux-gnu.so; cannot find cuobjdump
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.6.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           True
GPU 0,1                 GeForce GTX 1080 Ti (arch=6.1)
CUDA_HOME               /opt/common/cuda/cuda-10.0.130/lib64/
Pillow                  7.2.0
torchvision             0.7.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision
torchvision arch flags  /cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                  0.1.2.post20201030
cv2                     4.4.0
----------------------  --------------------------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

[10/31 13:45:43] detectron2 INFO: Command line arguments: Namespace(config_file='configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml', dist_url='tcp://127.0.0.1:63010', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=['MODEL.MASK_ON', 'True', 'SOLVER.BASE_LR', '0.015', 'OUTPUT_DIR', 'temp'], resume=False)
[10/31 13:45:43] detectron2 INFO: Contents of args.config_file=configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml:
_BASE_: "../Base-RCNN-FPN.yaml"
MODEL:
  WEIGHTS: "https://download.pytorch.org/models/resnet18-5c106cde.pth"
  MASK_ON: True
  RESNETS:
    DEPTH: 18
    RES2_OUT_CHANNELS: 64
# DATASETS:
#   TRAIN: ("coco_2017_val",)
#   TEST: ("coco_2017_val",) #("coco_2017_test-dev",)
SOLVER:
  BASE_LR: 0.02
  CHECKPOINT_PERIOD: 7330
  # IMS_PER_BATCH: 24
  # STEPS: (40000, 53500)
  # MAX_ITER: 60000
  # WARMUP_FACTOR: 5e-4
  # WARMUP_ITERS: 2000
OUTPUT_DIR: output/faster_r18_fpn

[10/31 13:45:43] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  FILTER_EMPTY_ANNOTATIONS: True
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: ()
  PROPOSAL_FILES_TRAIN: ()
  TEST: ('coco_2017_val',)
  TRAIN: ('coco_2017_train',)
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: False
    SIZE: [0.9, 0.9]
    TYPE: relative_range
  FORMAT: BGR
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES: [[-90, 0, 90]]
    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES: [[32], [64], [128], [256], [512]]
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_fpn_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']
    NORM: 
    OUT_CHANNELS: 256
  KEYPOINT_ON: False
  LOAD_PROPOSALS: False
  MASK_ON: True
  META_ARCHITECTURE: GeneralizedRCNN
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: True
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN: [103.53, 116.28, 123.675]
  PIXEL_STD: [1.0, 1.0, 1.0]
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: False
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE: [False, False, False, False]
    DEPTH: 18
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']
    RES2_OUT_CHANNELS: 64
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.4, 0.5]
    NMS_THRESH_TEST: 0.5
    NORM: 
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))
    IOUS: (0.5, 0.6, 0.7)
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    CLS_AGNOSTIC_BBOX_REG: False
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: FastRCNNConvFCHead
    NORM: 
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    IOU_LABELS: [0, 1]
    IOU_THRESHOLDS: [0.5]
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: True
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: False
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: 
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    BOUNDARY_THRESH: -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.3, 0.7]
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: https://download.pytorch.org/models/resnet18-5c106cde.pth
OUTPUT_DIR: temp
SEED: -1
SOLVER:
  AMP:
    ENABLED: False
  BASE_LR: 0.015
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 7330
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: False
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 16
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 90000
  MOMENTUM: 0.9
  NESTEROV: False
  REFERENCE_WORLD_SIZE: 0
  STEPS: (60000, 80000)
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: False
    FLIP: True
    MAX_SIZE: 4000
    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 0
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: False
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0
[10/31 13:45:43] detectron2 INFO: Full config saved to temp/config.yaml
[10/31 13:45:43] d2.utils.env INFO: Using a generated random seed 43865182
[10/31 13:45:44] d2.engine.defaults INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten()
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[10/31 13:46:04] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 20.36 seconds.
[10/31 13:46:05] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/31 13:46:13] d2.data.build INFO: Removed 1021 images with no usable annotations. 117266 images left.
[10/31 13:46:18] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[10/31 13:46:18] d2.data.common INFO: Serializing 117266 elements to byte tensors and concatenating them all ...
[10/31 13:46:22] d2.data.common INFO: Serialized dataset takes 451.21 MiB
[10/31 13:46:22] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[10/31 13:46:22] d2.data.build INFO: Using training sampler TrainingSampler
[10/31 13:46:26] fvcore.common.checkpoint INFO: Loading checkpoint from https://download.pytorch.org/models/resnet18-5c106cde.pth
[10/31 13:46:26] fvcore.common.file_io INFO: URL https://download.pytorch.org/models/resnet18-5c106cde.pth cached in /cfarhomes/shishira/.torch/fvcore_cache/models/resnet18-5c106cde.pth
[10/31 13:46:26] fvcore.common.checkpoint INFO: Some model parameters or buffers are not found in the checkpoint:
  [34mbackbone.bottom_up.res3.0.shortcut.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.conv1.norm.{bias, weight}[0m
  [34mroi_heads.box_head.fc1.{weight, bias}[0m
  [34mbackbone.bottom_up.stem.conv1.weight[0m
  [34mproposal_generator.anchor_generator.cell_anchors.{3, 4, 0, 1, 2}[0m
  [34mbackbone.bottom_up.res3.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res3.1.conv1.norm.{weight, bias}[0m
  [34mbackbone.fpn_output4.{bias, weight}[0m
  [34mroi_heads.box_predictor.cls_score.{weight, bias}[0m
  [34mbackbone.fpn_output3.{bias, weight}[0m
  [34mproposal_generator.rpn_head.conv.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.1.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.conv2.weight[0m
  [34mbackbone.fpn_lateral5.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.conv2.weight[0m
  [34mbackbone.bottom_up.res5.0.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv1.norm.{weight, bias}[0m
  [34mroi_heads.mask_head.deconv.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.conv1.weight[0m
  [34mbackbone.bottom_up.res5.1.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.1.conv1.weight[0m
  [34mbackbone.bottom_up.res3.1.conv2.weight[0m
  [34mbackbone.fpn_lateral2.{bias, weight}[0m
  [34mbackbone.bottom_up.stem.conv1.norm.{weight, bias}[0m
  [34mbackbone.fpn_lateral3.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.1.conv2.weight[0m
  [34mproposal_generator.rpn_head.objectness_logits.{weight, bias}[0m
  [34mbackbone.fpn_output5.{weight, bias}[0m
  [34mroi_heads.box_predictor.bbox_pred.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn2.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn4.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.conv1.weight[0m
  [34mbackbone.bottom_up.res2.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.1.conv1.weight[0m
  [34mbackbone.bottom_up.res3.0.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.1.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv2.weight[0m
  [34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.1.conv1.norm.{bias, weight}[0m
  [34mroi_heads.box_head.fc2.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn3.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.conv1.weight[0m
  [34mbackbone.bottom_up.res2.1.conv2.weight[0m
  [34mbackbone.bottom_up.res2.0.conv2.norm.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn1.{weight, bias}[0m
  [34mbackbone.fpn_lateral4.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.shortcut.norm.{weight, bias}[0m
  [34mbackbone.fpn_output2.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.conv1.norm.{weight, bias}[0m
  [34mroi_heads.mask_head.predictor.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv1.weight[0m
  [34mbackbone.bottom_up.res4.0.shortcut.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res4.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res3.0.conv2.weight[0m
  [34mbackbone.bottom_up.res4.1.conv2.weight[0m
  [34mbackbone.bottom_up.res3.1.conv1.weight[0m
  [34mbackbone.bottom_up.res4.1.conv1.weight[0m
[10/31 13:46:26] fvcore.common.checkpoint INFO: The checkpoint state_dict contains keys that are not used by the model:
  [35mconv1.weight[0m
  [35mbn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv1.weight[0m
  [35mlayer1.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv2.weight[0m
  [35mlayer1.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv1.weight[0m
  [35mlayer1.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv2.weight[0m
  [35mlayer1.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv1.weight[0m
  [35mlayer2.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv2.weight[0m
  [35mlayer2.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.downsample.0.weight[0m
  [35mlayer2.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv1.weight[0m
  [35mlayer2.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv2.weight[0m
  [35mlayer2.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv1.weight[0m
  [35mlayer3.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv2.weight[0m
  [35mlayer3.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.downsample.0.weight[0m
  [35mlayer3.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv1.weight[0m
  [35mlayer3.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv2.weight[0m
  [35mlayer3.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv1.weight[0m
  [35mlayer4.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv2.weight[0m
  [35mlayer4.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.downsample.0.weight[0m
  [35mlayer4.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv1.weight[0m
  [35mlayer4.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv2.weight[0m
  [35mlayer4.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mfc.{weight, bias}[0m
[10/31 13:48:34] detectron2 INFO: Rank of current process: 0. World size: 2
[10/31 13:48:35] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.3 (default, May 19 2020, 18:47:26) [GCC 7.3.0]
numpy                   1.19.1
detectron2              0.2.1 @/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 5.4
CUDA compiler           not available
detectron2 arch flags   /cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/_C.cpython-38-x86_64-linux-gnu.so; cannot find cuobjdump
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.6.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           True
GPU 0,1                 GeForce GTX 1080 Ti (arch=6.1)
CUDA_HOME               /opt/common/cuda/cuda-10.0.130/lib64/
Pillow                  7.2.0
torchvision             0.7.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision
torchvision arch flags  /cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                  0.1.2.post20201030
cv2                     4.4.0
----------------------  --------------------------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

[10/31 13:48:35] detectron2 INFO: Command line arguments: Namespace(config_file='configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml', dist_url='tcp://127.0.0.1:63010', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=['MODEL.MASK_ON', 'True', 'SOLVER.BASE_LR', '0.015', 'OUTPUT_DIR', 'temp'], resume=False)
[10/31 13:48:35] detectron2 INFO: Contents of args.config_file=configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml:
_BASE_: "../Base-RCNN-FPN.yaml"
MODEL:
  WEIGHTS: "https://download.pytorch.org/models/resnet18-5c106cde.pth"
  MASK_ON: True
  RESNETS:
    DEPTH: 18
    RES2_OUT_CHANNELS: 64
# DATASETS:
#   TRAIN: ("coco_2017_val",)
#   TEST: ("coco_2017_val",) #("coco_2017_test-dev",)
SOLVER:
  BASE_LR: 0.02
  CHECKPOINT_PERIOD: 7330
  # IMS_PER_BATCH: 24
  # STEPS: (40000, 53500)
  # MAX_ITER: 60000
  # WARMUP_FACTOR: 5e-4
  # WARMUP_ITERS: 2000
OUTPUT_DIR: output/faster_r18_fpn

[10/31 13:48:35] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  FILTER_EMPTY_ANNOTATIONS: True
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: ()
  PROPOSAL_FILES_TRAIN: ()
  TEST: ('coco_2017_val',)
  TRAIN: ('coco_2017_train',)
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: False
    SIZE: [0.9, 0.9]
    TYPE: relative_range
  FORMAT: BGR
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES: [[-90, 0, 90]]
    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES: [[32], [64], [128], [256], [512]]
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_fpn_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']
    NORM: 
    OUT_CHANNELS: 256
  KEYPOINT_ON: False
  LOAD_PROPOSALS: False
  MASK_ON: True
  META_ARCHITECTURE: GeneralizedRCNN
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: True
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN: [103.53, 116.28, 123.675]
  PIXEL_STD: [1.0, 1.0, 1.0]
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: False
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE: [False, False, False, False]
    DEPTH: 18
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']
    RES2_OUT_CHANNELS: 64
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.4, 0.5]
    NMS_THRESH_TEST: 0.5
    NORM: 
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))
    IOUS: (0.5, 0.6, 0.7)
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    CLS_AGNOSTIC_BBOX_REG: False
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: FastRCNNConvFCHead
    NORM: 
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    IOU_LABELS: [0, 1]
    IOU_THRESHOLDS: [0.5]
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: True
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: False
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: 
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    BOUNDARY_THRESH: -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.3, 0.7]
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: https://download.pytorch.org/models/resnet18-5c106cde.pth
OUTPUT_DIR: temp
SEED: -1
SOLVER:
  AMP:
    ENABLED: False
  BASE_LR: 0.015
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 7330
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: False
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 16
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 90000
  MOMENTUM: 0.9
  NESTEROV: False
  REFERENCE_WORLD_SIZE: 0
  STEPS: (60000, 80000)
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: False
    FLIP: True
    MAX_SIZE: 4000
    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 0
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: False
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0
[10/31 13:48:35] detectron2 INFO: Full config saved to temp/config.yaml
[10/31 13:48:35] d2.utils.env INFO: Using a generated random seed 35349542
[10/31 13:48:35] d2.engine.defaults INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten()
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[10/31 13:48:55] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 19.85 seconds.
[10/31 13:48:56] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/31 13:49:04] d2.data.build INFO: Removed 1021 images with no usable annotations. 117266 images left.
[10/31 13:49:09] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[10/31 13:49:09] d2.data.common INFO: Serializing 117266 elements to byte tensors and concatenating them all ...
[10/31 13:49:13] d2.data.common INFO: Serialized dataset takes 451.21 MiB
[10/31 13:49:13] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[10/31 13:49:13] d2.data.build INFO: Using training sampler TrainingSampler
[10/31 13:49:17] fvcore.common.checkpoint INFO: Loading checkpoint from https://download.pytorch.org/models/resnet18-5c106cde.pth
[10/31 13:49:17] fvcore.common.file_io INFO: URL https://download.pytorch.org/models/resnet18-5c106cde.pth cached in /cfarhomes/shishira/.torch/fvcore_cache/models/resnet18-5c106cde.pth
[10/31 13:49:17] fvcore.common.checkpoint INFO: Some model parameters or buffers are not found in the checkpoint:
  [34mbackbone.fpn_lateral3.{bias, weight}[0m
  [34mbackbone.fpn_output5.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.conv2.norm.{weight, bias}[0m
  [34mbackbone.fpn_output4.{weight, bias}[0m
  [34mroi_heads.box_head.fc1.{bias, weight}[0m
  [34mproposal_generator.anchor_generator.cell_anchors.{0, 2, 3, 4, 1}[0m
  [34mbackbone.bottom_up.res2.1.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.stem.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.1.conv1.weight[0m
  [34mbackbone.bottom_up.res4.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.shortcut.norm.{weight, bias}[0m
  [34mbackbone.fpn_lateral2.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.1.conv1.norm.{weight, bias}[0m
  [34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.conv2.weight[0m
  [34mbackbone.fpn_output3.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.1.conv2.weight[0m
  [34mbackbone.bottom_up.res3.1.conv1.norm.{weight, bias}[0m
  [34mroi_heads.mask_head.mask_fcn1.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.1.conv1.weight[0m
  [34mroi_heads.box_head.fc2.{bias, weight}[0m
  [34mbackbone.fpn_lateral5.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.1.conv2.norm.{bias, weight}[0m
  [34mroi_heads.box_predictor.cls_score.{bias, weight}[0m
  [34mproposal_generator.rpn_head.conv.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.shortcut.weight[0m
  [34mroi_heads.mask_head.mask_fcn3.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.1.conv1.norm.{bias, weight}[0m
  [34mroi_heads.box_predictor.bbox_pred.{bias, weight}[0m
  [34mroi_heads.mask_head.deconv.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.shortcut.norm.{weight, bias}[0m
  [34mroi_heads.mask_head.mask_fcn4.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.1.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.0.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.conv1.weight[0m
  [34mbackbone.bottom_up.res2.0.conv1.weight[0m
  [34mbackbone.bottom_up.res3.0.conv1.norm.{bias, weight}[0m
  [34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.shortcut.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.1.conv1.weight[0m
  [34mbackbone.bottom_up.res4.1.conv2.weight[0m
  [34mbackbone.bottom_up.res2.1.conv2.weight[0m
  [34mbackbone.bottom_up.res2.0.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.conv2.weight[0m
  [34mbackbone.bottom_up.res4.1.conv1.weight[0m
  [34mbackbone.bottom_up.res4.0.conv2.norm.{bias, weight}[0m
  [34mbackbone.fpn_lateral4.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res3.0.conv1.weight[0m
  [34mroi_heads.mask_head.predictor.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn2.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.1.conv2.weight[0m
  [34mbackbone.bottom_up.res3.0.conv2.norm.{weight, bias}[0m
  [34mbackbone.fpn_output2.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res4.0.conv1.weight[0m
  [34mbackbone.bottom_up.res4.1.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.conv2.weight[0m
  [34mbackbone.bottom_up.stem.conv1.weight[0m
  [34mbackbone.bottom_up.res2.0.conv2.weight[0m
[10/31 13:49:17] fvcore.common.checkpoint INFO: The checkpoint state_dict contains keys that are not used by the model:
  [35mconv1.weight[0m
  [35mbn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv1.weight[0m
  [35mlayer1.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv2.weight[0m
  [35mlayer1.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv1.weight[0m
  [35mlayer1.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv2.weight[0m
  [35mlayer1.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv1.weight[0m
  [35mlayer2.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv2.weight[0m
  [35mlayer2.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.downsample.0.weight[0m
  [35mlayer2.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv1.weight[0m
  [35mlayer2.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv2.weight[0m
  [35mlayer2.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv1.weight[0m
  [35mlayer3.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv2.weight[0m
  [35mlayer3.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.downsample.0.weight[0m
  [35mlayer3.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv1.weight[0m
  [35mlayer3.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv2.weight[0m
  [35mlayer3.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv1.weight[0m
  [35mlayer4.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv2.weight[0m
  [35mlayer4.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.downsample.0.weight[0m
  [35mlayer4.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv1.weight[0m
  [35mlayer4.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv2.weight[0m
  [35mlayer4.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mfc.{weight, bias}[0m
[10/31 13:49:52] detectron2 INFO: Rank of current process: 0. World size: 2
[10/31 13:49:53] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.3 (default, May 19 2020, 18:47:26) [GCC 7.3.0]
numpy                   1.19.1
detectron2              0.2.1 @/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 5.4
CUDA compiler           not available
detectron2 arch flags   /cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/_C.cpython-38-x86_64-linux-gnu.so; cannot find cuobjdump
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.6.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           True
GPU 0,1                 GeForce GTX 1080 Ti (arch=6.1)
CUDA_HOME               /opt/common/cuda/cuda-10.0.130/lib64/
Pillow                  7.2.0
torchvision             0.7.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision
torchvision arch flags  /cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                  0.1.2.post20201030
cv2                     4.4.0
----------------------  --------------------------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

[10/31 13:49:53] detectron2 INFO: Command line arguments: Namespace(config_file='configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml', dist_url='tcp://127.0.0.1:63010', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=['MODEL.MASK_ON', 'True', 'SOLVER.BASE_LR', '0.015', 'OUTPUT_DIR', 'temp'], resume=False)
[10/31 13:49:53] detectron2 INFO: Contents of args.config_file=configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml:
_BASE_: "../Base-RCNN-FPN.yaml"
MODEL:
  WEIGHTS: "https://download.pytorch.org/models/resnet18-5c106cde.pth"
  MASK_ON: True
  RESNETS:
    DEPTH: 18
    RES2_OUT_CHANNELS: 64
# DATASETS:
#   TRAIN: ("coco_2017_val",)
#   TEST: ("coco_2017_val",) #("coco_2017_test-dev",)
SOLVER:
  BASE_LR: 0.02
  CHECKPOINT_PERIOD: 7330
  # IMS_PER_BATCH: 24
  # STEPS: (40000, 53500)
  # MAX_ITER: 60000
  # WARMUP_FACTOR: 5e-4
  # WARMUP_ITERS: 2000
OUTPUT_DIR: output/faster_r18_fpn

[10/31 13:49:53] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  FILTER_EMPTY_ANNOTATIONS: True
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: ()
  PROPOSAL_FILES_TRAIN: ()
  TEST: ('coco_2017_val',)
  TRAIN: ('coco_2017_train',)
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: False
    SIZE: [0.9, 0.9]
    TYPE: relative_range
  FORMAT: BGR
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES: [[-90, 0, 90]]
    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES: [[32], [64], [128], [256], [512]]
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_fpn_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']
    NORM: 
    OUT_CHANNELS: 256
  KEYPOINT_ON: False
  LOAD_PROPOSALS: False
  MASK_ON: True
  META_ARCHITECTURE: GeneralizedRCNN
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: True
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN: [103.53, 116.28, 123.675]
  PIXEL_STD: [1.0, 1.0, 1.0]
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: False
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE: [False, False, False, False]
    DEPTH: 18
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']
    RES2_OUT_CHANNELS: 64
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.4, 0.5]
    NMS_THRESH_TEST: 0.5
    NORM: 
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))
    IOUS: (0.5, 0.6, 0.7)
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    CLS_AGNOSTIC_BBOX_REG: False
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: FastRCNNConvFCHead
    NORM: 
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    IOU_LABELS: [0, 1]
    IOU_THRESHOLDS: [0.5]
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: True
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: False
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: 
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    BOUNDARY_THRESH: -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.3, 0.7]
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: https://download.pytorch.org/models/resnet18-5c106cde.pth
OUTPUT_DIR: temp
SEED: -1
SOLVER:
  AMP:
    ENABLED: False
  BASE_LR: 0.015
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 7330
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: False
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 16
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 90000
  MOMENTUM: 0.9
  NESTEROV: False
  REFERENCE_WORLD_SIZE: 0
  STEPS: (60000, 80000)
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: False
    FLIP: True
    MAX_SIZE: 4000
    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 0
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: False
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0
[10/31 13:49:53] detectron2 INFO: Full config saved to temp/config.yaml
[10/31 13:49:53] d2.utils.env INFO: Using a generated random seed 53650299
[10/31 13:49:54] d2.engine.defaults INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten()
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[10/31 13:50:14] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 20.06 seconds.
[10/31 13:50:15] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/31 13:50:22] d2.data.build INFO: Removed 1021 images with no usable annotations. 117266 images left.
[10/31 13:50:28] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[10/31 13:50:28] d2.data.common INFO: Serializing 117266 elements to byte tensors and concatenating them all ...
[10/31 13:50:32] d2.data.common INFO: Serialized dataset takes 451.21 MiB
[10/31 13:50:32] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[10/31 13:50:32] d2.data.build INFO: Using training sampler TrainingSampler
[10/31 13:50:35] fvcore.common.checkpoint INFO: Loading checkpoint from https://download.pytorch.org/models/resnet18-5c106cde.pth
[10/31 13:50:35] fvcore.common.file_io INFO: URL https://download.pytorch.org/models/resnet18-5c106cde.pth cached in /cfarhomes/shishira/.torch/fvcore_cache/models/resnet18-5c106cde.pth
[10/31 13:50:35] fvcore.common.checkpoint INFO: Some model parameters or buffers are not found in the checkpoint:
  [34mbackbone.bottom_up.res2.0.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res3.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.1.conv2.weight[0m
  [34mbackbone.fpn_lateral5.{bias, weight}[0m
  [34mproposal_generator.rpn_head.conv.{weight, bias}[0m
  [34mroi_heads.mask_head.mask_fcn3.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.shortcut.norm.{bias, weight}[0m
  [34mbackbone.fpn_lateral2.{bias, weight}[0m
  [34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
  [34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
  [34mroi_heads.box_head.fc1.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.1.conv1.norm.{bias, weight}[0m
  [34mroi_heads.box_head.fc2.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.1.conv2.norm.{weight, bias}[0m
  [34mroi_heads.mask_head.deconv.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.shortcut.norm.{weight, bias}[0m
  [34mproposal_generator.anchor_generator.cell_anchors.{0, 4, 2, 1, 3}[0m
  [34mbackbone.bottom_up.res4.1.conv1.norm.{bias, weight}[0m
  [34mroi_heads.mask_head.predictor.{weight, bias}[0m
  [34mbackbone.fpn_output3.{weight, bias}[0m
  [34mbackbone.fpn_lateral4.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.conv1.weight[0m
  [34mbackbone.bottom_up.res5.0.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv1.weight[0m
  [34mbackbone.fpn_lateral3.{bias, weight}[0m
  [34mbackbone.fpn_output4.{bias, weight}[0m
  [34mbackbone.fpn_output5.{weight, bias}[0m
  [34mbackbone.bottom_up.stem.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.1.conv1.weight[0m
  [34mbackbone.bottom_up.res3.0.conv2.norm.{weight, bias}[0m
  [34mroi_heads.mask_head.mask_fcn1.{bias, weight}[0m
  [34mbackbone.fpn_output2.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.1.conv1.weight[0m
  [34mroi_heads.mask_head.mask_fcn4.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.shortcut.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.1.conv2.norm.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn2.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.conv2.norm.{bias, weight}[0m
  [34mroi_heads.box_predictor.cls_score.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.1.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.conv2.weight[0m
  [34mbackbone.bottom_up.res5.0.conv2.weight[0m
  [34mbackbone.bottom_up.res4.1.conv2.weight[0m
  [34mroi_heads.box_predictor.bbox_pred.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.1.conv1.weight[0m
  [34mbackbone.bottom_up.res3.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res2.1.conv2.weight[0m
  [34mbackbone.bottom_up.res3.1.conv2.weight[0m
  [34mbackbone.bottom_up.res3.0.conv1.weight[0m
  [34mbackbone.bottom_up.res3.0.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.shortcut.weight[0m
  [34mbackbone.bottom_up.stem.conv1.weight[0m
  [34mbackbone.bottom_up.res4.0.conv1.weight[0m
  [34mbackbone.bottom_up.res3.0.conv2.weight[0m
  [34mbackbone.bottom_up.res2.0.conv2.weight[0m
  [34mbackbone.bottom_up.res4.1.conv1.weight[0m
  [34mbackbone.bottom_up.res5.1.conv1.norm.{bias, weight}[0m
[10/31 13:50:35] fvcore.common.checkpoint INFO: The checkpoint state_dict contains keys that are not used by the model:
  [35mconv1.weight[0m
  [35mbn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv1.weight[0m
  [35mlayer1.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv2.weight[0m
  [35mlayer1.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv1.weight[0m
  [35mlayer1.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv2.weight[0m
  [35mlayer1.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv1.weight[0m
  [35mlayer2.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv2.weight[0m
  [35mlayer2.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.downsample.0.weight[0m
  [35mlayer2.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv1.weight[0m
  [35mlayer2.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv2.weight[0m
  [35mlayer2.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv1.weight[0m
  [35mlayer3.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv2.weight[0m
  [35mlayer3.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.downsample.0.weight[0m
  [35mlayer3.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv1.weight[0m
  [35mlayer3.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv2.weight[0m
  [35mlayer3.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv1.weight[0m
  [35mlayer4.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv2.weight[0m
  [35mlayer4.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.downsample.0.weight[0m
  [35mlayer4.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv1.weight[0m
  [35mlayer4.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv2.weight[0m
  [35mlayer4.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mfc.{weight, bias}[0m
[10/31 13:50:59] detectron2 INFO: Rank of current process: 0. World size: 2
[10/31 13:51:00] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.3 (default, May 19 2020, 18:47:26) [GCC 7.3.0]
numpy                   1.19.1
detectron2              0.2.1 @/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 5.4
CUDA compiler           not available
detectron2 arch flags   /cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/_C.cpython-38-x86_64-linux-gnu.so; cannot find cuobjdump
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.6.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           True
GPU 0,1                 GeForce GTX 1080 Ti (arch=6.1)
CUDA_HOME               /opt/common/cuda/cuda-10.0.130/lib64/
Pillow                  7.2.0
torchvision             0.7.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision
torchvision arch flags  /cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                  0.1.2.post20201030
cv2                     4.4.0
----------------------  --------------------------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

[10/31 13:51:00] detectron2 INFO: Command line arguments: Namespace(config_file='configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml', dist_url='tcp://127.0.0.1:63010', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=['MODEL.MASK_ON', 'True', 'SOLVER.BASE_LR', '0.015', 'OUTPUT_DIR', 'temp'], resume=False)
[10/31 13:51:00] detectron2 INFO: Contents of args.config_file=configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml:
_BASE_: "../Base-RCNN-FPN.yaml"
MODEL:
  WEIGHTS: "https://download.pytorch.org/models/resnet18-5c106cde.pth"
  MASK_ON: True
  RESNETS:
    DEPTH: 18
    RES2_OUT_CHANNELS: 64
# DATASETS:
#   TRAIN: ("coco_2017_val",)
#   TEST: ("coco_2017_val",) #("coco_2017_test-dev",)
SOLVER:
  BASE_LR: 0.02
  CHECKPOINT_PERIOD: 7330
  # IMS_PER_BATCH: 24
  # STEPS: (40000, 53500)
  # MAX_ITER: 60000
  # WARMUP_FACTOR: 5e-4
  # WARMUP_ITERS: 2000
OUTPUT_DIR: output/faster_r18_fpn

[10/31 13:51:00] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  FILTER_EMPTY_ANNOTATIONS: True
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: ()
  PROPOSAL_FILES_TRAIN: ()
  TEST: ('coco_2017_val',)
  TRAIN: ('coco_2017_train',)
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: False
    SIZE: [0.9, 0.9]
    TYPE: relative_range
  FORMAT: BGR
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES: [[-90, 0, 90]]
    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES: [[32], [64], [128], [256], [512]]
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_fpn_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']
    NORM: 
    OUT_CHANNELS: 256
  KEYPOINT_ON: False
  LOAD_PROPOSALS: False
  MASK_ON: True
  META_ARCHITECTURE: GeneralizedRCNN
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: True
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN: [103.53, 116.28, 123.675]
  PIXEL_STD: [1.0, 1.0, 1.0]
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: False
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE: [False, False, False, False]
    DEPTH: 18
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']
    RES2_OUT_CHANNELS: 64
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.4, 0.5]
    NMS_THRESH_TEST: 0.5
    NORM: 
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))
    IOUS: (0.5, 0.6, 0.7)
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    CLS_AGNOSTIC_BBOX_REG: False
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: FastRCNNConvFCHead
    NORM: 
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    IOU_LABELS: [0, 1]
    IOU_THRESHOLDS: [0.5]
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: True
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: False
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: 
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    BOUNDARY_THRESH: -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.3, 0.7]
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: https://download.pytorch.org/models/resnet18-5c106cde.pth
OUTPUT_DIR: temp
SEED: -1
SOLVER:
  AMP:
    ENABLED: False
  BASE_LR: 0.015
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 7330
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: False
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 16
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 90000
  MOMENTUM: 0.9
  NESTEROV: False
  REFERENCE_WORLD_SIZE: 0
  STEPS: (60000, 80000)
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: False
    FLIP: True
    MAX_SIZE: 4000
    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 0
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: False
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0
[10/31 13:51:00] detectron2 INFO: Full config saved to temp/config.yaml
[10/31 13:51:00] d2.utils.env INFO: Using a generated random seed 960219
[10/31 13:51:01] d2.engine.defaults INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten()
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[10/31 13:51:21] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 20.25 seconds.
[10/31 13:51:22] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/31 13:51:30] d2.data.build INFO: Removed 1021 images with no usable annotations. 117266 images left.
[10/31 13:51:35] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[10/31 13:51:35] d2.data.common INFO: Serializing 117266 elements to byte tensors and concatenating them all ...
[10/31 13:51:39] d2.data.common INFO: Serialized dataset takes 451.21 MiB
[10/31 13:51:39] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[10/31 13:51:39] d2.data.build INFO: Using training sampler TrainingSampler
[10/31 13:51:42] fvcore.common.checkpoint INFO: Loading checkpoint from https://download.pytorch.org/models/resnet18-5c106cde.pth
[10/31 13:51:42] fvcore.common.file_io INFO: URL https://download.pytorch.org/models/resnet18-5c106cde.pth cached in /cfarhomes/shishira/.torch/fvcore_cache/models/resnet18-5c106cde.pth
[10/31 13:51:42] fvcore.common.checkpoint INFO: Some model parameters or buffers are not found in the checkpoint:
  [34mbackbone.bottom_up.res2.0.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.shortcut.norm.{bias, weight}[0m
  [34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
  [34mbackbone.bottom_up.stem.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.conv2.weight[0m
  [34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv2.weight[0m
  [34mbackbone.bottom_up.res3.1.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.1.conv1.weight[0m
  [34mbackbone.bottom_up.res3.1.conv2.norm.{weight, bias}[0m
  [34mbackbone.fpn_lateral4.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.shortcut.norm.{bias, weight}[0m
  [34mroi_heads.box_head.fc1.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.1.conv2.weight[0m
  [34mbackbone.fpn_output3.{bias, weight}[0m
  [34mbackbone.fpn_lateral2.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn4.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res4.0.conv2.norm.{weight, bias}[0m
  [34mroi_heads.mask_head.deconv.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.1.conv2.norm.{weight, bias}[0m
  [34mproposal_generator.anchor_generator.cell_anchors.{2, 0, 1, 3, 4}[0m
  [34mbackbone.bottom_up.res5.0.conv2.norm.{bias, weight}[0m
  [34mbackbone.fpn_lateral5.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.conv2.weight[0m
  [34mbackbone.bottom_up.res2.1.conv2.weight[0m
  [34mbackbone.bottom_up.res2.1.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.1.conv1.weight[0m
  [34mbackbone.bottom_up.res4.1.conv2.norm.{weight, bias}[0m
  [34mroi_heads.box_head.fc2.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.shortcut.weight[0m
  [34mbackbone.fpn_lateral3.{bias, weight}[0m
  [34mroi_heads.box_predictor.bbox_pred.{bias, weight}[0m
  [34mroi_heads.mask_head.predictor.{weight, bias}[0m
  [34mroi_heads.box_predictor.cls_score.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.1.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.fpn_output2.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.conv2.norm.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn1.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.conv1.weight[0m
  [34mbackbone.bottom_up.res2.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.fpn_output5.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.1.conv1.weight[0m
  [34mroi_heads.mask_head.mask_fcn3.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.shortcut.norm.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn2.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res5.0.conv1.weight[0m
  [34mproposal_generator.rpn_head.conv.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.conv2.weight[0m
  [34mbackbone.fpn_output4.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.0.conv1.weight[0m
  [34mbackbone.bottom_up.res4.0.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.1.conv1.weight[0m
  [34mbackbone.bottom_up.res4.1.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.conv1.weight[0m
  [34mbackbone.bottom_up.res5.1.conv2.weight[0m
  [34mbackbone.bottom_up.stem.conv1.weight[0m
  [34mbackbone.bottom_up.res3.1.conv2.weight[0m
[10/31 13:51:42] fvcore.common.checkpoint INFO: The checkpoint state_dict contains keys that are not used by the model:
  [35mconv1.weight[0m
  [35mbn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv1.weight[0m
  [35mlayer1.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv2.weight[0m
  [35mlayer1.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv1.weight[0m
  [35mlayer1.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv2.weight[0m
  [35mlayer1.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv1.weight[0m
  [35mlayer2.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv2.weight[0m
  [35mlayer2.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.downsample.0.weight[0m
  [35mlayer2.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv1.weight[0m
  [35mlayer2.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv2.weight[0m
  [35mlayer2.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv1.weight[0m
  [35mlayer3.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv2.weight[0m
  [35mlayer3.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.downsample.0.weight[0m
  [35mlayer3.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv1.weight[0m
  [35mlayer3.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv2.weight[0m
  [35mlayer3.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv1.weight[0m
  [35mlayer4.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv2.weight[0m
  [35mlayer4.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.downsample.0.weight[0m
  [35mlayer4.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv1.weight[0m
  [35mlayer4.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv2.weight[0m
  [35mlayer4.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mfc.{weight, bias}[0m
[10/31 13:53:16] detectron2 INFO: Rank of current process: 0. World size: 2
[10/31 13:53:17] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.3 (default, May 19 2020, 18:47:26) [GCC 7.3.0]
numpy                   1.19.1
detectron2              0.2.1 @/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 5.4
CUDA compiler           not available
detectron2 arch flags   /cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/_C.cpython-38-x86_64-linux-gnu.so; cannot find cuobjdump
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.6.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           True
GPU 0,1                 GeForce GTX 1080 Ti (arch=6.1)
CUDA_HOME               /opt/common/cuda/cuda-10.0.130/lib64/
Pillow                  7.2.0
torchvision             0.7.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision
torchvision arch flags  /cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                  0.1.2.post20201030
cv2                     4.4.0
----------------------  --------------------------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

[10/31 13:53:17] detectron2 INFO: Command line arguments: Namespace(config_file='configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml', dist_url='tcp://127.0.0.1:63010', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=['MODEL.MASK_ON', 'True', 'SOLVER.BASE_LR', '0.015', 'OUTPUT_DIR', 'temp'], resume=False)
[10/31 13:53:17] detectron2 INFO: Contents of args.config_file=configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml:
_BASE_: "../Base-RCNN-FPN.yaml"
MODEL:
  WEIGHTS: "https://download.pytorch.org/models/resnet18-5c106cde.pth"
  MASK_ON: True
  RESNETS:
    DEPTH: 18
    RES2_OUT_CHANNELS: 64
# DATASETS:
#   TRAIN: ("coco_2017_val",)
#   TEST: ("coco_2017_val",) #("coco_2017_test-dev",)
SOLVER:
  BASE_LR: 0.02
  CHECKPOINT_PERIOD: 7330
  # IMS_PER_BATCH: 24
  # STEPS: (40000, 53500)
  # MAX_ITER: 60000
  # WARMUP_FACTOR: 5e-4
  # WARMUP_ITERS: 2000
OUTPUT_DIR: output/faster_r18_fpn

[10/31 13:53:17] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  FILTER_EMPTY_ANNOTATIONS: True
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: ()
  PROPOSAL_FILES_TRAIN: ()
  TEST: ('coco_2017_val',)
  TRAIN: ('coco_2017_train',)
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: False
    SIZE: [0.9, 0.9]
    TYPE: relative_range
  FORMAT: BGR
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES: [[-90, 0, 90]]
    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES: [[32], [64], [128], [256], [512]]
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_fpn_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']
    NORM: 
    OUT_CHANNELS: 256
  KEYPOINT_ON: False
  LOAD_PROPOSALS: False
  MASK_ON: True
  META_ARCHITECTURE: GeneralizedRCNN
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: True
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN: [103.53, 116.28, 123.675]
  PIXEL_STD: [1.0, 1.0, 1.0]
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: False
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE: [False, False, False, False]
    DEPTH: 18
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']
    RES2_OUT_CHANNELS: 64
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.4, 0.5]
    NMS_THRESH_TEST: 0.5
    NORM: 
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))
    IOUS: (0.5, 0.6, 0.7)
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    CLS_AGNOSTIC_BBOX_REG: False
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: FastRCNNConvFCHead
    NORM: 
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    IOU_LABELS: [0, 1]
    IOU_THRESHOLDS: [0.5]
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: True
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: False
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: 
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    BOUNDARY_THRESH: -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.3, 0.7]
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: https://download.pytorch.org/models/resnet18-5c106cde.pth
OUTPUT_DIR: temp
SEED: -1
SOLVER:
  AMP:
    ENABLED: False
  BASE_LR: 0.015
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 7330
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: False
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 16
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 90000
  MOMENTUM: 0.9
  NESTEROV: False
  REFERENCE_WORLD_SIZE: 0
  STEPS: (60000, 80000)
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: False
    FLIP: True
    MAX_SIZE: 4000
    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 0
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: False
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0
[10/31 13:53:17] detectron2 INFO: Full config saved to temp/config.yaml
[10/31 13:53:17] d2.utils.env INFO: Using a generated random seed 17543121
[10/31 13:53:18] d2.engine.defaults INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten()
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[10/31 13:53:38] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 20.21 seconds.
[10/31 13:53:39] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/31 13:53:47] d2.data.build INFO: Removed 1021 images with no usable annotations. 117266 images left.
[10/31 13:53:52] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[10/31 13:53:52] d2.data.common INFO: Serializing 117266 elements to byte tensors and concatenating them all ...
[10/31 13:53:56] d2.data.common INFO: Serialized dataset takes 451.21 MiB
[10/31 13:53:56] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[10/31 13:53:56] d2.data.build INFO: Using training sampler TrainingSampler
[10/31 13:54:01] fvcore.common.checkpoint INFO: Loading checkpoint from https://download.pytorch.org/models/resnet18-5c106cde.pth
[10/31 13:54:01] fvcore.common.file_io INFO: URL https://download.pytorch.org/models/resnet18-5c106cde.pth cached in /cfarhomes/shishira/.torch/fvcore_cache/models/resnet18-5c106cde.pth
[10/31 13:54:01] fvcore.common.checkpoint INFO: Some model parameters or buffers are not found in the checkpoint:
  [34mbackbone.bottom_up.res4.0.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.1.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.conv2.weight[0m
  [34mbackbone.bottom_up.res5.0.conv1.weight[0m
  [34mproposal_generator.anchor_generator.cell_anchors.{4, 2, 1, 0, 3}[0m
  [34mbackbone.bottom_up.res4.0.conv1.weight[0m
  [34mbackbone.bottom_up.stem.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.shortcut.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv2.weight[0m
  [34mbackbone.bottom_up.res5.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res5.0.shortcut.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.1.conv2.weight[0m
  [34mbackbone.bottom_up.res4.0.conv2.weight[0m
  [34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
  [34mroi_heads.box_head.fc1.{weight, bias}[0m
  [34mroi_heads.mask_head.deconv.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv1.norm.{bias, weight}[0m
  [34mroi_heads.box_predictor.bbox_pred.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.1.conv1.norm.{bias, weight}[0m
  [34mbackbone.fpn_output4.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.conv1.weight[0m
  [34mbackbone.bottom_up.res3.1.conv2.weight[0m
  [34mroi_heads.box_head.fc2.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.1.conv1.weight[0m
  [34mroi_heads.mask_head.mask_fcn4.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.conv1.norm.{weight, bias}[0m
  [34mroi_heads.mask_head.mask_fcn3.{weight, bias}[0m
  [34mbackbone.fpn_lateral5.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn1.{weight, bias}[0m
  [34mbackbone.fpn_lateral4.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.fpn_lateral3.{bias, weight}[0m
  [34mproposal_generator.rpn_head.objectness_logits.{weight, bias}[0m
  [34mroi_heads.box_predictor.cls_score.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.1.conv1.norm.{weight, bias}[0m
  [34mroi_heads.mask_head.predictor.{weight, bias}[0m
  [34mbackbone.bottom_up.stem.conv1.weight[0m
  [34mbackbone.bottom_up.res3.0.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.1.conv2.weight[0m
  [34mbackbone.bottom_up.res5.1.conv2.weight[0m
  [34mbackbone.fpn_output5.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.conv1.norm.{weight, bias}[0m
  [34mroi_heads.mask_head.mask_fcn2.{bias, weight}[0m
  [34mbackbone.fpn_output2.{weight, bias}[0m
  [34mbackbone.fpn_lateral2.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv1.weight[0m
  [34mbackbone.bottom_up.res3.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.1.conv1.weight[0m
  [34mbackbone.bottom_up.res2.1.conv1.norm.{weight, bias}[0m
  [34mbackbone.fpn_output3.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.shortcut.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res3.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res5.1.conv1.weight[0m
  [34mbackbone.bottom_up.res2.1.conv1.weight[0m
  [34mproposal_generator.rpn_head.conv.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.conv2.weight[0m
[10/31 13:54:01] fvcore.common.checkpoint INFO: The checkpoint state_dict contains keys that are not used by the model:
  [35mconv1.weight[0m
  [35mbn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv1.weight[0m
  [35mlayer1.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv2.weight[0m
  [35mlayer1.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv1.weight[0m
  [35mlayer1.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv2.weight[0m
  [35mlayer1.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv1.weight[0m
  [35mlayer2.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv2.weight[0m
  [35mlayer2.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.downsample.0.weight[0m
  [35mlayer2.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv1.weight[0m
  [35mlayer2.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv2.weight[0m
  [35mlayer2.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv1.weight[0m
  [35mlayer3.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv2.weight[0m
  [35mlayer3.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.downsample.0.weight[0m
  [35mlayer3.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv1.weight[0m
  [35mlayer3.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv2.weight[0m
  [35mlayer3.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv1.weight[0m
  [35mlayer4.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv2.weight[0m
  [35mlayer4.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.downsample.0.weight[0m
  [35mlayer4.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv1.weight[0m
  [35mlayer4.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv2.weight[0m
  [35mlayer4.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mfc.{weight, bias}[0m
[10/31 13:56:13] detectron2 INFO: Rank of current process: 0. World size: 2
[10/31 13:56:14] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.3 (default, May 19 2020, 18:47:26) [GCC 7.3.0]
numpy                   1.19.1
detectron2              0.2.1 @/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 5.4
CUDA compiler           not available
detectron2 arch flags   /cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/_C.cpython-38-x86_64-linux-gnu.so; cannot find cuobjdump
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.6.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           True
GPU 0,1                 GeForce GTX 1080 Ti (arch=6.1)
CUDA_HOME               /opt/common/cuda/cuda-10.0.130/lib64/
Pillow                  7.2.0
torchvision             0.7.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision
torchvision arch flags  /cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                  0.1.2.post20201030
cv2                     4.4.0
----------------------  --------------------------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

[10/31 13:56:14] detectron2 INFO: Command line arguments: Namespace(config_file='configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml', dist_url='tcp://127.0.0.1:63010', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=['MODEL.MASK_ON', 'True', 'SOLVER.BASE_LR', '0.015', 'OUTPUT_DIR', 'temp'], resume=False)
[10/31 13:56:14] detectron2 INFO: Contents of args.config_file=configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml:
_BASE_: "../Base-RCNN-FPN.yaml"
MODEL:
  WEIGHTS: "https://download.pytorch.org/models/resnet18-5c106cde.pth"
  MASK_ON: True
  RESNETS:
    DEPTH: 18
    RES2_OUT_CHANNELS: 64
# DATASETS:
#   TRAIN: ("coco_2017_val",)
#   TEST: ("coco_2017_val",) #("coco_2017_test-dev",)
SOLVER:
  BASE_LR: 0.02
  CHECKPOINT_PERIOD: 7330
  # IMS_PER_BATCH: 24
  # STEPS: (40000, 53500)
  # MAX_ITER: 60000
  # WARMUP_FACTOR: 5e-4
  # WARMUP_ITERS: 2000
OUTPUT_DIR: output/faster_r18_fpn

[10/31 13:56:14] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  FILTER_EMPTY_ANNOTATIONS: True
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: ()
  PROPOSAL_FILES_TRAIN: ()
  TEST: ('coco_2017_val',)
  TRAIN: ('coco_2017_train',)
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: False
    SIZE: [0.9, 0.9]
    TYPE: relative_range
  FORMAT: BGR
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES: [[-90, 0, 90]]
    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES: [[32], [64], [128], [256], [512]]
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_fpn_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']
    NORM: 
    OUT_CHANNELS: 256
  KEYPOINT_ON: False
  LOAD_PROPOSALS: False
  MASK_ON: True
  META_ARCHITECTURE: GeneralizedRCNN
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: True
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN: [103.53, 116.28, 123.675]
  PIXEL_STD: [1.0, 1.0, 1.0]
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: False
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE: [False, False, False, False]
    DEPTH: 18
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']
    RES2_OUT_CHANNELS: 64
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.4, 0.5]
    NMS_THRESH_TEST: 0.5
    NORM: 
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))
    IOUS: (0.5, 0.6, 0.7)
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    CLS_AGNOSTIC_BBOX_REG: False
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: FastRCNNConvFCHead
    NORM: 
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    IOU_LABELS: [0, 1]
    IOU_THRESHOLDS: [0.5]
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: True
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: False
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: 
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    BOUNDARY_THRESH: -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.3, 0.7]
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: https://download.pytorch.org/models/resnet18-5c106cde.pth
OUTPUT_DIR: temp
SEED: -1
SOLVER:
  AMP:
    ENABLED: False
  BASE_LR: 0.015
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 7330
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: False
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 16
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 90000
  MOMENTUM: 0.9
  NESTEROV: False
  REFERENCE_WORLD_SIZE: 0
  STEPS: (60000, 80000)
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: False
    FLIP: True
    MAX_SIZE: 4000
    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 0
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: False
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0
[10/31 13:56:14] detectron2 INFO: Full config saved to temp/config.yaml
[10/31 13:56:14] d2.utils.env INFO: Using a generated random seed 14867735
[10/31 13:56:15] d2.engine.defaults INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten()
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[10/31 13:56:35] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 20.10 seconds.
[10/31 13:56:36] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/31 13:56:44] d2.data.build INFO: Removed 1021 images with no usable annotations. 117266 images left.
[10/31 13:56:49] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[10/31 13:56:49] d2.data.common INFO: Serializing 117266 elements to byte tensors and concatenating them all ...
[10/31 13:56:53] d2.data.common INFO: Serialized dataset takes 451.21 MiB
[10/31 13:56:53] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[10/31 13:56:53] d2.data.build INFO: Using training sampler TrainingSampler
[10/31 13:56:56] fvcore.common.checkpoint INFO: Loading checkpoint from https://download.pytorch.org/models/resnet18-5c106cde.pth
[10/31 13:56:56] fvcore.common.file_io INFO: URL https://download.pytorch.org/models/resnet18-5c106cde.pth cached in /cfarhomes/shishira/.torch/fvcore_cache/models/resnet18-5c106cde.pth
[10/31 13:56:56] fvcore.common.checkpoint INFO: Some model parameters or buffers are not found in the checkpoint:
  [34mbackbone.bottom_up.res3.0.shortcut.weight[0m
  [34mroi_heads.box_predictor.cls_score.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv2.weight[0m
  [34mproposal_generator.anchor_generator.cell_anchors.{4, 0, 2, 1, 3}[0m
  [34mbackbone.bottom_up.res5.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res3.0.conv1.norm.{weight, bias}[0m
  [34mbackbone.fpn_lateral4.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.1.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.conv2.norm.{bias, weight}[0m
  [34mbackbone.fpn_lateral3.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.1.conv2.weight[0m
  [34mbackbone.bottom_up.res5.1.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.stem.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.shortcut.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.1.conv2.norm.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn1.{bias, weight}[0m
  [34mbackbone.fpn_lateral5.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.shortcut.norm.{bias, weight}[0m
  [34mroi_heads.mask_head.predictor.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.0.conv1.norm.{weight, bias}[0m
  [34mbackbone.fpn_output5.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.1.conv2.norm.{weight, bias}[0m
  [34mbackbone.fpn_output4.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.1.conv2.weight[0m
  [34mroi_heads.box_predictor.bbox_pred.{weight, bias}[0m
  [34mbackbone.fpn_output2.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.1.conv1.weight[0m
  [34mproposal_generator.rpn_head.anchor_deltas.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.shortcut.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.conv2.weight[0m
  [34mroi_heads.mask_head.mask_fcn3.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn4.{weight, bias}[0m
  [34mproposal_generator.rpn_head.conv.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.1.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv2.norm.{weight, bias}[0m
  [34mroi_heads.mask_head.mask_fcn2.{weight, bias}[0m
  [34mbackbone.fpn_output3.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.conv1.norm.{bias, weight}[0m
  [34mroi_heads.mask_head.deconv.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.conv1.weight[0m
  [34mbackbone.fpn_lateral2.{weight, bias}[0m
  [34mroi_heads.box_head.fc1.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.1.conv2.weight[0m
  [34mbackbone.bottom_up.res4.1.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.1.conv2.weight[0m
  [34mbackbone.bottom_up.res4.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res4.0.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.1.conv1.norm.{bias, weight}[0m
  [34mroi_heads.box_head.fc2.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.1.conv1.weight[0m
  [34mbackbone.bottom_up.res3.0.conv2.weight[0m
  [34mbackbone.bottom_up.res5.0.conv1.weight[0m
  [34mproposal_generator.rpn_head.objectness_logits.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.conv2.weight[0m
  [34mbackbone.bottom_up.res2.1.conv1.weight[0m
  [34mbackbone.bottom_up.res5.1.conv1.weight[0m
  [34mbackbone.bottom_up.res4.0.conv1.weight[0m
  [34mbackbone.bottom_up.stem.conv1.weight[0m
  [34mbackbone.bottom_up.res2.0.conv1.weight[0m
[10/31 13:56:56] fvcore.common.checkpoint INFO: The checkpoint state_dict contains keys that are not used by the model:
  [35mconv1.weight[0m
  [35mbn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv1.weight[0m
  [35mlayer1.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv2.weight[0m
  [35mlayer1.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv1.weight[0m
  [35mlayer1.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv2.weight[0m
  [35mlayer1.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv1.weight[0m
  [35mlayer2.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv2.weight[0m
  [35mlayer2.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.downsample.0.weight[0m
  [35mlayer2.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv1.weight[0m
  [35mlayer2.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv2.weight[0m
  [35mlayer2.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv1.weight[0m
  [35mlayer3.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv2.weight[0m
  [35mlayer3.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.downsample.0.weight[0m
  [35mlayer3.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv1.weight[0m
  [35mlayer3.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv2.weight[0m
  [35mlayer3.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv1.weight[0m
  [35mlayer4.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv2.weight[0m
  [35mlayer4.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.downsample.0.weight[0m
  [35mlayer4.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv1.weight[0m
  [35mlayer4.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv2.weight[0m
  [35mlayer4.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mfc.{weight, bias}[0m
[10/31 13:58:19] detectron2 INFO: Rank of current process: 0. World size: 2
[10/31 13:58:20] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.3 (default, May 19 2020, 18:47:26) [GCC 7.3.0]
numpy                   1.19.1
detectron2              0.2.1 @/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 5.4
CUDA compiler           not available
detectron2 arch flags   /cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/_C.cpython-38-x86_64-linux-gnu.so; cannot find cuobjdump
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.6.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           True
GPU 0,1                 GeForce GTX 1080 Ti (arch=6.1)
CUDA_HOME               /opt/common/cuda/cuda-10.0.130/lib64/
Pillow                  7.2.0
torchvision             0.7.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision
torchvision arch flags  /cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                  0.1.2.post20201030
cv2                     4.4.0
----------------------  --------------------------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

[10/31 13:58:20] detectron2 INFO: Command line arguments: Namespace(config_file='configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml', dist_url='tcp://127.0.0.1:63010', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=['MODEL.MASK_ON', 'True', 'SOLVER.BASE_LR', '0.015', 'OUTPUT_DIR', 'temp'], resume=False)
[10/31 13:58:20] detectron2 INFO: Contents of args.config_file=configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml:
_BASE_: "../Base-RCNN-FPN.yaml"
MODEL:
  WEIGHTS: "https://download.pytorch.org/models/resnet18-5c106cde.pth"
  MASK_ON: True
  RESNETS:
    DEPTH: 18
    RES2_OUT_CHANNELS: 64
# DATASETS:
#   TRAIN: ("coco_2017_val",)
#   TEST: ("coco_2017_val",) #("coco_2017_test-dev",)
SOLVER:
  BASE_LR: 0.02
  CHECKPOINT_PERIOD: 7330
  # IMS_PER_BATCH: 24
  # STEPS: (40000, 53500)
  # MAX_ITER: 60000
  # WARMUP_FACTOR: 5e-4
  # WARMUP_ITERS: 2000
OUTPUT_DIR: output/faster_r18_fpn

[10/31 13:58:20] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  FILTER_EMPTY_ANNOTATIONS: True
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: ()
  PROPOSAL_FILES_TRAIN: ()
  TEST: ('coco_2017_val',)
  TRAIN: ('coco_2017_train',)
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: False
    SIZE: [0.9, 0.9]
    TYPE: relative_range
  FORMAT: BGR
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES: [[-90, 0, 90]]
    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES: [[32], [64], [128], [256], [512]]
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_fpn_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']
    NORM: 
    OUT_CHANNELS: 256
  KEYPOINT_ON: False
  LOAD_PROPOSALS: False
  MASK_ON: True
  META_ARCHITECTURE: GeneralizedRCNN
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: True
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN: [103.53, 116.28, 123.675]
  PIXEL_STD: [1.0, 1.0, 1.0]
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: False
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE: [False, False, False, False]
    DEPTH: 18
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']
    RES2_OUT_CHANNELS: 64
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.4, 0.5]
    NMS_THRESH_TEST: 0.5
    NORM: 
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))
    IOUS: (0.5, 0.6, 0.7)
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    CLS_AGNOSTIC_BBOX_REG: False
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: FastRCNNConvFCHead
    NORM: 
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    IOU_LABELS: [0, 1]
    IOU_THRESHOLDS: [0.5]
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: True
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: False
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: 
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    BOUNDARY_THRESH: -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.3, 0.7]
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: https://download.pytorch.org/models/resnet18-5c106cde.pth
OUTPUT_DIR: temp
SEED: -1
SOLVER:
  AMP:
    ENABLED: False
  BASE_LR: 0.015
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 7330
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: False
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 16
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 90000
  MOMENTUM: 0.9
  NESTEROV: False
  REFERENCE_WORLD_SIZE: 0
  STEPS: (60000, 80000)
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: False
    FLIP: True
    MAX_SIZE: 4000
    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 0
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: False
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0
[10/31 13:58:20] detectron2 INFO: Full config saved to temp/config.yaml
[10/31 13:58:20] d2.utils.env INFO: Using a generated random seed 20725941
[10/31 13:58:21] d2.engine.defaults INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten()
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[10/31 13:58:41] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 19.82 seconds.
[10/31 13:58:42] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/31 13:58:49] d2.data.build INFO: Removed 1021 images with no usable annotations. 117266 images left.
[10/31 13:58:55] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[10/31 13:58:55] d2.data.common INFO: Serializing 117266 elements to byte tensors and concatenating them all ...
[10/31 13:58:59] d2.data.common INFO: Serialized dataset takes 451.21 MiB
[10/31 13:58:59] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[10/31 13:58:59] d2.data.build INFO: Using training sampler TrainingSampler
[10/31 13:59:03] fvcore.common.checkpoint INFO: Loading checkpoint from https://download.pytorch.org/models/resnet18-5c106cde.pth
[10/31 13:59:03] fvcore.common.file_io INFO: URL https://download.pytorch.org/models/resnet18-5c106cde.pth cached in /cfarhomes/shishira/.torch/fvcore_cache/models/resnet18-5c106cde.pth
[10/31 13:59:03] fvcore.common.checkpoint INFO: Some model parameters or buffers are not found in the checkpoint:
  [34mbackbone.bottom_up.res3.0.conv2.norm.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn2.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.stem.conv1.norm.{bias, weight}[0m
  [34mbackbone.fpn_output4.{bias, weight}[0m
  [34mproposal_generator.rpn_head.anchor_deltas.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.shortcut.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.1.conv1.weight[0m
  [34mbackbone.fpn_lateral3.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.conv2.norm.{bias, weight}[0m
  [34mbackbone.fpn_lateral5.{bias, weight}[0m
  [34mbackbone.fpn_output3.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.conv2.weight[0m
  [34mbackbone.bottom_up.res5.0.conv1.norm.{weight, bias}[0m
  [34mroi_heads.mask_head.mask_fcn1.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.1.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.stem.conv1.weight[0m
  [34mroi_heads.box_predictor.bbox_pred.{bias, weight}[0m
  [34mroi_heads.box_predictor.cls_score.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.0.conv2.norm.{bias, weight}[0m
  [34mproposal_generator.anchor_generator.cell_anchors.{3, 0, 1, 4, 2}[0m
  [34mbackbone.bottom_up.res2.1.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.1.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.1.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.conv1.weight[0m
  [34mbackbone.bottom_up.res3.0.shortcut.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.1.conv2.weight[0m
  [34mbackbone.bottom_up.res3.0.conv2.weight[0m
  [34mbackbone.bottom_up.res5.1.conv1.weight[0m
  [34mbackbone.bottom_up.res2.0.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.fpn_output2.{bias, weight}[0m
  [34mroi_heads.mask_head.predictor.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.conv1.weight[0m
  [34mroi_heads.box_head.fc2.{weight, bias}[0m
  [34mproposal_generator.rpn_head.conv.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res5.1.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.1.conv2.weight[0m
  [34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.1.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.shortcut.norm.{weight, bias}[0m
  [34mroi_heads.mask_head.mask_fcn3.{bias, weight}[0m
  [34mbackbone.fpn_lateral2.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res2.0.conv2.weight[0m
  [34mbackbone.bottom_up.res5.0.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.conv1.weight[0m
  [34mroi_heads.mask_head.deconv.{weight, bias}[0m
  [34mbackbone.fpn_lateral4.{weight, bias}[0m
  [34mroi_heads.box_head.fc1.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.1.conv1.weight[0m
  [34mroi_heads.mask_head.mask_fcn4.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.1.conv2.weight[0m
  [34mbackbone.fpn_output5.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.1.conv1.weight[0m
  [34mbackbone.bottom_up.res2.0.conv1.weight[0m
  [34mbackbone.bottom_up.res5.0.conv2.weight[0m
  [34mbackbone.bottom_up.res3.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res5.1.conv2.weight[0m
[10/31 13:59:03] fvcore.common.checkpoint INFO: The checkpoint state_dict contains keys that are not used by the model:
  [35mconv1.weight[0m
  [35mbn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv1.weight[0m
  [35mlayer1.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv2.weight[0m
  [35mlayer1.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv1.weight[0m
  [35mlayer1.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv2.weight[0m
  [35mlayer1.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv1.weight[0m
  [35mlayer2.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv2.weight[0m
  [35mlayer2.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.downsample.0.weight[0m
  [35mlayer2.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv1.weight[0m
  [35mlayer2.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv2.weight[0m
  [35mlayer2.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv1.weight[0m
  [35mlayer3.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv2.weight[0m
  [35mlayer3.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.downsample.0.weight[0m
  [35mlayer3.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv1.weight[0m
  [35mlayer3.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv2.weight[0m
  [35mlayer3.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv1.weight[0m
  [35mlayer4.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv2.weight[0m
  [35mlayer4.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.downsample.0.weight[0m
  [35mlayer4.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv1.weight[0m
  [35mlayer4.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv2.weight[0m
  [35mlayer4.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mfc.{weight, bias}[0m
[10/31 14:01:50] detectron2 INFO: Rank of current process: 0. World size: 2
[10/31 14:01:51] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.3 (default, May 19 2020, 18:47:26) [GCC 7.3.0]
numpy                   1.19.1
detectron2              0.2.1 @/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 5.4
CUDA compiler           not available
detectron2 arch flags   /cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/_C.cpython-38-x86_64-linux-gnu.so; cannot find cuobjdump
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.6.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           True
GPU 0,1                 GeForce GTX 1080 Ti (arch=6.1)
CUDA_HOME               /opt/common/cuda/cuda-10.0.130/lib64/
Pillow                  7.2.0
torchvision             0.7.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision
torchvision arch flags  /cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                  0.1.2.post20201030
cv2                     4.4.0
----------------------  --------------------------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

[10/31 14:01:51] detectron2 INFO: Command line arguments: Namespace(config_file='configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml', dist_url='tcp://127.0.0.1:63010', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=['MODEL.MASK_ON', 'True', 'SOLVER.BASE_LR', '0.015', 'OUTPUT_DIR', 'temp'], resume=False)
[10/31 14:01:51] detectron2 INFO: Contents of args.config_file=configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml:
_BASE_: "../Base-RCNN-FPN.yaml"
MODEL:
  WEIGHTS: "https://download.pytorch.org/models/resnet18-5c106cde.pth"
  MASK_ON: True
  RESNETS:
    DEPTH: 18
    RES2_OUT_CHANNELS: 64
# DATASETS:
#   TRAIN: ("coco_2017_val",)
#   TEST: ("coco_2017_val",) #("coco_2017_test-dev",)
SOLVER:
  BASE_LR: 0.02
  CHECKPOINT_PERIOD: 7330
  # IMS_PER_BATCH: 24
  # STEPS: (40000, 53500)
  # MAX_ITER: 60000
  # WARMUP_FACTOR: 5e-4
  # WARMUP_ITERS: 2000
OUTPUT_DIR: output/faster_r18_fpn

[10/31 14:01:51] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  FILTER_EMPTY_ANNOTATIONS: True
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: ()
  PROPOSAL_FILES_TRAIN: ()
  TEST: ('coco_2017_val',)
  TRAIN: ('coco_2017_train',)
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: False
    SIZE: [0.9, 0.9]
    TYPE: relative_range
  FORMAT: BGR
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES: [[-90, 0, 90]]
    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES: [[32], [64], [128], [256], [512]]
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_fpn_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']
    NORM: 
    OUT_CHANNELS: 256
  KEYPOINT_ON: False
  LOAD_PROPOSALS: False
  MASK_ON: True
  META_ARCHITECTURE: GeneralizedRCNN
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: True
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN: [103.53, 116.28, 123.675]
  PIXEL_STD: [1.0, 1.0, 1.0]
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: False
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE: [False, False, False, False]
    DEPTH: 18
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']
    RES2_OUT_CHANNELS: 64
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.4, 0.5]
    NMS_THRESH_TEST: 0.5
    NORM: 
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))
    IOUS: (0.5, 0.6, 0.7)
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    CLS_AGNOSTIC_BBOX_REG: False
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: FastRCNNConvFCHead
    NORM: 
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    IOU_LABELS: [0, 1]
    IOU_THRESHOLDS: [0.5]
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: True
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: False
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: 
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    BOUNDARY_THRESH: -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.3, 0.7]
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: https://download.pytorch.org/models/resnet18-5c106cde.pth
OUTPUT_DIR: temp
SEED: -1
SOLVER:
  AMP:
    ENABLED: False
  BASE_LR: 0.015
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 7330
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: False
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 16
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 90000
  MOMENTUM: 0.9
  NESTEROV: False
  REFERENCE_WORLD_SIZE: 0
  STEPS: (60000, 80000)
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: False
    FLIP: True
    MAX_SIZE: 4000
    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 0
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: False
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0
[10/31 14:01:51] detectron2 INFO: Full config saved to temp/config.yaml
[10/31 14:01:51] d2.utils.env INFO: Using a generated random seed 51660614
[10/31 14:01:52] d2.engine.defaults INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten()
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[10/31 14:02:11] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 19.80 seconds.
[10/31 14:02:13] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/31 14:02:20] d2.data.build INFO: Removed 1021 images with no usable annotations. 117266 images left.
[10/31 14:02:26] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[10/31 14:02:26] d2.data.common INFO: Serializing 117266 elements to byte tensors and concatenating them all ...
[10/31 14:02:30] d2.data.common INFO: Serialized dataset takes 451.21 MiB
[10/31 14:02:30] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[10/31 14:02:30] d2.data.build INFO: Using training sampler TrainingSampler
[10/31 14:02:34] fvcore.common.checkpoint INFO: Loading checkpoint from https://download.pytorch.org/models/resnet18-5c106cde.pth
[10/31 14:02:34] fvcore.common.file_io INFO: URL https://download.pytorch.org/models/resnet18-5c106cde.pth cached in /cfarhomes/shishira/.torch/fvcore_cache/models/resnet18-5c106cde.pth
[10/31 14:02:34] fvcore.common.checkpoint INFO: Some model parameters or buffers are not found in the checkpoint:
  [34mbackbone.bottom_up.res4.1.conv2.weight[0m
  [34mbackbone.bottom_up.res2.1.conv2.norm.{weight, bias}[0m
  [34mroi_heads.box_head.fc2.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn4.{weight, bias}[0m
  [34mbackbone.fpn_output2.{weight, bias}[0m
  [34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv2.norm.{weight, bias}[0m
  [34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.1.conv1.weight[0m
  [34mroi_heads.mask_head.mask_fcn1.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.1.conv1.norm.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn2.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.conv1.norm.{weight, bias}[0m
  [34mroi_heads.box_predictor.bbox_pred.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.conv2.norm.{weight, bias}[0m
  [34mbackbone.fpn_lateral3.{bias, weight}[0m
  [34mproposal_generator.rpn_head.conv.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.1.conv1.weight[0m
  [34mbackbone.bottom_up.stem.conv1.weight[0m
  [34mbackbone.bottom_up.res5.0.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.shortcut.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.1.conv1.weight[0m
  [34mbackbone.bottom_up.stem.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.1.conv1.norm.{bias, weight}[0m
  [34mroi_heads.mask_head.deconv.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.1.conv2.weight[0m
  [34mproposal_generator.anchor_generator.cell_anchors.{1, 4, 3, 2, 0}[0m
  [34mroi_heads.box_head.fc1.{bias, weight}[0m
  [34mbackbone.fpn_lateral4.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.1.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.shortcut.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res4.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.shortcut.norm.{weight, bias}[0m
  [34mroi_heads.box_predictor.cls_score.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.conv2.weight[0m
  [34mbackbone.bottom_up.res3.0.shortcut.weight[0m
  [34mbackbone.fpn_output5.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.conv2.weight[0m
  [34mbackbone.fpn_output3.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.conv2.norm.{bias, weight}[0m
  [34mroi_heads.mask_head.predictor.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.1.conv2.weight[0m
  [34mbackbone.bottom_up.res5.1.conv2.weight[0m
  [34mbackbone.fpn_output4.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.1.conv2.norm.{weight, bias}[0m
  [34mroi_heads.mask_head.mask_fcn3.{weight, bias}[0m
  [34mbackbone.fpn_lateral5.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.1.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.conv1.weight[0m
  [34mbackbone.bottom_up.res2.0.conv1.weight[0m
  [34mbackbone.fpn_lateral2.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.1.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv2.weight[0m
  [34mbackbone.bottom_up.res5.0.conv1.weight[0m
  [34mbackbone.bottom_up.res4.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res4.1.conv1.weight[0m
  [34mbackbone.bottom_up.res5.0.conv2.weight[0m
  [34mbackbone.bottom_up.res4.0.conv1.weight[0m
[10/31 14:02:34] fvcore.common.checkpoint INFO: The checkpoint state_dict contains keys that are not used by the model:
  [35mconv1.weight[0m
  [35mbn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv1.weight[0m
  [35mlayer1.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv2.weight[0m
  [35mlayer1.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv1.weight[0m
  [35mlayer1.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv2.weight[0m
  [35mlayer1.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv1.weight[0m
  [35mlayer2.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv2.weight[0m
  [35mlayer2.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.downsample.0.weight[0m
  [35mlayer2.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv1.weight[0m
  [35mlayer2.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv2.weight[0m
  [35mlayer2.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv1.weight[0m
  [35mlayer3.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv2.weight[0m
  [35mlayer3.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.downsample.0.weight[0m
  [35mlayer3.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv1.weight[0m
  [35mlayer3.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv2.weight[0m
  [35mlayer3.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv1.weight[0m
  [35mlayer4.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv2.weight[0m
  [35mlayer4.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.downsample.0.weight[0m
  [35mlayer4.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv1.weight[0m
  [35mlayer4.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv2.weight[0m
  [35mlayer4.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mfc.{weight, bias}[0m
[10/31 14:03:11] detectron2 INFO: Rank of current process: 0. World size: 2
[10/31 14:03:12] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.3 (default, May 19 2020, 18:47:26) [GCC 7.3.0]
numpy                   1.19.1
detectron2              0.2.1 @/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 5.4
CUDA compiler           not available
detectron2 arch flags   /cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/_C.cpython-38-x86_64-linux-gnu.so; cannot find cuobjdump
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.6.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           True
GPU 0,1                 GeForce GTX 1080 Ti (arch=6.1)
CUDA_HOME               /opt/common/cuda/cuda-10.0.130/lib64/
Pillow                  7.2.0
torchvision             0.7.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision
torchvision arch flags  /cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                  0.1.2.post20201030
cv2                     4.4.0
----------------------  --------------------------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

[10/31 14:03:12] detectron2 INFO: Command line arguments: Namespace(config_file='configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml', dist_url='tcp://127.0.0.1:63010', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=['MODEL.MASK_ON', 'True', 'SOLVER.BASE_LR', '0.015', 'OUTPUT_DIR', 'temp'], resume=False)
[10/31 14:03:12] detectron2 INFO: Contents of args.config_file=configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml:
_BASE_: "../Base-RCNN-FPN.yaml"
MODEL:
  WEIGHTS: "https://download.pytorch.org/models/resnet18-5c106cde.pth"
  MASK_ON: True
  RESNETS:
    DEPTH: 18
    RES2_OUT_CHANNELS: 64
# DATASETS:
#   TRAIN: ("coco_2017_val",)
#   TEST: ("coco_2017_val",) #("coco_2017_test-dev",)
SOLVER:
  BASE_LR: 0.02
  CHECKPOINT_PERIOD: 7330
  # IMS_PER_BATCH: 24
  # STEPS: (40000, 53500)
  # MAX_ITER: 60000
  # WARMUP_FACTOR: 5e-4
  # WARMUP_ITERS: 2000
OUTPUT_DIR: output/faster_r18_fpn

[10/31 14:03:12] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  FILTER_EMPTY_ANNOTATIONS: True
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: ()
  PROPOSAL_FILES_TRAIN: ()
  TEST: ('coco_2017_val',)
  TRAIN: ('coco_2017_train',)
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: False
    SIZE: [0.9, 0.9]
    TYPE: relative_range
  FORMAT: BGR
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES: [[-90, 0, 90]]
    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES: [[32], [64], [128], [256], [512]]
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_fpn_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']
    NORM: 
    OUT_CHANNELS: 256
  KEYPOINT_ON: False
  LOAD_PROPOSALS: False
  MASK_ON: True
  META_ARCHITECTURE: GeneralizedRCNN
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: True
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN: [103.53, 116.28, 123.675]
  PIXEL_STD: [1.0, 1.0, 1.0]
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: False
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE: [False, False, False, False]
    DEPTH: 18
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']
    RES2_OUT_CHANNELS: 64
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.4, 0.5]
    NMS_THRESH_TEST: 0.5
    NORM: 
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))
    IOUS: (0.5, 0.6, 0.7)
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    CLS_AGNOSTIC_BBOX_REG: False
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: FastRCNNConvFCHead
    NORM: 
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    IOU_LABELS: [0, 1]
    IOU_THRESHOLDS: [0.5]
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: True
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: False
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: 
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    BOUNDARY_THRESH: -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.3, 0.7]
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: https://download.pytorch.org/models/resnet18-5c106cde.pth
OUTPUT_DIR: temp
SEED: -1
SOLVER:
  AMP:
    ENABLED: False
  BASE_LR: 0.015
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 7330
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: False
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 16
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 90000
  MOMENTUM: 0.9
  NESTEROV: False
  REFERENCE_WORLD_SIZE: 0
  STEPS: (60000, 80000)
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: False
    FLIP: True
    MAX_SIZE: 4000
    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 0
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: False
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0
[10/31 14:03:12] detectron2 INFO: Full config saved to temp/config.yaml
[10/31 14:03:12] d2.utils.env INFO: Using a generated random seed 12612855
[10/31 14:03:13] d2.engine.defaults INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten()
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[10/31 14:03:33] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 20.40 seconds.
[10/31 14:03:34] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/31 14:03:42] d2.data.build INFO: Removed 1021 images with no usable annotations. 117266 images left.
[10/31 14:03:47] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[10/31 14:03:47] d2.data.common INFO: Serializing 117266 elements to byte tensors and concatenating them all ...
[10/31 14:03:52] d2.data.common INFO: Serialized dataset takes 451.21 MiB
[10/31 14:03:52] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[10/31 14:03:52] d2.data.build INFO: Using training sampler TrainingSampler
[10/31 14:03:55] fvcore.common.checkpoint INFO: Loading checkpoint from https://download.pytorch.org/models/resnet18-5c106cde.pth
[10/31 14:03:55] fvcore.common.file_io INFO: URL https://download.pytorch.org/models/resnet18-5c106cde.pth cached in /cfarhomes/shishira/.torch/fvcore_cache/models/resnet18-5c106cde.pth
[10/31 14:03:55] fvcore.common.checkpoint INFO: Some model parameters or buffers are not found in the checkpoint:
  [34mbackbone.bottom_up.res2.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.1.conv2.weight[0m
  [34mbackbone.bottom_up.res5.0.shortcut.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.0.conv2.norm.{bias, weight}[0m
  [34mproposal_generator.anchor_generator.cell_anchors.{3, 4, 0, 2, 1}[0m
  [34mbackbone.bottom_up.res2.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.fpn_lateral4.{bias, weight}[0m
  [34mproposal_generator.rpn_head.conv.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn4.{bias, weight}[0m
  [34mproposal_generator.rpn_head.objectness_logits.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.1.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.fpn_output4.{weight, bias}[0m
  [34mbackbone.fpn_lateral2.{bias, weight}[0m
  [34mroi_heads.box_head.fc1.{weight, bias}[0m
  [34mroi_heads.mask_head.mask_fcn2.{weight, bias}[0m
  [34mroi_heads.mask_head.mask_fcn3.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.1.conv2.norm.{weight, bias}[0m
  [34mroi_heads.mask_head.deconv.{bias, weight}[0m
  [34mbackbone.fpn_output2.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.shortcut.weight[0m
  [34mbackbone.fpn_lateral3.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.1.conv1.norm.{bias, weight}[0m
  [34mroi_heads.box_predictor.cls_score.{bias, weight}[0m
  [34mbackbone.fpn_output3.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.conv2.norm.{bias, weight}[0m
  [34mroi_heads.box_head.fc2.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.conv1.weight[0m
  [34mbackbone.bottom_up.res3.1.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv2.weight[0m
  [34mbackbone.bottom_up.res4.1.conv2.weight[0m
  [34mroi_heads.mask_head.predictor.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.1.conv1.weight[0m
  [34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.1.conv2.weight[0m
  [34mbackbone.fpn_lateral5.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn1.{weight, bias}[0m
  [34mroi_heads.box_predictor.bbox_pred.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.1.conv1.weight[0m
  [34mbackbone.bottom_up.res5.1.conv2.weight[0m
  [34mbackbone.bottom_up.res3.0.conv2.weight[0m
  [34mbackbone.bottom_up.res2.1.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.conv2.weight[0m
  [34mbackbone.bottom_up.res2.0.conv1.weight[0m
  [34mbackbone.bottom_up.res3.1.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.1.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.fpn_output5.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.shortcut.weight[0m
  [34mbackbone.bottom_up.stem.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.shortcut.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.shortcut.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.stem.conv1.weight[0m
  [34mbackbone.bottom_up.res4.0.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res5.0.conv2.weight[0m
  [34mbackbone.bottom_up.res3.1.conv1.weight[0m
  [34mbackbone.bottom_up.res3.0.conv1.weight[0m
  [34mbackbone.bottom_up.res5.0.conv1.weight[0m
  [34mbackbone.bottom_up.res2.1.conv1.weight[0m
[10/31 14:03:55] fvcore.common.checkpoint INFO: The checkpoint state_dict contains keys that are not used by the model:
  [35mconv1.weight[0m
  [35mbn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv1.weight[0m
  [35mlayer1.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv2.weight[0m
  [35mlayer1.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv1.weight[0m
  [35mlayer1.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv2.weight[0m
  [35mlayer1.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv1.weight[0m
  [35mlayer2.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv2.weight[0m
  [35mlayer2.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.downsample.0.weight[0m
  [35mlayer2.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv1.weight[0m
  [35mlayer2.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv2.weight[0m
  [35mlayer2.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv1.weight[0m
  [35mlayer3.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv2.weight[0m
  [35mlayer3.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.downsample.0.weight[0m
  [35mlayer3.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv1.weight[0m
  [35mlayer3.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv2.weight[0m
  [35mlayer3.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv1.weight[0m
  [35mlayer4.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv2.weight[0m
  [35mlayer4.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.downsample.0.weight[0m
  [35mlayer4.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv1.weight[0m
  [35mlayer4.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv2.weight[0m
  [35mlayer4.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mfc.{weight, bias}[0m
[10/31 14:03:56] d2.engine.train_loop INFO: Starting training from iteration 0
[10/31 14:04:11] d2.engine.hooks INFO: Total training time: 0:00:14 (0:00:00 on hooks)
[10/31 14:04:11] d2.utils.events INFO:  iter: 0    lr: N/A  max_mem: 751M
[10/31 14:04:20] detectron2 INFO: Rank of current process: 0. World size: 2
[10/31 14:04:21] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.3 (default, May 19 2020, 18:47:26) [GCC 7.3.0]
numpy                   1.19.1
detectron2              0.2.1 @/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 5.4
CUDA compiler           not available
detectron2 arch flags   /cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/_C.cpython-38-x86_64-linux-gnu.so; cannot find cuobjdump
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.6.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           True
GPU 0,1                 GeForce GTX 1080 Ti (arch=6.1)
CUDA_HOME               /opt/common/cuda/cuda-10.0.130/lib64/
Pillow                  7.2.0
torchvision             0.7.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision
torchvision arch flags  /cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                  0.1.2.post20201030
cv2                     4.4.0
----------------------  --------------------------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

[10/31 14:04:21] detectron2 INFO: Command line arguments: Namespace(config_file='configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml', dist_url='tcp://127.0.0.1:63010', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=['MODEL.MASK_ON', 'True', 'SOLVER.BASE_LR', '0.015', 'OUTPUT_DIR', 'temp'], resume=False)
[10/31 14:04:21] detectron2 INFO: Contents of args.config_file=configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml:
_BASE_: "../Base-RCNN-FPN.yaml"
MODEL:
  WEIGHTS: "https://download.pytorch.org/models/resnet18-5c106cde.pth"
  MASK_ON: True
  RESNETS:
    DEPTH: 18
    RES2_OUT_CHANNELS: 64
# DATASETS:
#   TRAIN: ("coco_2017_val",)
#   TEST: ("coco_2017_val",) #("coco_2017_test-dev",)
SOLVER:
  BASE_LR: 0.02
  CHECKPOINT_PERIOD: 7330
  # IMS_PER_BATCH: 24
  # STEPS: (40000, 53500)
  # MAX_ITER: 60000
  # WARMUP_FACTOR: 5e-4
  # WARMUP_ITERS: 2000
OUTPUT_DIR: output/faster_r18_fpn

[10/31 14:04:21] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  FILTER_EMPTY_ANNOTATIONS: True
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: ()
  PROPOSAL_FILES_TRAIN: ()
  TEST: ('coco_2017_val',)
  TRAIN: ('coco_2017_train',)
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: False
    SIZE: [0.9, 0.9]
    TYPE: relative_range
  FORMAT: BGR
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES: [[-90, 0, 90]]
    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES: [[32], [64], [128], [256], [512]]
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_fpn_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']
    NORM: 
    OUT_CHANNELS: 256
  KEYPOINT_ON: False
  LOAD_PROPOSALS: False
  MASK_ON: True
  META_ARCHITECTURE: GeneralizedRCNN
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: True
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN: [103.53, 116.28, 123.675]
  PIXEL_STD: [1.0, 1.0, 1.0]
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: False
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE: [False, False, False, False]
    DEPTH: 18
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']
    RES2_OUT_CHANNELS: 64
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.4, 0.5]
    NMS_THRESH_TEST: 0.5
    NORM: 
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))
    IOUS: (0.5, 0.6, 0.7)
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    CLS_AGNOSTIC_BBOX_REG: False
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: FastRCNNConvFCHead
    NORM: 
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    IOU_LABELS: [0, 1]
    IOU_THRESHOLDS: [0.5]
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: True
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: False
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: 
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    BOUNDARY_THRESH: -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.3, 0.7]
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: https://download.pytorch.org/models/resnet18-5c106cde.pth
OUTPUT_DIR: temp
SEED: -1
SOLVER:
  AMP:
    ENABLED: False
  BASE_LR: 0.015
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 7330
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: False
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 16
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 90000
  MOMENTUM: 0.9
  NESTEROV: False
  REFERENCE_WORLD_SIZE: 0
  STEPS: (60000, 80000)
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: False
    FLIP: True
    MAX_SIZE: 4000
    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 0
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: False
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0
[10/31 14:04:21] detectron2 INFO: Full config saved to temp/config.yaml
[10/31 14:04:21] d2.utils.env INFO: Using a generated random seed 21726579
[10/31 14:04:22] d2.engine.defaults INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten()
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[10/31 14:04:42] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 19.88 seconds.
[10/31 14:04:43] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/31 14:04:50] d2.data.build INFO: Removed 1021 images with no usable annotations. 117266 images left.
[10/31 14:04:56] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[10/31 14:04:56] d2.data.common INFO: Serializing 117266 elements to byte tensors and concatenating them all ...
[10/31 14:05:00] d2.data.common INFO: Serialized dataset takes 451.21 MiB
[10/31 14:05:00] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[10/31 14:05:00] d2.data.build INFO: Using training sampler TrainingSampler
[10/31 14:05:03] fvcore.common.checkpoint INFO: Loading checkpoint from https://download.pytorch.org/models/resnet18-5c106cde.pth
[10/31 14:05:03] fvcore.common.file_io INFO: URL https://download.pytorch.org/models/resnet18-5c106cde.pth cached in /cfarhomes/shishira/.torch/fvcore_cache/models/resnet18-5c106cde.pth
[10/31 14:05:03] fvcore.common.checkpoint INFO: Some model parameters or buffers are not found in the checkpoint:
  [34mbackbone.bottom_up.res4.0.conv1.weight[0m
  [34mbackbone.bottom_up.res4.1.conv2.weight[0m
  [34mbackbone.bottom_up.res3.1.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.shortcut.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.stem.conv1.norm.{bias, weight}[0m
  [34mbackbone.fpn_output3.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.1.conv2.norm.{bias, weight}[0m
  [34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
  [34mbackbone.fpn_lateral4.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.conv2.weight[0m
  [34mbackbone.bottom_up.res3.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.fpn_output4.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.1.conv1.norm.{bias, weight}[0m
  [34mproposal_generator.anchor_generator.cell_anchors.{1, 2, 4, 3, 0}[0m
  [34mbackbone.bottom_up.res4.0.conv2.norm.{bias, weight}[0m
  [34mproposal_generator.rpn_head.objectness_logits.{weight, bias}[0m
  [34mbackbone.fpn_output5.{weight, bias}[0m
  [34mbackbone.fpn_lateral3.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.1.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.stem.conv1.weight[0m
  [34mbackbone.bottom_up.res2.0.conv1.norm.{weight, bias}[0m
  [34mproposal_generator.rpn_head.conv.{weight, bias}[0m
  [34mroi_heads.box_predictor.bbox_pred.{bias, weight}[0m
  [34mroi_heads.box_predictor.cls_score.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.1.conv2.weight[0m
  [34mroi_heads.box_head.fc1.{weight, bias}[0m
  [34mbackbone.fpn_lateral5.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.1.conv1.weight[0m
  [34mbackbone.bottom_up.res5.0.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.conv2.norm.{weight, bias}[0m
  [34mroi_heads.box_head.fc2.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.1.conv1.norm.{weight, bias}[0m
  [34mroi_heads.mask_head.mask_fcn2.{weight, bias}[0m
  [34mbackbone.fpn_output2.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.1.conv1.weight[0m
  [34mroi_heads.mask_head.mask_fcn3.{weight, bias}[0m
  [34mroi_heads.mask_head.mask_fcn4.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.0.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv1.weight[0m
  [34mbackbone.bottom_up.res5.0.shortcut.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.1.conv1.weight[0m
  [34mbackbone.bottom_up.res3.0.shortcut.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res3.0.conv1.weight[0m
  [34mroi_heads.mask_head.deconv.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.conv2.weight[0m
  [34mbackbone.bottom_up.res2.1.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.conv2.weight[0m
  [34mbackbone.bottom_up.res5.1.conv2.weight[0m
  [34mroi_heads.mask_head.mask_fcn1.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.0.conv2.weight[0m
  [34mbackbone.bottom_up.res5.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res2.1.conv2.weight[0m
  [34mroi_heads.mask_head.predictor.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.1.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.1.conv1.weight[0m
  [34mbackbone.fpn_lateral2.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res2.1.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.conv1.weight[0m
  [34mbackbone.bottom_up.res5.0.conv1.norm.{bias, weight}[0m
[10/31 14:05:03] fvcore.common.checkpoint INFO: The checkpoint state_dict contains keys that are not used by the model:
  [35mconv1.weight[0m
  [35mbn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv1.weight[0m
  [35mlayer1.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv2.weight[0m
  [35mlayer1.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv1.weight[0m
  [35mlayer1.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv2.weight[0m
  [35mlayer1.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv1.weight[0m
  [35mlayer2.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv2.weight[0m
  [35mlayer2.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.downsample.0.weight[0m
  [35mlayer2.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv1.weight[0m
  [35mlayer2.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv2.weight[0m
  [35mlayer2.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv1.weight[0m
  [35mlayer3.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv2.weight[0m
  [35mlayer3.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.downsample.0.weight[0m
  [35mlayer3.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv1.weight[0m
  [35mlayer3.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv2.weight[0m
  [35mlayer3.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv1.weight[0m
  [35mlayer4.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv2.weight[0m
  [35mlayer4.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.downsample.0.weight[0m
  [35mlayer4.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv1.weight[0m
  [35mlayer4.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv2.weight[0m
  [35mlayer4.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mfc.{weight, bias}[0m
[10/31 14:05:05] d2.engine.train_loop INFO: Starting training from iteration 0
[10/31 14:05:18] d2.engine.hooks INFO: Total training time: 0:00:13 (0:00:00 on hooks)
[10/31 14:05:18] d2.utils.events INFO:  iter: 0    lr: N/A  max_mem: 751M
[10/31 14:06:14] detectron2 INFO: Rank of current process: 0. World size: 2
[10/31 14:06:15] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.3 (default, May 19 2020, 18:47:26) [GCC 7.3.0]
numpy                   1.19.1
detectron2              0.2.1 @/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 5.4
CUDA compiler           not available
detectron2 arch flags   /cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/_C.cpython-38-x86_64-linux-gnu.so; cannot find cuobjdump
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.6.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           True
GPU 0,1                 GeForce GTX 1080 Ti (arch=6.1)
CUDA_HOME               /opt/common/cuda/cuda-10.0.130/lib64/
Pillow                  7.2.0
torchvision             0.7.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision
torchvision arch flags  /cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                  0.1.2.post20201030
cv2                     4.4.0
----------------------  --------------------------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

[10/31 14:06:15] detectron2 INFO: Command line arguments: Namespace(config_file='configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml', dist_url='tcp://127.0.0.1:63010', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=['MODEL.MASK_ON', 'True', 'SOLVER.BASE_LR', '0.015', 'OUTPUT_DIR', 'temp'], resume=False)
[10/31 14:06:15] detectron2 INFO: Contents of args.config_file=configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml:
_BASE_: "../Base-RCNN-FPN.yaml"
MODEL:
  WEIGHTS: "https://download.pytorch.org/models/resnet18-5c106cde.pth"
  MASK_ON: True
  RESNETS:
    DEPTH: 18
    RES2_OUT_CHANNELS: 64
# DATASETS:
#   TRAIN: ("coco_2017_val",)
#   TEST: ("coco_2017_val",) #("coco_2017_test-dev",)
SOLVER:
  BASE_LR: 0.02
  CHECKPOINT_PERIOD: 7330
  # IMS_PER_BATCH: 24
  # STEPS: (40000, 53500)
  # MAX_ITER: 60000
  # WARMUP_FACTOR: 5e-4
  # WARMUP_ITERS: 2000
OUTPUT_DIR: output/faster_r18_fpn

[10/31 14:06:15] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  FILTER_EMPTY_ANNOTATIONS: True
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: ()
  PROPOSAL_FILES_TRAIN: ()
  TEST: ('coco_2017_val',)
  TRAIN: ('coco_2017_train',)
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: False
    SIZE: [0.9, 0.9]
    TYPE: relative_range
  FORMAT: BGR
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES: [[-90, 0, 90]]
    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES: [[32], [64], [128], [256], [512]]
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_fpn_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']
    NORM: 
    OUT_CHANNELS: 256
  KEYPOINT_ON: False
  LOAD_PROPOSALS: False
  MASK_ON: True
  META_ARCHITECTURE: GeneralizedRCNN
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: True
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN: [103.53, 116.28, 123.675]
  PIXEL_STD: [1.0, 1.0, 1.0]
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: False
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE: [False, False, False, False]
    DEPTH: 18
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']
    RES2_OUT_CHANNELS: 64
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.4, 0.5]
    NMS_THRESH_TEST: 0.5
    NORM: 
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))
    IOUS: (0.5, 0.6, 0.7)
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    CLS_AGNOSTIC_BBOX_REG: False
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: FastRCNNConvFCHead
    NORM: 
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    IOU_LABELS: [0, 1]
    IOU_THRESHOLDS: [0.5]
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: True
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: False
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: 
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    BOUNDARY_THRESH: -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.3, 0.7]
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: https://download.pytorch.org/models/resnet18-5c106cde.pth
OUTPUT_DIR: temp
SEED: -1
SOLVER:
  AMP:
    ENABLED: False
  BASE_LR: 0.015
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 7330
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: False
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 16
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 90000
  MOMENTUM: 0.9
  NESTEROV: False
  REFERENCE_WORLD_SIZE: 0
  STEPS: (60000, 80000)
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: False
    FLIP: True
    MAX_SIZE: 4000
    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 0
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: False
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0
[10/31 14:06:15] detectron2 INFO: Full config saved to temp/config.yaml
[10/31 14:06:15] d2.utils.env INFO: Using a generated random seed 16006209
[10/31 14:06:16] d2.engine.defaults INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten()
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[10/31 14:06:36] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 20.08 seconds.
[10/31 14:06:38] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/31 14:06:45] d2.data.build INFO: Removed 1021 images with no usable annotations. 117266 images left.
[10/31 14:06:51] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[10/31 14:06:51] d2.data.common INFO: Serializing 117266 elements to byte tensors and concatenating them all ...
[10/31 14:06:55] d2.data.common INFO: Serialized dataset takes 451.21 MiB
[10/31 14:06:55] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[10/31 14:06:55] d2.data.build INFO: Using training sampler TrainingSampler
[10/31 14:06:58] fvcore.common.checkpoint INFO: Loading checkpoint from https://download.pytorch.org/models/resnet18-5c106cde.pth
[10/31 14:06:58] fvcore.common.file_io INFO: URL https://download.pytorch.org/models/resnet18-5c106cde.pth cached in /cfarhomes/shishira/.torch/fvcore_cache/models/resnet18-5c106cde.pth
[10/31 14:06:59] fvcore.common.checkpoint INFO: Some model parameters or buffers are not found in the checkpoint:
  [34mbackbone.bottom_up.res3.0.shortcut.norm.{weight, bias}[0m
  [34mroi_heads.box_predictor.bbox_pred.{bias, weight}[0m
  [34mroi_heads.box_head.fc2.{weight, bias}[0m
  [34mroi_heads.mask_head.deconv.{weight, bias}[0m
  [34mproposal_generator.anchor_generator.cell_anchors.{0, 2, 3, 1, 4}[0m
  [34mbackbone.fpn_output3.{bias, weight}[0m
  [34mbackbone.bottom_up.stem.conv1.weight[0m
  [34mbackbone.bottom_up.res4.0.shortcut.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.stem.conv1.norm.{weight, bias}[0m
  [34mroi_heads.mask_head.mask_fcn3.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.1.conv2.norm.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn1.{bias, weight}[0m
  [34mbackbone.fpn_output5.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.1.conv1.weight[0m
  [34mbackbone.bottom_up.res2.1.conv1.weight[0m
  [34mbackbone.bottom_up.res5.1.conv1.norm.{weight, bias}[0m
  [34mroi_heads.mask_head.predictor.{weight, bias}[0m
  [34mbackbone.fpn_lateral2.{bias, weight}[0m
  [34mbackbone.fpn_output4.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.conv1.weight[0m
  [34mbackbone.bottom_up.res5.1.conv2.weight[0m
  [34mbackbone.bottom_up.res4.0.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res3.0.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.1.conv1.weight[0m
  [34mbackbone.bottom_up.res3.1.conv1.weight[0m
  [34mbackbone.bottom_up.res5.0.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.conv2.weight[0m
  [34mroi_heads.box_head.fc1.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.1.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.1.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.1.conv2.weight[0m
  [34mproposal_generator.rpn_head.objectness_logits.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.0.conv2.weight[0m
  [34mbackbone.fpn_lateral4.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.1.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.shortcut.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.conv2.weight[0m
  [34mbackbone.fpn_lateral5.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.conv2.weight[0m
  [34mbackbone.fpn_output2.{bias, weight}[0m
  [34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.1.conv2.weight[0m
  [34mbackbone.bottom_up.res4.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res5.1.conv2.norm.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn2.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv2.norm.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn4.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.conv1.weight[0m
  [34mroi_heads.box_predictor.cls_score.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.1.conv2.weight[0m
  [34mbackbone.bottom_up.res5.0.conv2.norm.{weight, bias}[0m
  [34mbackbone.fpn_lateral3.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv1.weight[0m
  [34mbackbone.bottom_up.res5.0.conv1.weight[0m
  [34mproposal_generator.rpn_head.conv.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.shortcut.weight[0m
[10/31 14:06:59] fvcore.common.checkpoint INFO: The checkpoint state_dict contains keys that are not used by the model:
  [35mconv1.weight[0m
  [35mbn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv1.weight[0m
  [35mlayer1.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv2.weight[0m
  [35mlayer1.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv1.weight[0m
  [35mlayer1.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv2.weight[0m
  [35mlayer1.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv1.weight[0m
  [35mlayer2.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv2.weight[0m
  [35mlayer2.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.downsample.0.weight[0m
  [35mlayer2.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv1.weight[0m
  [35mlayer2.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv2.weight[0m
  [35mlayer2.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv1.weight[0m
  [35mlayer3.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv2.weight[0m
  [35mlayer3.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.downsample.0.weight[0m
  [35mlayer3.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv1.weight[0m
  [35mlayer3.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv2.weight[0m
  [35mlayer3.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv1.weight[0m
  [35mlayer4.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv2.weight[0m
  [35mlayer4.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.downsample.0.weight[0m
  [35mlayer4.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv1.weight[0m
  [35mlayer4.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv2.weight[0m
  [35mlayer4.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mfc.{weight, bias}[0m
[10/31 14:07:14] detectron2 INFO: Rank of current process: 0. World size: 2
[10/31 14:07:14] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.3 (default, May 19 2020, 18:47:26) [GCC 7.3.0]
numpy                   1.19.1
detectron2              0.2.1 @/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 5.4
CUDA compiler           not available
detectron2 arch flags   /cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/_C.cpython-38-x86_64-linux-gnu.so; cannot find cuobjdump
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.6.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           True
GPU 0,1                 GeForce GTX 1080 Ti (arch=6.1)
CUDA_HOME               /opt/common/cuda/cuda-10.0.130/lib64/
Pillow                  7.2.0
torchvision             0.7.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision
torchvision arch flags  /cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                  0.1.2.post20201030
cv2                     4.4.0
----------------------  --------------------------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

[10/31 14:07:14] detectron2 INFO: Command line arguments: Namespace(config_file='configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml', dist_url='tcp://127.0.0.1:63010', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=['MODEL.MASK_ON', 'True', 'SOLVER.BASE_LR', '0.015', 'OUTPUT_DIR', 'temp'], resume=False)
[10/31 14:07:14] detectron2 INFO: Contents of args.config_file=configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml:
_BASE_: "../Base-RCNN-FPN.yaml"
MODEL:
  WEIGHTS: "https://download.pytorch.org/models/resnet18-5c106cde.pth"
  MASK_ON: True
  RESNETS:
    DEPTH: 18
    RES2_OUT_CHANNELS: 64
# DATASETS:
#   TRAIN: ("coco_2017_val",)
#   TEST: ("coco_2017_val",) #("coco_2017_test-dev",)
SOLVER:
  BASE_LR: 0.02
  CHECKPOINT_PERIOD: 7330
  # IMS_PER_BATCH: 24
  # STEPS: (40000, 53500)
  # MAX_ITER: 60000
  # WARMUP_FACTOR: 5e-4
  # WARMUP_ITERS: 2000
OUTPUT_DIR: output/faster_r18_fpn

[10/31 14:07:14] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  FILTER_EMPTY_ANNOTATIONS: True
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: ()
  PROPOSAL_FILES_TRAIN: ()
  TEST: ('coco_2017_val',)
  TRAIN: ('coco_2017_train',)
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: False
    SIZE: [0.9, 0.9]
    TYPE: relative_range
  FORMAT: BGR
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES: [[-90, 0, 90]]
    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES: [[32], [64], [128], [256], [512]]
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_fpn_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']
    NORM: 
    OUT_CHANNELS: 256
  KEYPOINT_ON: False
  LOAD_PROPOSALS: False
  MASK_ON: True
  META_ARCHITECTURE: GeneralizedRCNN
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: True
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN: [103.53, 116.28, 123.675]
  PIXEL_STD: [1.0, 1.0, 1.0]
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: False
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE: [False, False, False, False]
    DEPTH: 18
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']
    RES2_OUT_CHANNELS: 64
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.4, 0.5]
    NMS_THRESH_TEST: 0.5
    NORM: 
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))
    IOUS: (0.5, 0.6, 0.7)
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    CLS_AGNOSTIC_BBOX_REG: False
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: FastRCNNConvFCHead
    NORM: 
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    IOU_LABELS: [0, 1]
    IOU_THRESHOLDS: [0.5]
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: True
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: False
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: 
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    BOUNDARY_THRESH: -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.3, 0.7]
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: https://download.pytorch.org/models/resnet18-5c106cde.pth
OUTPUT_DIR: temp
SEED: -1
SOLVER:
  AMP:
    ENABLED: False
  BASE_LR: 0.015
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 7330
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: False
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 16
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 90000
  MOMENTUM: 0.9
  NESTEROV: False
  REFERENCE_WORLD_SIZE: 0
  STEPS: (60000, 80000)
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: False
    FLIP: True
    MAX_SIZE: 4000
    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 0
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: False
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0
[10/31 14:07:15] detectron2 INFO: Full config saved to temp/config.yaml
[10/31 14:07:15] d2.utils.env INFO: Using a generated random seed 15126285
[10/31 14:07:15] d2.engine.defaults INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten()
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[10/31 14:07:35] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 20.19 seconds.
[10/31 14:07:37] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/31 14:07:44] d2.data.build INFO: Removed 1021 images with no usable annotations. 117266 images left.
[10/31 14:07:50] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[10/31 14:07:50] d2.data.common INFO: Serializing 117266 elements to byte tensors and concatenating them all ...
[10/31 14:07:54] d2.data.common INFO: Serialized dataset takes 451.21 MiB
[10/31 14:07:54] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[10/31 14:07:54] d2.data.build INFO: Using training sampler TrainingSampler
[10/31 14:07:57] fvcore.common.checkpoint INFO: Loading checkpoint from https://download.pytorch.org/models/resnet18-5c106cde.pth
[10/31 14:07:57] fvcore.common.file_io INFO: URL https://download.pytorch.org/models/resnet18-5c106cde.pth cached in /cfarhomes/shishira/.torch/fvcore_cache/models/resnet18-5c106cde.pth
[10/31 14:07:57] fvcore.common.checkpoint INFO: Some model parameters or buffers are not found in the checkpoint:
  [34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.1.conv2.weight[0m
  [34mbackbone.bottom_up.res3.0.conv1.weight[0m
  [34mbackbone.bottom_up.res3.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.conv2.norm.{bias, weight}[0m
  [34mroi_heads.mask_head.deconv.{weight, bias}[0m
  [34mbackbone.fpn_output5.{bias, weight}[0m
  [34mbackbone.fpn_lateral5.{weight, bias}[0m
  [34mbackbone.fpn_output3.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.shortcut.norm.{bias, weight}[0m
  [34mproposal_generator.rpn_head.conv.{weight, bias}[0m
  [34mbackbone.fpn_lateral3.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.1.conv1.weight[0m
  [34mbackbone.bottom_up.res3.0.shortcut.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.1.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.1.conv1.norm.{bias, weight}[0m
  [34mbackbone.fpn_output2.{weight, bias}[0m
  [34mroi_heads.mask_head.mask_fcn3.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn1.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.fpn_output4.{bias, weight}[0m
  [34mbackbone.bottom_up.stem.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.1.conv2.weight[0m
  [34mbackbone.bottom_up.res2.0.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.0.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.1.conv1.weight[0m
  [34mbackbone.bottom_up.res5.1.conv1.norm.{weight, bias}[0m
  [34mroi_heads.box_head.fc1.{weight, bias}[0m
  [34mroi_heads.mask_head.mask_fcn4.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.conv2.norm.{bias, weight}[0m
  [34mbackbone.fpn_lateral4.{weight, bias}[0m
  [34mroi_heads.box_predictor.cls_score.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn2.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.shortcut.norm.{bias, weight}[0m
  [34mbackbone.fpn_lateral2.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.1.conv1.weight[0m
  [34mbackbone.bottom_up.res2.0.conv2.weight[0m
  [34mproposal_generator.anchor_generator.cell_anchors.{1, 2, 0, 3, 4}[0m
  [34mbackbone.bottom_up.res5.1.conv2.weight[0m
  [34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res2.0.conv1.weight[0m
  [34mroi_heads.box_head.fc2.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.1.conv2.weight[0m
  [34mbackbone.bottom_up.res4.1.conv1.weight[0m
  [34mbackbone.bottom_up.res3.0.conv2.weight[0m
  [34mbackbone.bottom_up.res5.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res3.1.conv2.norm.{bias, weight}[0m
  [34mroi_heads.box_predictor.bbox_pred.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.conv2.weight[0m
  [34mroi_heads.mask_head.predictor.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.1.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.conv2.weight[0m
  [34mbackbone.bottom_up.stem.conv1.weight[0m
  [34mbackbone.bottom_up.res4.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res5.0.conv1.weight[0m
  [34mbackbone.bottom_up.res4.0.conv1.weight[0m
[10/31 14:07:57] fvcore.common.checkpoint INFO: The checkpoint state_dict contains keys that are not used by the model:
  [35mconv1.weight[0m
  [35mbn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv1.weight[0m
  [35mlayer1.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv2.weight[0m
  [35mlayer1.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv1.weight[0m
  [35mlayer1.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv2.weight[0m
  [35mlayer1.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv1.weight[0m
  [35mlayer2.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv2.weight[0m
  [35mlayer2.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.downsample.0.weight[0m
  [35mlayer2.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv1.weight[0m
  [35mlayer2.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv2.weight[0m
  [35mlayer2.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv1.weight[0m
  [35mlayer3.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv2.weight[0m
  [35mlayer3.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.downsample.0.weight[0m
  [35mlayer3.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv1.weight[0m
  [35mlayer3.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv2.weight[0m
  [35mlayer3.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv1.weight[0m
  [35mlayer4.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv2.weight[0m
  [35mlayer4.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.downsample.0.weight[0m
  [35mlayer4.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv1.weight[0m
  [35mlayer4.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv2.weight[0m
  [35mlayer4.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mfc.{weight, bias}[0m
[10/31 14:08:19] detectron2 INFO: Rank of current process: 0. World size: 2
[10/31 14:08:20] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.3 (default, May 19 2020, 18:47:26) [GCC 7.3.0]
numpy                   1.19.1
detectron2              0.2.1 @/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 5.4
CUDA compiler           not available
detectron2 arch flags   /cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/_C.cpython-38-x86_64-linux-gnu.so; cannot find cuobjdump
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.6.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           True
GPU 0,1                 GeForce GTX 1080 Ti (arch=6.1)
CUDA_HOME               /opt/common/cuda/cuda-10.0.130/lib64/
Pillow                  7.2.0
torchvision             0.7.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision
torchvision arch flags  /cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                  0.1.2.post20201030
cv2                     4.4.0
----------------------  --------------------------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

[10/31 14:08:20] detectron2 INFO: Command line arguments: Namespace(config_file='configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml', dist_url='tcp://127.0.0.1:63010', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=['MODEL.MASK_ON', 'True', 'SOLVER.BASE_LR', '0.015', 'OUTPUT_DIR', 'temp'], resume=False)
[10/31 14:08:20] detectron2 INFO: Contents of args.config_file=configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml:
_BASE_: "../Base-RCNN-FPN.yaml"
MODEL:
  WEIGHTS: "https://download.pytorch.org/models/resnet18-5c106cde.pth"
  MASK_ON: True
  RESNETS:
    DEPTH: 18
    RES2_OUT_CHANNELS: 64
# DATASETS:
#   TRAIN: ("coco_2017_val",)
#   TEST: ("coco_2017_val",) #("coco_2017_test-dev",)
SOLVER:
  BASE_LR: 0.02
  CHECKPOINT_PERIOD: 7330
  # IMS_PER_BATCH: 24
  # STEPS: (40000, 53500)
  # MAX_ITER: 60000
  # WARMUP_FACTOR: 5e-4
  # WARMUP_ITERS: 2000
OUTPUT_DIR: output/faster_r18_fpn

[10/31 14:08:20] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  FILTER_EMPTY_ANNOTATIONS: True
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: ()
  PROPOSAL_FILES_TRAIN: ()
  TEST: ('coco_2017_val',)
  TRAIN: ('coco_2017_train',)
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: False
    SIZE: [0.9, 0.9]
    TYPE: relative_range
  FORMAT: BGR
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES: [[-90, 0, 90]]
    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES: [[32], [64], [128], [256], [512]]
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_fpn_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']
    NORM: 
    OUT_CHANNELS: 256
  KEYPOINT_ON: False
  LOAD_PROPOSALS: False
  MASK_ON: True
  META_ARCHITECTURE: GeneralizedRCNN
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: True
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN: [103.53, 116.28, 123.675]
  PIXEL_STD: [1.0, 1.0, 1.0]
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: False
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE: [False, False, False, False]
    DEPTH: 18
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']
    RES2_OUT_CHANNELS: 64
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.4, 0.5]
    NMS_THRESH_TEST: 0.5
    NORM: 
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))
    IOUS: (0.5, 0.6, 0.7)
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    CLS_AGNOSTIC_BBOX_REG: False
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: FastRCNNConvFCHead
    NORM: 
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    IOU_LABELS: [0, 1]
    IOU_THRESHOLDS: [0.5]
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: True
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: False
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: 
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    BOUNDARY_THRESH: -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.3, 0.7]
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: https://download.pytorch.org/models/resnet18-5c106cde.pth
OUTPUT_DIR: temp
SEED: -1
SOLVER:
  AMP:
    ENABLED: False
  BASE_LR: 0.015
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 7330
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: False
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 16
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 90000
  MOMENTUM: 0.9
  NESTEROV: False
  REFERENCE_WORLD_SIZE: 0
  STEPS: (60000, 80000)
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: False
    FLIP: True
    MAX_SIZE: 4000
    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 0
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: False
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0
[10/31 14:08:20] detectron2 INFO: Full config saved to temp/config.yaml
[10/31 14:08:20] d2.utils.env INFO: Using a generated random seed 20521937
[10/31 14:08:21] d2.engine.defaults INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten()
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[10/31 14:08:41] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 20.20 seconds.
[10/31 14:08:42] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/31 14:08:50] d2.data.build INFO: Removed 1021 images with no usable annotations. 117266 images left.
[10/31 14:08:55] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[10/31 14:08:55] d2.data.common INFO: Serializing 117266 elements to byte tensors and concatenating them all ...
[10/31 14:08:59] d2.data.common INFO: Serialized dataset takes 451.21 MiB
[10/31 14:08:59] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[10/31 14:08:59] d2.data.build INFO: Using training sampler TrainingSampler
[10/31 14:09:03] fvcore.common.checkpoint INFO: Loading checkpoint from https://download.pytorch.org/models/resnet18-5c106cde.pth
[10/31 14:09:03] fvcore.common.file_io INFO: URL https://download.pytorch.org/models/resnet18-5c106cde.pth cached in /cfarhomes/shishira/.torch/fvcore_cache/models/resnet18-5c106cde.pth
[10/31 14:09:03] fvcore.common.checkpoint INFO: Some model parameters or buffers are not found in the checkpoint:
  [34mroi_heads.box_head.fc1.{bias, weight}[0m
  [34mbackbone.fpn_lateral3.{bias, weight}[0m
  [34mbackbone.fpn_lateral4.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.shortcut.norm.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn4.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.1.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.1.conv1.norm.{weight, bias}[0m
  [34mroi_heads.mask_head.mask_fcn3.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.conv2.norm.{weight, bias}[0m
  [34mroi_heads.mask_head.deconv.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.1.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.1.conv1.norm.{bias, weight}[0m
  [34mproposal_generator.rpn_head.conv.{weight, bias}[0m
  [34mbackbone.fpn_output4.{bias, weight}[0m
  [34mproposal_generator.rpn_head.anchor_deltas.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.1.conv2.weight[0m
  [34mbackbone.bottom_up.res4.0.conv2.norm.{weight, bias}[0m
  [34mbackbone.fpn_output5.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.conv1.norm.{weight, bias}[0m
  [34mproposal_generator.anchor_generator.cell_anchors.{3, 1, 2, 0, 4}[0m
  [34mroi_heads.mask_head.predictor.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.shortcut.weight[0m
  [34mroi_heads.mask_head.mask_fcn1.{weight, bias}[0m
  [34mbackbone.fpn_lateral2.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv1.weight[0m
  [34mbackbone.bottom_up.res5.1.conv2.weight[0m
  [34mbackbone.bottom_up.res5.0.shortcut.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.1.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.0.conv2.norm.{weight, bias}[0m
  [34mbackbone.fpn_output3.{bias, weight}[0m
  [34mroi_heads.box_head.fc2.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.1.conv2.weight[0m
  [34mbackbone.bottom_up.stem.conv1.weight[0m
  [34mroi_heads.box_predictor.bbox_pred.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.conv2.weight[0m
  [34mbackbone.fpn_lateral5.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn2.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.conv2.weight[0m
  [34mroi_heads.box_predictor.cls_score.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.0.conv2.weight[0m
  [34mbackbone.fpn_output2.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.1.conv1.weight[0m
  [34mbackbone.bottom_up.res3.1.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.1.conv1.weight[0m
  [34mbackbone.bottom_up.res3.0.shortcut.norm.{weight, bias}[0m
  [34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.1.conv1.weight[0m
  [34mbackbone.bottom_up.stem.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.conv1.weight[0m
  [34mbackbone.bottom_up.res5.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res5.1.conv1.weight[0m
  [34mbackbone.bottom_up.res3.0.conv1.weight[0m
  [34mbackbone.bottom_up.res4.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res5.0.conv2.weight[0m
  [34mbackbone.bottom_up.res4.1.conv2.weight[0m
  [34mbackbone.bottom_up.res5.0.conv1.weight[0m
[10/31 14:09:03] fvcore.common.checkpoint INFO: The checkpoint state_dict contains keys that are not used by the model:
  [35mconv1.weight[0m
  [35mbn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv1.weight[0m
  [35mlayer1.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv2.weight[0m
  [35mlayer1.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv1.weight[0m
  [35mlayer1.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv2.weight[0m
  [35mlayer1.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv1.weight[0m
  [35mlayer2.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv2.weight[0m
  [35mlayer2.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.downsample.0.weight[0m
  [35mlayer2.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv1.weight[0m
  [35mlayer2.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv2.weight[0m
  [35mlayer2.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv1.weight[0m
  [35mlayer3.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv2.weight[0m
  [35mlayer3.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.downsample.0.weight[0m
  [35mlayer3.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv1.weight[0m
  [35mlayer3.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv2.weight[0m
  [35mlayer3.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv1.weight[0m
  [35mlayer4.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv2.weight[0m
  [35mlayer4.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.downsample.0.weight[0m
  [35mlayer4.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv1.weight[0m
  [35mlayer4.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv2.weight[0m
  [35mlayer4.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mfc.{weight, bias}[0m
[10/31 14:20:58] detectron2 INFO: Rank of current process: 0. World size: 2
[10/31 14:21:00] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.3 (default, May 19 2020, 18:47:26) [GCC 7.3.0]
numpy                   1.19.1
detectron2              0.2.1 @/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 5.4
CUDA compiler           not available
detectron2 arch flags   /cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/_C.cpython-38-x86_64-linux-gnu.so; cannot find cuobjdump
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.6.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           True
GPU 0,1                 GeForce GTX 1080 Ti (arch=6.1)
CUDA_HOME               /opt/common/cuda/cuda-10.0.130/lib64/
Pillow                  7.2.0
torchvision             0.7.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision
torchvision arch flags  /cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                  0.1.2.post20201030
cv2                     4.4.0
----------------------  --------------------------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

[10/31 14:21:00] detectron2 INFO: Command line arguments: Namespace(config_file='configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml', dist_url='tcp://127.0.0.1:63010', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=['MODEL.MASK_ON', 'True', 'SOLVER.BASE_LR', '0.015', 'OUTPUT_DIR', 'temp'], resume=False)
[10/31 14:21:00] detectron2 INFO: Contents of args.config_file=configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml:
_BASE_: "../Base-RCNN-FPN.yaml"
MODEL:
  WEIGHTS: "https://download.pytorch.org/models/resnet18-5c106cde.pth"
  MASK_ON: True
  RESNETS:
    DEPTH: 18
    RES2_OUT_CHANNELS: 64
# DATASETS:
#   TRAIN: ("coco_2017_val",)
#   TEST: ("coco_2017_val",) #("coco_2017_test-dev",)
SOLVER:
  BASE_LR: 0.02
  CHECKPOINT_PERIOD: 7330
  # IMS_PER_BATCH: 24
  # STEPS: (40000, 53500)
  # MAX_ITER: 60000
  # WARMUP_FACTOR: 5e-4
  # WARMUP_ITERS: 2000
OUTPUT_DIR: output/faster_r18_fpn

[10/31 14:21:00] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  FILTER_EMPTY_ANNOTATIONS: True
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: ()
  PROPOSAL_FILES_TRAIN: ()
  TEST: ('coco_2017_val',)
  TRAIN: ('coco_2017_train',)
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: False
    SIZE: [0.9, 0.9]
    TYPE: relative_range
  FORMAT: BGR
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES: [[-90, 0, 90]]
    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES: [[32], [64], [128], [256], [512]]
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_fpn_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']
    NORM: 
    OUT_CHANNELS: 256
  KEYPOINT_ON: False
  LOAD_PROPOSALS: False
  MASK_ON: True
  META_ARCHITECTURE: GeneralizedRCNN
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: True
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN: [103.53, 116.28, 123.675]
  PIXEL_STD: [1.0, 1.0, 1.0]
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: False
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE: [False, False, False, False]
    DEPTH: 18
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']
    RES2_OUT_CHANNELS: 64
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.4, 0.5]
    NMS_THRESH_TEST: 0.5
    NORM: 
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))
    IOUS: (0.5, 0.6, 0.7)
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    CLS_AGNOSTIC_BBOX_REG: False
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: FastRCNNConvFCHead
    NORM: 
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    IOU_LABELS: [0, 1]
    IOU_THRESHOLDS: [0.5]
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: True
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: False
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: 
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    BOUNDARY_THRESH: -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.3, 0.7]
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: https://download.pytorch.org/models/resnet18-5c106cde.pth
OUTPUT_DIR: temp
SEED: -1
SOLVER:
  AMP:
    ENABLED: False
  BASE_LR: 0.015
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 7330
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: False
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 16
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 90000
  MOMENTUM: 0.9
  NESTEROV: False
  REFERENCE_WORLD_SIZE: 0
  STEPS: (60000, 80000)
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: False
    FLIP: True
    MAX_SIZE: 4000
    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 0
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: False
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0
[10/31 14:21:00] detectron2 INFO: Full config saved to temp/config.yaml
[10/31 14:21:00] d2.utils.env INFO: Using a generated random seed 878039
[10/31 14:21:01] d2.engine.defaults INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten()
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[10/31 14:21:25] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 24.19 seconds.
[10/31 14:21:26] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/31 14:21:34] d2.data.build INFO: Removed 1021 images with no usable annotations. 117266 images left.
[10/31 14:21:39] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[10/31 14:21:39] d2.data.common INFO: Serializing 117266 elements to byte tensors and concatenating them all ...
[10/31 14:21:43] d2.data.common INFO: Serialized dataset takes 451.21 MiB
[10/31 14:21:43] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[10/31 14:21:43] d2.data.build INFO: Using training sampler TrainingSampler
[10/31 14:21:47] fvcore.common.checkpoint INFO: Loading checkpoint from https://download.pytorch.org/models/resnet18-5c106cde.pth
[10/31 14:21:47] fvcore.common.file_io INFO: URL https://download.pytorch.org/models/resnet18-5c106cde.pth cached in /cfarhomes/shishira/.torch/fvcore_cache/models/resnet18-5c106cde.pth
[10/31 14:21:47] fvcore.common.checkpoint INFO: Some model parameters or buffers are not found in the checkpoint:
  [34mbackbone.fpn_lateral5.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.stem.conv1.weight[0m
  [34mbackbone.bottom_up.res5.1.conv1.weight[0m
  [34mbackbone.fpn_output5.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.shortcut.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv1.weight[0m
  [34mbackbone.bottom_up.res2.1.conv2.weight[0m
  [34mbackbone.fpn_lateral4.{bias, weight}[0m
  [34mproposal_generator.anchor_generator.cell_anchors.{1, 4, 2, 0, 3}[0m
  [34mroi_heads.mask_head.mask_fcn1.{bias, weight}[0m
  [34mbackbone.fpn_lateral2.{weight, bias}[0m
  [34mbackbone.fpn_output3.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.conv2.weight[0m
  [34mbackbone.bottom_up.res3.0.conv2.norm.{weight, bias}[0m
  [34mroi_heads.mask_head.deconv.{weight, bias}[0m
  [34mbackbone.fpn_lateral3.{bias, weight}[0m
  [34mbackbone.fpn_output2.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv2.norm.{bias, weight}[0m
  [34mroi_heads.box_predictor.cls_score.{bias, weight}[0m
  [34mbackbone.bottom_up.stem.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.1.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.1.conv1.weight[0m
  [34mroi_heads.mask_head.mask_fcn2.{bias, weight}[0m
  [34mroi_heads.box_predictor.bbox_pred.{weight, bias}[0m
  [34mproposal_generator.rpn_head.anchor_deltas.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.conv1.weight[0m
  [34mbackbone.bottom_up.res2.0.conv2.weight[0m
  [34mbackbone.bottom_up.res5.0.shortcut.weight[0m
  [34mbackbone.fpn_output4.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.conv1.norm.{bias, weight}[0m
  [34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.1.conv1.norm.{weight, bias}[0m
  [34mroi_heads.mask_head.mask_fcn4.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.1.conv2.norm.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn3.{weight, bias}[0m
  [34mroi_heads.mask_head.predictor.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.1.conv2.weight[0m
  [34mroi_heads.box_head.fc1.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.conv2.weight[0m
  [34mbackbone.bottom_up.res3.0.shortcut.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.1.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.conv1.weight[0m
  [34mbackbone.bottom_up.res3.0.conv1.weight[0m
  [34mroi_heads.box_head.fc2.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.1.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.1.conv1.weight[0m
  [34mbackbone.bottom_up.res4.0.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.shortcut.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.1.conv1.norm.{bias, weight}[0m
  [34mproposal_generator.rpn_head.conv.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.1.conv2.weight[0m
  [34mbackbone.bottom_up.res4.1.conv2.weight[0m
  [34mbackbone.bottom_up.res4.1.conv1.weight[0m
  [34mbackbone.bottom_up.res4.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res3.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res5.0.conv2.weight[0m
[10/31 14:21:47] fvcore.common.checkpoint INFO: The checkpoint state_dict contains keys that are not used by the model:
  [35mconv1.weight[0m
  [35mbn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv1.weight[0m
  [35mlayer1.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv2.weight[0m
  [35mlayer1.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv1.weight[0m
  [35mlayer1.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv2.weight[0m
  [35mlayer1.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv1.weight[0m
  [35mlayer2.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv2.weight[0m
  [35mlayer2.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.downsample.0.weight[0m
  [35mlayer2.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv1.weight[0m
  [35mlayer2.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv2.weight[0m
  [35mlayer2.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv1.weight[0m
  [35mlayer3.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv2.weight[0m
  [35mlayer3.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.downsample.0.weight[0m
  [35mlayer3.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv1.weight[0m
  [35mlayer3.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv2.weight[0m
  [35mlayer3.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv1.weight[0m
  [35mlayer4.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv2.weight[0m
  [35mlayer4.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.downsample.0.weight[0m
  [35mlayer4.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv1.weight[0m
  [35mlayer4.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv2.weight[0m
  [35mlayer4.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mfc.{weight, bias}[0m
[10/31 14:21:49] d2.engine.train_loop INFO: Starting training from iteration 0
[10/31 14:21:52] d2.engine.hooks INFO: Total training time: 0:00:02 (0:00:00 on hooks)
[10/31 14:21:52] d2.utils.events INFO:  iter: 0    lr: N/A  max_mem: 751M
[10/31 14:25:25] detectron2 INFO: Rank of current process: 0. World size: 2
[10/31 14:25:26] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.3 (default, May 19 2020, 18:47:26) [GCC 7.3.0]
numpy                   1.19.1
detectron2              0.2.1 @/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 5.4
CUDA compiler           not available
detectron2 arch flags   /cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/_C.cpython-38-x86_64-linux-gnu.so; cannot find cuobjdump
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.6.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           True
GPU 0,1                 GeForce GTX 1080 Ti (arch=6.1)
CUDA_HOME               /opt/common/cuda/cuda-10.0.130/lib64/
Pillow                  7.2.0
torchvision             0.7.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision
torchvision arch flags  /cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                  0.1.2.post20201030
cv2                     4.4.0
----------------------  --------------------------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

[10/31 14:25:26] detectron2 INFO: Command line arguments: Namespace(config_file='configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml', dist_url='tcp://127.0.0.1:63010', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=['MODEL.MASK_ON', 'True', 'SOLVER.BASE_LR', '0.015', 'OUTPUT_DIR', 'temp'], resume=False)
[10/31 14:25:26] detectron2 INFO: Contents of args.config_file=configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml:
_BASE_: "../Base-RCNN-FPN.yaml"
MODEL:
  WEIGHTS: "https://download.pytorch.org/models/resnet18-5c106cde.pth"
  MASK_ON: True
  RESNETS:
    DEPTH: 18
    RES2_OUT_CHANNELS: 64
# DATASETS:
#   TRAIN: ("coco_2017_val",)
#   TEST: ("coco_2017_val",) #("coco_2017_test-dev",)
SOLVER:
  BASE_LR: 0.02
  CHECKPOINT_PERIOD: 7330
  # IMS_PER_BATCH: 24
  # STEPS: (40000, 53500)
  # MAX_ITER: 60000
  # WARMUP_FACTOR: 5e-4
  # WARMUP_ITERS: 2000
OUTPUT_DIR: output/faster_r18_fpn

[10/31 14:25:26] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  FILTER_EMPTY_ANNOTATIONS: True
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: ()
  PROPOSAL_FILES_TRAIN: ()
  TEST: ('coco_2017_val',)
  TRAIN: ('coco_2017_train',)
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: False
    SIZE: [0.9, 0.9]
    TYPE: relative_range
  FORMAT: BGR
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES: [[-90, 0, 90]]
    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES: [[32], [64], [128], [256], [512]]
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_fpn_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']
    NORM: 
    OUT_CHANNELS: 256
  KEYPOINT_ON: False
  LOAD_PROPOSALS: False
  MASK_ON: True
  META_ARCHITECTURE: GeneralizedRCNN
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: True
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN: [103.53, 116.28, 123.675]
  PIXEL_STD: [1.0, 1.0, 1.0]
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: False
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE: [False, False, False, False]
    DEPTH: 18
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']
    RES2_OUT_CHANNELS: 64
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.4, 0.5]
    NMS_THRESH_TEST: 0.5
    NORM: 
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))
    IOUS: (0.5, 0.6, 0.7)
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    CLS_AGNOSTIC_BBOX_REG: False
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: FastRCNNConvFCHead
    NORM: 
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    IOU_LABELS: [0, 1]
    IOU_THRESHOLDS: [0.5]
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: True
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: False
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: 
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    BOUNDARY_THRESH: -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.3, 0.7]
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: https://download.pytorch.org/models/resnet18-5c106cde.pth
OUTPUT_DIR: temp
SEED: -1
SOLVER:
  AMP:
    ENABLED: False
  BASE_LR: 0.015
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 7330
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: False
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 16
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 90000
  MOMENTUM: 0.9
  NESTEROV: False
  REFERENCE_WORLD_SIZE: 0
  STEPS: (60000, 80000)
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: False
    FLIP: True
    MAX_SIZE: 4000
    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 0
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: False
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0
[10/31 14:25:26] detectron2 INFO: Full config saved to temp/config.yaml
[10/31 14:25:26] d2.utils.env INFO: Using a generated random seed 26707755
[10/31 14:25:27] d2.engine.defaults INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten()
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[10/31 14:25:48] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 21.27 seconds.
[10/31 14:25:49] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/31 14:25:57] d2.data.build INFO: Removed 1021 images with no usable annotations. 117266 images left.
[10/31 14:26:02] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[10/31 14:26:02] d2.data.common INFO: Serializing 117266 elements to byte tensors and concatenating them all ...
[10/31 14:26:06] d2.data.common INFO: Serialized dataset takes 451.21 MiB
[10/31 14:26:06] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[10/31 14:26:06] d2.data.build INFO: Using training sampler TrainingSampler
[10/31 14:26:09] fvcore.common.checkpoint INFO: Loading checkpoint from https://download.pytorch.org/models/resnet18-5c106cde.pth
[10/31 14:26:09] fvcore.common.file_io INFO: URL https://download.pytorch.org/models/resnet18-5c106cde.pth cached in /cfarhomes/shishira/.torch/fvcore_cache/models/resnet18-5c106cde.pth
[10/31 14:26:10] fvcore.common.checkpoint INFO: Some model parameters or buffers are not found in the checkpoint:
  [34mbackbone.bottom_up.res3.1.conv1.norm.{weight, bias}[0m
  [34mroi_heads.box_predictor.bbox_pred.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.1.conv1.norm.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn1.{bias, weight}[0m
  [34mbackbone.fpn_output3.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.shortcut.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.shortcut.weight[0m
  [34mroi_heads.mask_head.predictor.{bias, weight}[0m
  [34mbackbone.fpn_output2.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.conv2.weight[0m
  [34mbackbone.bottom_up.res5.1.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.0.conv2.weight[0m
  [34mbackbone.bottom_up.res4.0.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.conv2.norm.{bias, weight}[0m
  [34mroi_heads.box_head.fc1.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.conv1.weight[0m
  [34mbackbone.bottom_up.res2.1.conv1.norm.{weight, bias}[0m
  [34mproposal_generator.anchor_generator.cell_anchors.{4, 2, 1, 3, 0}[0m
  [34mbackbone.bottom_up.res2.1.conv2.norm.{weight, bias}[0m
  [34mroi_heads.mask_head.mask_fcn4.{bias, weight}[0m
  [34mroi_heads.box_head.fc2.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.shortcut.norm.{bias, weight}[0m
  [34mroi_heads.box_predictor.cls_score.{weight, bias}[0m
  [34mbackbone.fpn_lateral2.{weight, bias}[0m
  [34mbackbone.fpn_lateral3.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.conv1.weight[0m
  [34mroi_heads.mask_head.deconv.{bias, weight}[0m
  [34mbackbone.fpn_lateral4.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv1.norm.{weight, bias}[0m
  [34mbackbone.fpn_output5.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.1.conv1.weight[0m
  [34mbackbone.bottom_up.res4.1.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.1.conv1.weight[0m
  [34mbackbone.fpn_lateral5.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.conv2.weight[0m
  [34mproposal_generator.rpn_head.conv.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn2.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn3.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.conv2.weight[0m
  [34mbackbone.bottom_up.stem.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res2.0.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.1.conv2.weight[0m
  [34mbackbone.bottom_up.res3.0.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.1.conv2.weight[0m
  [34mbackbone.bottom_up.res5.0.shortcut.norm.{weight, bias}[0m
  [34mproposal_generator.rpn_head.objectness_logits.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.1.conv1.weight[0m
  [34mbackbone.bottom_up.res5.0.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res5.1.conv2.weight[0m
  [34mproposal_generator.rpn_head.anchor_deltas.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.1.conv2.weight[0m
  [34mbackbone.bottom_up.res2.0.conv1.weight[0m
  [34mbackbone.bottom_up.res4.0.conv1.weight[0m
  [34mbackbone.bottom_up.res3.1.conv1.weight[0m
  [34mbackbone.bottom_up.stem.conv1.weight[0m
  [34mbackbone.fpn_output4.{weight, bias}[0m
[10/31 14:26:10] fvcore.common.checkpoint INFO: The checkpoint state_dict contains keys that are not used by the model:
  [35mconv1.weight[0m
  [35mbn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv1.weight[0m
  [35mlayer1.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv2.weight[0m
  [35mlayer1.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv1.weight[0m
  [35mlayer1.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv2.weight[0m
  [35mlayer1.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv1.weight[0m
  [35mlayer2.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv2.weight[0m
  [35mlayer2.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.downsample.0.weight[0m
  [35mlayer2.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv1.weight[0m
  [35mlayer2.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv2.weight[0m
  [35mlayer2.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv1.weight[0m
  [35mlayer3.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv2.weight[0m
  [35mlayer3.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.downsample.0.weight[0m
  [35mlayer3.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv1.weight[0m
  [35mlayer3.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv2.weight[0m
  [35mlayer3.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv1.weight[0m
  [35mlayer4.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv2.weight[0m
  [35mlayer4.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.downsample.0.weight[0m
  [35mlayer4.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv1.weight[0m
  [35mlayer4.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv2.weight[0m
  [35mlayer4.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mfc.{weight, bias}[0m
[10/31 14:26:11] d2.engine.train_loop INFO: Starting training from iteration 0
[10/31 14:26:52] d2.utils.events INFO:  eta: 1 day, 1:19:49  iter: 19  total_loss: 6.056  loss_cls: 4.391  loss_box_reg: 0.1104  loss_mask: 0.6907  loss_rpn_cls: 0.6785  loss_rpn_loc: 0.1776  time: 1.2542  data_time: 1.2858  lr: 0.00029971  max_mem: 6596M
[10/31 14:27:16] d2.utils.events INFO:  eta: 1 day, 1:25:04  iter: 39  total_loss: 5.817  loss_cls: 4.344  loss_box_reg: 0.1419  loss_mask: 0.6804  loss_rpn_cls: 0.5145  loss_rpn_loc: 0.1421  time: 1.2362  data_time: 0.4493  lr: 0.00059941  max_mem: 6596M
[10/31 14:27:40] d2.utils.events INFO:  eta: 1 day, 2:14:01  iter: 59  total_loss: 5.642  loss_cls: 4.239  loss_box_reg: 0.1893  loss_mask: 0.6742  loss_rpn_cls: 0.4154  loss_rpn_loc: 0.124  time: 1.2154  data_time: 0.4571  lr: 0.00089911  max_mem: 6596M
[10/31 14:28:07] d2.utils.events INFO:  eta: 1 day, 2:13:40  iter: 79  total_loss: 5.523  loss_cls: 4.081  loss_box_reg: 0.1806  loss_mask: 0.6661  loss_rpn_cls: 0.4345  loss_rpn_loc: 0.1468  time: 1.2488  data_time: 0.6112  lr: 0.0011988  max_mem: 6596M
[10/31 14:28:31] d2.utils.events INFO:  eta: 1 day, 1:33:38  iter: 99  total_loss: 5.316  loss_cls: 3.877  loss_box_reg: 0.2124  loss_mask: 0.6628  loss_rpn_cls: 0.4005  loss_rpn_loc: 0.1406  time: 1.2374  data_time: 0.3999  lr: 0.0014985  max_mem: 6596M
[10/31 14:28:55] d2.utils.events INFO:  eta: 1 day, 1:26:36  iter: 119  total_loss: 5.085  loss_cls: 3.614  loss_box_reg: 0.2148  loss_mask: 0.6591  loss_rpn_cls: 0.4232  loss_rpn_loc: 0.1282  time: 1.2257  data_time: 0.4501  lr: 0.0017982  max_mem: 6596M
[10/31 14:29:21] d2.utils.events INFO:  eta: 1 day, 1:53:12  iter: 139  total_loss: 4.712  loss_cls: 3.317  loss_box_reg: 0.2128  loss_mask: 0.66  loss_rpn_cls: 0.3873  loss_rpn_loc: 0.1332  time: 1.2305  data_time: 0.5128  lr: 0.0020979  max_mem: 6596M
[10/31 14:29:47] d2.utils.events INFO:  eta: 1 day, 2:02:37  iter: 159  total_loss: 4.455  loss_cls: 2.996  loss_box_reg: 0.2254  loss_mask: 0.6555  loss_rpn_cls: 0.3866  loss_rpn_loc: 0.1493  time: 1.2315  data_time: 0.4623  lr: 0.0023976  max_mem: 6596M
[10/31 14:30:16] d2.utils.events INFO:  eta: 1 day, 2:02:16  iter: 179  total_loss: 3.918  loss_cls: 2.599  loss_box_reg: 0.1789  loss_mask: 0.6466  loss_rpn_cls: 0.389  loss_rpn_loc: 0.1337  time: 1.2502  data_time: 0.6639  lr: 0.0026973  max_mem: 6596M
[10/31 14:30:43] d2.utils.events INFO:  eta: 1 day, 1:56:41  iter: 199  total_loss: 3.584  loss_cls: 2.216  loss_box_reg: 0.2035  loss_mask: 0.6506  loss_rpn_cls: 0.3438  loss_rpn_loc: 0.1377  time: 1.2616  data_time: 0.5796  lr: 0.002997  max_mem: 6596M
[10/31 14:31:04] d2.utils.events INFO:  eta: 1 day, 1:39:03  iter: 219  total_loss: 3.159  loss_cls: 1.846  loss_box_reg: 0.2033  loss_mask: 0.6431  loss_rpn_cls: 0.3271  loss_rpn_loc: 0.1391  time: 1.2416  data_time: 0.2546  lr: 0.0032967  max_mem: 6596M
[10/31 14:31:27] d2.utils.events INFO:  eta: 1 day, 1:37:44  iter: 239  total_loss: 2.826  loss_cls: 1.509  loss_box_reg: 0.1997  loss_mask: 0.6412  loss_rpn_cls: 0.3454  loss_rpn_loc: 0.1674  time: 1.2327  data_time: 0.3483  lr: 0.0035964  max_mem: 6596M
[10/31 14:31:53] d2.utils.events INFO:  eta: 1 day, 1:57:04  iter: 259  total_loss: 2.536  loss_cls: 1.19  loss_box_reg: 0.1892  loss_mask: 0.6437  loss_rpn_cls: 0.3804  loss_rpn_loc: 0.1633  time: 1.2378  data_time: 0.5807  lr: 0.0038961  max_mem: 6596M
[10/31 14:32:21] d2.utils.events INFO:  eta: 1 day, 1:53:43  iter: 279  total_loss: 2.294  loss_cls: 0.9746  loss_box_reg: 0.1832  loss_mask: 0.6457  loss_rpn_cls: 0.3524  loss_rpn_loc: 0.1543  time: 1.2473  data_time: 0.5786  lr: 0.0041958  max_mem: 6596M
[10/31 14:32:46] d2.utils.events INFO:  eta: 1 day, 1:56:22  iter: 299  total_loss: 2.253  loss_cls: 0.8422  loss_box_reg: 0.2176  loss_mask: 0.641  loss_rpn_cls: 0.3728  loss_rpn_loc: 0.1468  time: 1.2463  data_time: 0.4779  lr: 0.0044955  max_mem: 6596M
[10/31 14:33:11] d2.utils.events INFO:  eta: 1 day, 1:53:01  iter: 319  total_loss: 2  loss_cls: 0.6964  loss_box_reg: 0.1802  loss_mask: 0.6417  loss_rpn_cls: 0.3478  loss_rpn_loc: 0.1292  time: 1.2482  data_time: 0.5088  lr: 0.0047952  max_mem: 6596M
[10/31 14:33:35] d2.utils.events INFO:  eta: 1 day, 1:45:18  iter: 339  total_loss: 2.042  loss_cls: 0.6919  loss_box_reg: 0.2241  loss_mask: 0.636  loss_rpn_cls: 0.3604  loss_rpn_loc: 0.1904  time: 1.2445  data_time: 0.4406  lr: 0.0050949  max_mem: 6596M
[10/31 14:33:58] d2.utils.events INFO:  eta: 1 day, 1:37:28  iter: 359  total_loss: 1.935  loss_cls: 0.5698  loss_box_reg: 0.1889  loss_mask: 0.638  loss_rpn_cls: 0.3751  loss_rpn_loc: 0.1605  time: 1.2381  data_time: 0.3869  lr: 0.0053946  max_mem: 6596M
[10/31 14:34:24] d2.utils.events INFO:  eta: 1 day, 1:36:28  iter: 379  total_loss: 1.906  loss_cls: 0.5354  loss_box_reg: 0.1857  loss_mask: 0.6393  loss_rpn_cls: 0.3644  loss_rpn_loc: 0.1856  time: 1.2420  data_time: 0.5587  lr: 0.0056943  max_mem: 6596M
[10/31 14:34:49] d2.utils.events INFO:  eta: 1 day, 1:36:08  iter: 399  total_loss: 1.89  loss_cls: 0.555  loss_box_reg: 0.2103  loss_mask: 0.6202  loss_rpn_cls: 0.3576  loss_rpn_loc: 0.171  time: 1.2431  data_time: 0.4853  lr: 0.005994  max_mem: 6596M
[10/31 14:35:16] d2.utils.events INFO:  eta: 1 day, 1:36:26  iter: 419  total_loss: 1.858  loss_cls: 0.5028  loss_box_reg: 0.1774  loss_mask: 0.6198  loss_rpn_cls: 0.3709  loss_rpn_loc: 0.1587  time: 1.2467  data_time: 0.5357  lr: 0.0062937  max_mem: 6596M
[10/31 14:35:38] d2.utils.events INFO:  eta: 1 day, 1:32:49  iter: 439  total_loss: 1.81  loss_cls: 0.5084  loss_box_reg: 0.1967  loss_mask: 0.6263  loss_rpn_cls: 0.3035  loss_rpn_loc: 0.1271  time: 1.2396  data_time: 0.2989  lr: 0.0065934  max_mem: 6596M
[10/31 14:36:03] d2.utils.events INFO:  eta: 1 day, 1:33:17  iter: 459  total_loss: 1.877  loss_cls: 0.5107  loss_box_reg: 0.1983  loss_mask: 0.6205  loss_rpn_cls: 0.3397  loss_rpn_loc: 0.1762  time: 1.2415  data_time: 0.5480  lr: 0.0068931  max_mem: 6596M
[10/31 14:36:34] d2.utils.events INFO:  eta: 1 day, 1:33:58  iter: 479  total_loss: 1.848  loss_cls: 0.4884  loss_box_reg: 0.1972  loss_mask: 0.6152  loss_rpn_cls: 0.3986  loss_rpn_loc: 0.1474  time: 1.2508  data_time: 0.7226  lr: 0.0071928  max_mem: 6596M
[10/31 14:36:57] d2.utils.events INFO:  eta: 1 day, 1:32:36  iter: 499  total_loss: 1.724  loss_cls: 0.4456  loss_box_reg: 0.1696  loss_mask: 0.6121  loss_rpn_cls: 0.3417  loss_rpn_loc: 0.1531  time: 1.2465  data_time: 0.4062  lr: 0.0074925  max_mem: 6596M
[10/31 14:37:23] d2.utils.events INFO:  eta: 1 day, 1:33:17  iter: 519  total_loss: 1.814  loss_cls: 0.4916  loss_box_reg: 0.2011  loss_mask: 0.6227  loss_rpn_cls: 0.3618  loss_rpn_loc: 0.1558  time: 1.2483  data_time: 0.5690  lr: 0.0077922  max_mem: 6596M
[10/31 14:37:43] d2.utils.events INFO:  eta: 1 day, 1:29:45  iter: 539  total_loss: 1.701  loss_cls: 0.4451  loss_box_reg: 0.1748  loss_mask: 0.6178  loss_rpn_cls: 0.3421  loss_rpn_loc: 0.1467  time: 1.2391  data_time: 0.2337  lr: 0.0080919  max_mem: 6596M
[10/31 14:38:07] d2.utils.events INFO:  eta: 1 day, 1:29:47  iter: 559  total_loss: 1.949  loss_cls: 0.5207  loss_box_reg: 0.225  loss_mask: 0.6162  loss_rpn_cls: 0.3951  loss_rpn_loc: 0.1756  time: 1.2384  data_time: 0.4189  lr: 0.0083916  max_mem: 6596M
[10/31 14:38:31] d2.utils.events INFO:  eta: 1 day, 1:27:32  iter: 579  total_loss: 1.879  loss_cls: 0.4674  loss_box_reg: 0.2025  loss_mask: 0.6152  loss_rpn_cls: 0.3815  loss_rpn_loc: 0.1827  time: 1.2359  data_time: 0.3682  lr: 0.0086913  max_mem: 6596M
[10/31 14:39:00] d2.utils.events INFO:  eta: 1 day, 1:27:12  iter: 599  total_loss: 1.789  loss_cls: 0.446  loss_box_reg: 0.1729  loss_mask: 0.6203  loss_rpn_cls: 0.377  loss_rpn_loc: 0.1704  time: 1.2429  data_time: 0.7376  lr: 0.008991  max_mem: 6596M
[10/31 14:39:26] d2.utils.events INFO:  eta: 1 day, 1:26:51  iter: 619  total_loss: 1.873  loss_cls: 0.4501  loss_box_reg: 0.1903  loss_mask: 0.6456  loss_rpn_cls: 0.403  loss_rpn_loc: 0.1662  time: 1.2448  data_time: 0.5754  lr: 0.0092907  max_mem: 6596M
[10/31 14:39:50] d2.utils.events INFO:  eta: 1 day, 1:28:02  iter: 639  total_loss: 1.768  loss_cls: 0.4384  loss_box_reg: 0.1747  loss_mask: 0.6357  loss_rpn_cls: 0.3647  loss_rpn_loc: 0.1376  time: 1.2425  data_time: 0.4507  lr: 0.0095904  max_mem: 6596M
[10/31 14:40:18] d2.utils.events INFO:  eta: 1 day, 1:29:19  iter: 659  total_loss: 1.749  loss_cls: 0.4318  loss_box_reg: 0.1767  loss_mask: 0.6258  loss_rpn_cls: 0.3589  loss_rpn_loc: 0.1331  time: 1.2456  data_time: 0.6705  lr: 0.0098901  max_mem: 6596M
[10/31 14:40:43] d2.utils.events INFO:  eta: 1 day, 1:31:43  iter: 679  total_loss: 1.682  loss_cls: 0.4364  loss_box_reg: 0.1818  loss_mask: 0.62  loss_rpn_cls: 0.3521  loss_rpn_loc: 0.1155  time: 1.2452  data_time: 0.4951  lr: 0.01019  max_mem: 6596M
[10/31 14:41:08] d2.utils.events INFO:  eta: 1 day, 1:29:51  iter: 699  total_loss: 1.752  loss_cls: 0.4529  loss_box_reg: 0.1922  loss_mask: 0.6155  loss_rpn_cls: 0.3509  loss_rpn_loc: 0.1342  time: 1.2436  data_time: 0.4557  lr: 0.01049  max_mem: 6596M
[10/31 14:41:34] d2.utils.events INFO:  eta: 1 day, 1:28:50  iter: 719  total_loss: 1.691  loss_cls: 0.422  loss_box_reg: 0.178  loss_mask: 0.6127  loss_rpn_cls: 0.3434  loss_rpn_loc: 0.1259  time: 1.2459  data_time: 0.5775  lr: 0.010789  max_mem: 6596M
[10/31 14:42:00] d2.utils.events INFO:  eta: 1 day, 1:29:31  iter: 739  total_loss: 1.771  loss_cls: 0.4449  loss_box_reg: 0.1777  loss_mask: 0.6054  loss_rpn_cls: 0.3536  loss_rpn_loc: 0.1398  time: 1.2466  data_time: 0.5039  lr: 0.011089  max_mem: 6596M
[10/31 14:42:22] d2.utils.events INFO:  eta: 1 day, 1:28:09  iter: 759  total_loss: 1.74  loss_cls: 0.4349  loss_box_reg: 0.1811  loss_mask: 0.6151  loss_rpn_cls: 0.3503  loss_rpn_loc: 0.1449  time: 1.2438  data_time: 0.3526  lr: 0.011389  max_mem: 6596M
[10/31 14:42:46] d2.utils.events INFO:  eta: 1 day, 1:26:23  iter: 779  total_loss: 1.668  loss_cls: 0.4367  loss_box_reg: 0.1843  loss_mask: 0.6007  loss_rpn_cls: 0.3346  loss_rpn_loc: 0.1293  time: 1.2425  data_time: 0.4500  lr: 0.011688  max_mem: 6596M
[10/31 14:43:12] d2.utils.events INFO:  eta: 1 day, 1:26:55  iter: 799  total_loss: 1.77  loss_cls: 0.4443  loss_box_reg: 0.1808  loss_mask: 0.6343  loss_rpn_cls: 0.3586  loss_rpn_loc: 0.1579  time: 1.2430  data_time: 0.5411  lr: 0.011988  max_mem: 6596M
[10/31 14:43:36] d2.utils.events INFO:  eta: 1 day, 1:26:16  iter: 819  total_loss: 1.813  loss_cls: 0.4795  loss_box_reg: 0.2052  loss_mask: 0.6141  loss_rpn_cls: 0.3377  loss_rpn_loc: 0.1691  time: 1.2426  data_time: 0.5072  lr: 0.012288  max_mem: 6596M
[10/31 14:44:03] d2.utils.events INFO:  eta: 1 day, 1:27:48  iter: 839  total_loss: 1.681  loss_cls: 0.4075  loss_box_reg: 0.1578  loss_mask: 0.6165  loss_rpn_cls: 0.371  loss_rpn_loc: 0.1734  time: 1.2448  data_time: 0.6376  lr: 0.012587  max_mem: 6596M
[10/31 14:44:25] d2.utils.events INFO:  eta: 1 day, 1:25:35  iter: 859  total_loss: 1.447  loss_cls: 0.1649  loss_box_reg: 0.0004114  loss_mask: 0.706  loss_rpn_cls: 0.4198  loss_rpn_loc: 0.1577  time: 1.2406  data_time: 0.4799  lr: 0.012887  max_mem: 6596M
[10/31 14:44:46] d2.utils.events INFO:  eta: 1 day, 1:24:18  iter: 879  total_loss: 1.376  loss_cls: 0.1466  loss_box_reg: 1.64e-05  loss_mask: 0.6761  loss_rpn_cls: 0.3966  loss_rpn_loc: 0.1317  time: 1.2359  data_time: 0.4695  lr: 0.013187  max_mem: 6596M
[10/31 14:45:15] d2.utils.events INFO:  eta: 1 day, 1:24:54  iter: 899  total_loss: 1.345  loss_cls: 0.1418  loss_box_reg: 1.256e-05  loss_mask: 0.6933  loss_rpn_cls: 0.3845  loss_rpn_loc: 0.1391  time: 1.2400  data_time: 0.8717  lr: 0.013487  max_mem: 6596M
[10/31 14:45:36] d2.utils.events INFO:  eta: 1 day, 1:22:07  iter: 919  total_loss: 1.413  loss_cls: 0.1434  loss_box_reg: 1.888e-05  loss_mask: 0.6909  loss_rpn_cls: 0.4181  loss_rpn_loc: 0.1547  time: 1.2361  data_time: 0.4974  lr: 0.013786  max_mem: 6596M
[10/31 14:45:56] d2.utils.events INFO:  eta: 1 day, 1:18:47  iter: 939  total_loss: 1.36  loss_cls: 0.1377  loss_box_reg: 1.364e-05  loss_mask: 0.6897  loss_rpn_cls: 0.3946  loss_rpn_loc: 0.1286  time: 1.2308  data_time: 0.4071  lr: 0.014086  max_mem: 6596M
[10/31 14:46:18] d2.utils.events INFO:  eta: 1 day, 1:17:04  iter: 959  total_loss: 1.383  loss_cls: 0.1453  loss_box_reg: 1.42e-05  loss_mask: 0.6895  loss_rpn_cls: 0.4002  loss_rpn_loc: 0.1295  time: 1.2283  data_time: 0.5308  lr: 0.014386  max_mem: 6596M
[10/31 14:46:43] d2.utils.events INFO:  eta: 1 day, 1:15:44  iter: 979  total_loss: 1.361  loss_cls: 0.1336  loss_box_reg: 1.274e-05  loss_mask: 0.6877  loss_rpn_cls: 0.3926  loss_rpn_loc: 0.1451  time: 1.2288  data_time: 0.6942  lr: 0.014685  max_mem: 6596M
[10/31 14:47:07] d2.utils.events INFO:  eta: 1 day, 1:15:23  iter: 999  total_loss: 1.371  loss_cls: 0.1401  loss_box_reg: 1.315e-05  loss_mask: 0.6874  loss_rpn_cls: 0.4047  loss_rpn_loc: 0.1413  time: 1.2283  data_time: 0.6323  lr: 0.014985  max_mem: 6596M
[10/31 14:47:31] d2.utils.events INFO:  eta: 1 day, 1:15:03  iter: 1019  total_loss: 1.386  loss_cls: 0.1481  loss_box_reg: 1.273e-05  loss_mask: 0.6858  loss_rpn_cls: 0.4063  loss_rpn_loc: 0.1549  time: 1.2274  data_time: 0.6281  lr: 0.015  max_mem: 6596M
[10/31 14:47:52] d2.utils.events INFO:  eta: 1 day, 1:13:40  iter: 1039  total_loss: 1.393  loss_cls: 0.1409  loss_box_reg: 8.152e-06  loss_mask: 0.6842  loss_rpn_cls: 0.4074  loss_rpn_loc: 0.1453  time: 1.2242  data_time: 0.4921  lr: 0.015  max_mem: 6596M
[10/31 14:48:21] d2.utils.events INFO:  eta: 1 day, 1:12:22  iter: 1059  total_loss: 1.536  loss_cls: 0.1992  loss_box_reg: 0.01141  loss_mask: 0.682  loss_rpn_cls: 0.3562  loss_rpn_loc: 0.1431  time: 1.2279  data_time: 0.7523  lr: 0.015  max_mem: 6596M
[10/31 14:48:50] d2.utils.events INFO:  eta: 1 day, 1:12:21  iter: 1079  total_loss: 1.692  loss_cls: 0.3875  loss_box_reg: 0.1533  loss_mask: 0.6769  loss_rpn_cls: 0.3361  loss_rpn_loc: 0.1404  time: 1.2316  data_time: 0.6994  lr: 0.015  max_mem: 6596M
[10/31 14:49:18] d2.utils.events INFO:  eta: 1 day, 1:13:10  iter: 1099  total_loss: 1.725  loss_cls: 0.4078  loss_box_reg: 0.1644  loss_mask: 0.6762  loss_rpn_cls: 0.3502  loss_rpn_loc: 0.1403  time: 1.2339  data_time: 0.6492  lr: 0.015  max_mem: 6596M
[10/31 14:49:37] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 134, in train
    self.run_step()
  File "/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 423, in run_step
    self._trainer.run_step()
  File "/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 228, in run_step
    loss_dict = self.model(data)
  File "/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 511, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 160, in forward
    proposals, proposal_losses = self.proposal_generator(images, features, gt_instances)
  File "/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/modeling/proposal_generator/rpn.py", line 448, in forward
    proposals = self.predict_proposals(
  File "/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/modeling/proposal_generator/rpn.py", line 474, in predict_proposals
    return find_top_rpn_proposals(
  File "/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/modeling/proposal_generator/proposal_utils.py", line 91, in find_top_rpn_proposals
    raise FloatingPointError(
FloatingPointError: Predicted boxes or scores contain Inf/NaN. Training has diverged.
[10/31 14:49:38] d2.engine.hooks INFO: Overall training speed: 1113 iterations in 0:22:54 (1.2347 s / it)
[10/31 14:49:38] d2.engine.hooks INFO: Total training time: 0:23:08 (0:00:13 on hooks)
[10/31 14:49:38] d2.utils.events INFO:  eta: 1 day, 1:12:53  iter: 1115  total_loss: 1.69  loss_cls: 0.383  loss_box_reg: 0.1497  loss_mask: 0.6794  loss_rpn_cls: 0.3158  loss_rpn_loc: 0.1374  time: 1.2341  data_time: 0.6132  lr: 0.015  max_mem: 6596M
[10/31 15:52:04] detectron2 INFO: Rank of current process: 0. World size: 2
[10/31 15:52:06] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.3 (default, May 19 2020, 18:47:26) [GCC 7.3.0]
numpy                   1.19.1
detectron2              0.2.1 @/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 5.4
CUDA compiler           not available
detectron2 arch flags   /cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/_C.cpython-38-x86_64-linux-gnu.so; cannot find cuobjdump
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.6.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           True
GPU 0,1                 GeForce GTX 1080 Ti (arch=6.1)
CUDA_HOME               /opt/common/cuda/cuda-10.0.130/lib64/
Pillow                  7.2.0
torchvision             0.7.0 @/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision
torchvision arch flags  /cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                  0.1.2.post20201030
cv2                     4.4.0
----------------------  --------------------------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

[10/31 15:52:06] detectron2 INFO: Command line arguments: Namespace(config_file='configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml', dist_url='tcp://127.0.0.1:63010', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=['MODEL.MASK_ON', 'True', 'SOLVER.BASE_LR', '0.015', 'OUTPUT_DIR', 'temp'], resume=False)
[10/31 15:52:06] detectron2 INFO: Contents of args.config_file=configs/COCO-InstanceSegmentation/mask_rcnn_r18_FPN_1x.yaml:
_BASE_: "../Base-RCNN-FPN.yaml"
MODEL:
  WEIGHTS: "https://download.pytorch.org/models/resnet18-5c106cde.pth"
  MASK_ON: True
  RESNETS:
    DEPTH: 18
    RES2_OUT_CHANNELS: 64
# DATASETS:
#   TRAIN: ("coco_2017_val",)
#   TEST: ("coco_2017_val",) #("coco_2017_test-dev",)
SOLVER:
  BASE_LR: 0.02
  CHECKPOINT_PERIOD: 7330
  # IMS_PER_BATCH: 24
  # STEPS: (40000, 53500)
  # MAX_ITER: 60000
  # WARMUP_FACTOR: 5e-4
  # WARMUP_ITERS: 2000
OUTPUT_DIR: output/faster_r18_fpn

[10/31 15:52:06] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  FILTER_EMPTY_ANNOTATIONS: True
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: ()
  PROPOSAL_FILES_TRAIN: ()
  TEST: ('coco_2017_val',)
  TRAIN: ('coco_2017_train',)
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: False
    SIZE: [0.9, 0.9]
    TYPE: relative_range
  FORMAT: BGR
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES: [[-90, 0, 90]]
    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES: [[32], [64], [128], [256], [512]]
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_fpn_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']
    NORM: 
    OUT_CHANNELS: 256
  KEYPOINT_ON: False
  LOAD_PROPOSALS: False
  MASK_ON: True
  META_ARCHITECTURE: GeneralizedRCNN
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: True
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN: [103.53, 116.28, 123.675]
  PIXEL_STD: [1.0, 1.0, 1.0]
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: False
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE: [False, False, False, False]
    DEPTH: 18
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']
    RES2_OUT_CHANNELS: 64
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.4, 0.5]
    NMS_THRESH_TEST: 0.5
    NORM: 
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))
    IOUS: (0.5, 0.6, 0.7)
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    CLS_AGNOSTIC_BBOX_REG: False
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: FastRCNNConvFCHead
    NORM: 
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    IOU_LABELS: [0, 1]
    IOU_THRESHOLDS: [0.5]
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: True
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: False
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: 
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    BOUNDARY_THRESH: -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.3, 0.7]
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: https://download.pytorch.org/models/resnet18-5c106cde.pth
OUTPUT_DIR: temp
SEED: -1
SOLVER:
  AMP:
    ENABLED: False
  BASE_LR: 0.015
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 7330
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: False
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 16
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 90000
  MOMENTUM: 0.9
  NESTEROV: False
  REFERENCE_WORLD_SIZE: 0
  STEPS: (60000, 80000)
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: False
    FLIP: True
    MAX_SIZE: 4000
    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 0
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: False
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0
[10/31 15:52:07] detectron2 INFO: Full config saved to temp/config.yaml
[10/31 15:52:07] d2.utils.env INFO: Using a generated random seed 7959115
[10/31 15:52:08] d2.engine.defaults INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BasicBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten()
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[10/31 15:52:31] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 22.71 seconds.
[10/31 15:52:32] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[10/31 15:52:39] d2.data.build INFO: Removed 1021 images with no usable annotations. 117266 images left.
[10/31 15:52:45] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[10/31 15:52:45] d2.data.common INFO: Serializing 117266 elements to byte tensors and concatenating them all ...
[10/31 15:52:49] d2.data.common INFO: Serialized dataset takes 451.21 MiB
[10/31 15:52:49] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[10/31 15:52:49] d2.data.build INFO: Using training sampler TrainingSampler
[10/31 15:52:51] fvcore.common.checkpoint INFO: Loading checkpoint from https://download.pytorch.org/models/resnet18-5c106cde.pth
[10/31 15:52:51] fvcore.common.file_io INFO: URL https://download.pytorch.org/models/resnet18-5c106cde.pth cached in /cfarhomes/shishira/.torch/fvcore_cache/models/resnet18-5c106cde.pth
[10/31 15:52:51] fvcore.common.checkpoint INFO: Some model parameters or buffers are not found in the checkpoint:
  [34mbackbone.bottom_up.res3.0.shortcut.norm.{weight, bias}[0m
  [34mbackbone.fpn_lateral3.{bias, weight}[0m
  [34mproposal_generator.rpn_head.objectness_logits.{weight, bias}[0m
  [34mroi_heads.box_head.fc1.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.0.conv2.weight[0m
  [34mproposal_generator.anchor_generator.cell_anchors.{0, 2, 4, 1, 3}[0m
  [34mbackbone.bottom_up.res5.0.conv2.norm.{weight, bias}[0m
  [34mbackbone.fpn_lateral5.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.shortcut.norm.{weight, bias}[0m
  [34mbackbone.fpn_output2.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn1.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res3.1.conv2.weight[0m
  [34mbackbone.bottom_up.res4.0.shortcut.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.1.conv1.weight[0m
  [34mbackbone.bottom_up.res3.1.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.1.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.1.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.0.conv2.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res2.0.conv1.weight[0m
  [34mbackbone.bottom_up.stem.conv1.norm.{bias, weight}[0m
  [34mroi_heads.mask_head.mask_fcn2.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.1.conv2.norm.{weight, bias}[0m
  [34mroi_heads.box_predictor.cls_score.{weight, bias}[0m
  [34mroi_heads.box_head.fc2.{weight, bias}[0m
  [34mroi_heads.mask_head.mask_fcn3.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv1.norm.{weight, bias}[0m
  [34mbackbone.fpn_output5.{weight, bias}[0m
  [34mbackbone.bottom_up.res5.0.conv2.weight[0m
  [34mbackbone.bottom_up.res2.1.conv2.weight[0m
  [34mbackbone.bottom_up.res3.0.conv2.norm.{weight, bias}[0m
  [34mroi_heads.mask_head.mask_fcn4.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.0.conv2.weight[0m
  [34mbackbone.bottom_up.res2.1.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.conv1.norm.{weight, bias}[0m
  [34mroi_heads.mask_head.predictor.{weight, bias}[0m
  [34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
  [34mroi_heads.mask_head.deconv.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.1.conv1.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.1.conv2.weight[0m
  [34mbackbone.bottom_up.res4.0.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.1.conv2.weight[0m
  [34mbackbone.fpn_lateral2.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.1.conv2.norm.{bias, weight}[0m
  [34mbackbone.bottom_up.res4.0.conv1.weight[0m
  [34mbackbone.bottom_up.stem.conv1.weight[0m
  [34mbackbone.bottom_up.res3.0.shortcut.weight[0m
  [34mroi_heads.box_predictor.bbox_pred.{weight, bias}[0m
  [34mbackbone.bottom_up.res3.0.conv1.weight[0m
  [34mbackbone.fpn_output4.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.conv1.norm.{weight, bias}[0m
  [34mbackbone.bottom_up.res4.0.shortcut.weight[0m
  [34mproposal_generator.rpn_head.conv.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.conv1.weight[0m
  [34mbackbone.fpn_lateral4.{bias, weight}[0m
  [34mbackbone.bottom_up.res2.1.conv1.weight[0m
  [34mbackbone.bottom_up.res4.0.conv2.weight[0m
  [34mbackbone.fpn_output3.{bias, weight}[0m
  [34mbackbone.bottom_up.res5.0.shortcut.weight[0m
  [34mbackbone.bottom_up.res3.1.conv1.weight[0m
  [34mbackbone.bottom_up.res4.1.conv1.weight[0m
[10/31 15:52:51] fvcore.common.checkpoint INFO: The checkpoint state_dict contains keys that are not used by the model:
  [35mconv1.weight[0m
  [35mbn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv1.weight[0m
  [35mlayer1.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.0.conv2.weight[0m
  [35mlayer1.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv1.weight[0m
  [35mlayer1.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer1.1.conv2.weight[0m
  [35mlayer1.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv1.weight[0m
  [35mlayer2.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.conv2.weight[0m
  [35mlayer2.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.0.downsample.0.weight[0m
  [35mlayer2.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv1.weight[0m
  [35mlayer2.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer2.1.conv2.weight[0m
  [35mlayer2.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv1.weight[0m
  [35mlayer3.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.conv2.weight[0m
  [35mlayer3.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.0.downsample.0.weight[0m
  [35mlayer3.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv1.weight[0m
  [35mlayer3.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer3.1.conv2.weight[0m
  [35mlayer3.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv1.weight[0m
  [35mlayer4.0.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.conv2.weight[0m
  [35mlayer4.0.bn2.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.0.downsample.0.weight[0m
  [35mlayer4.0.downsample.1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv1.weight[0m
  [35mlayer4.1.bn1.{running_mean, running_var, weight, bias}[0m
  [35mlayer4.1.conv2.weight[0m
  [35mlayer4.1.bn2.{running_mean, running_var, weight, bias}[0m
  [35mfc.{weight, bias}[0m
[10/31 15:52:54] d2.engine.train_loop INFO: Starting training from iteration 0
[10/31 15:53:46] d2.utils.events INFO:  eta: 1 day, 16:44:02  iter: 19  total_loss: 6.041  loss_cls: 4.391  loss_box_reg: 0.1112  loss_mask: 0.6911  loss_rpn_cls: 0.6854  loss_rpn_loc: 0.1566  time: 1.7537  data_time: 1.9132  lr: 0.00029971  max_mem: 6593M
[10/31 15:54:21] d2.utils.events INFO:  eta: 1 day, 10:05:16  iter: 39  total_loss: 5.755  loss_cls: 4.343  loss_box_reg: 0.1384  loss_mask: 0.6817  loss_rpn_cls: 0.4735  loss_rpn_loc: 0.121  time: 1.7301  data_time: 1.0073  lr: 0.00059941  max_mem: 6593M
[10/31 15:54:52] d2.utils.events INFO:  eta: 1 day, 8:21:29  iter: 59  total_loss: 5.715  loss_cls: 4.238  loss_box_reg: 0.1606  loss_mask: 0.677  loss_rpn_cls: 0.4683  loss_rpn_loc: 0.136  time: 1.6703  data_time: 0.8329  lr: 0.00089911  max_mem: 6593M
[10/31 15:55:34] d2.utils.events INFO:  eta: 1 day, 9:32:32  iter: 79  total_loss: 5.523  loss_cls: 4.078  loss_box_reg: 0.1904  loss_mask: 0.6719  loss_rpn_cls: 0.4274  loss_rpn_loc: 0.1388  time: 1.7786  data_time: 1.3670  lr: 0.0011988  max_mem: 6593M
[10/31 15:56:27] d2.utils.events INFO:  eta: 1 day, 11:01:01  iter: 99  total_loss: 5.313  loss_cls: 3.873  loss_box_reg: 0.2172  loss_mask: 0.6665  loss_rpn_cls: 0.4368  loss_rpn_loc: 0.1429  time: 1.9335  data_time: 1.8267  lr: 0.0014985  max_mem: 6593M
[10/31 15:57:13] d2.utils.events INFO:  eta: 1 day, 11:55:39  iter: 119  total_loss: 5.032  loss_cls: 3.609  loss_box_reg: 0.1819  loss_mask: 0.6649  loss_rpn_cls: 0.3901  loss_rpn_loc: 0.1435  time: 1.9857  data_time: 1.5711  lr: 0.0017982  max_mem: 6593M
[10/31 15:58:04] d2.utils.events INFO:  eta: 1 day, 12:21:04  iter: 139  total_loss: 4.777  loss_cls: 3.326  loss_box_reg: 0.2028  loss_mask: 0.6626  loss_rpn_cls: 0.4203  loss_rpn_loc: 0.1493  time: 2.0520  data_time: 1.8233  lr: 0.0020979  max_mem: 6593M
[10/31 15:58:46] d2.utils.events INFO:  eta: 1 day, 11:54:41  iter: 159  total_loss: 4.356  loss_cls: 2.983  loss_box_reg: 0.1889  loss_mask: 0.6609  loss_rpn_cls: 0.375  loss_rpn_loc: 0.1586  time: 2.0566  data_time: 1.3849  lr: 0.0023976  max_mem: 6593M
[10/31 15:59:22] d2.utils.events INFO:  eta: 1 day, 12:20:06  iter: 179  total_loss: 3.977  loss_cls: 2.603  loss_box_reg: 0.199  loss_mask: 0.6498  loss_rpn_cls: 0.3763  loss_rpn_loc: 0.1414  time: 2.0205  data_time: 1.0650  lr: 0.0026973  max_mem: 6593M
[10/31 15:59:51] d2.utils.events INFO:  eta: 1 day, 11:15:28  iter: 199  total_loss: 3.656  loss_cls: 2.228  loss_box_reg: 0.2052  loss_mask: 0.6498  loss_rpn_cls: 0.3873  loss_rpn_loc: 0.1532  time: 1.9588  data_time: 0.6928  lr: 0.002997  max_mem: 6593M
[10/31 16:00:26] d2.utils.events INFO:  eta: 1 day, 10:58:13  iter: 219  total_loss: 3.222  loss_cls: 1.851  loss_box_reg: 0.2012  loss_mask: 0.65  loss_rpn_cls: 0.3098  loss_rpn_loc: 0.1426  time: 1.9385  data_time: 1.0184  lr: 0.0032967  max_mem: 6593M
[10/31 16:00:50] d2.utils.events INFO:  eta: 1 day, 9:37:39  iter: 239  total_loss: 2.849  loss_cls: 1.482  loss_box_reg: 0.1817  loss_mask: 0.6414  loss_rpn_cls: 0.3513  loss_rpn_loc: 0.1694  time: 1.8763  data_time: 0.4816  lr: 0.0035964  max_mem: 6593M
[10/31 16:01:18] d2.utils.events INFO:  eta: 1 day, 9:12:03  iter: 259  total_loss: 2.456  loss_cls: 1.166  loss_box_reg: 0.1874  loss_mask: 0.6466  loss_rpn_cls: 0.3315  loss_rpn_loc: 0.1456  time: 1.8395  data_time: 0.7265  lr: 0.0038961  max_mem: 6593M
[10/31 16:01:45] d2.utils.events INFO:  eta: 1 day, 9:05:22  iter: 279  total_loss: 2.165  loss_cls: 0.92  loss_box_reg: 0.1644  loss_mask: 0.6392  loss_rpn_cls: 0.3596  loss_rpn_loc: 0.12  time: 1.8033  data_time: 0.6693  lr: 0.0041958  max_mem: 6593M
[10/31 16:02:20] d2.utils.events INFO:  eta: 1 day, 9:11:10  iter: 299  total_loss: 2.223  loss_cls: 0.8485  loss_box_reg: 0.2234  loss_mask: 0.64  loss_rpn_cls: 0.309  loss_rpn_loc: 0.1517  time: 1.7997  data_time: 1.0366  lr: 0.0044955  max_mem: 6593M
[10/31 16:02:50] d2.utils.events INFO:  eta: 1 day, 8:01:32  iter: 319  total_loss: 2.057  loss_cls: 0.6997  loss_box_reg: 0.192  loss_mask: 0.6279  loss_rpn_cls: 0.3579  loss_rpn_loc: 0.1474  time: 1.7789  data_time: 0.7519  lr: 0.0047952  max_mem: 6593M
[10/31 16:03:19] d2.utils.events INFO:  eta: 1 day, 6:59:24  iter: 339  total_loss: 1.921  loss_cls: 0.6227  loss_box_reg: 0.1811  loss_mask: 0.6317  loss_rpn_cls: 0.3588  loss_rpn_loc: 0.1588  time: 1.7578  data_time: 0.7030  lr: 0.0050949  max_mem: 6593M
[10/31 16:03:52] d2.utils.events INFO:  eta: 1 day, 6:58:59  iter: 359  total_loss: 1.996  loss_cls: 0.5935  loss_box_reg: 0.1983  loss_mask: 0.638  loss_rpn_cls: 0.3445  loss_rpn_loc: 0.1734  time: 1.7539  data_time: 0.9613  lr: 0.0053946  max_mem: 6593M
[10/31 16:04:28] d2.utils.events INFO:  eta: 1 day, 6:25:16  iter: 379  total_loss: 1.967  loss_cls: 0.5622  loss_box_reg: 0.1897  loss_mask: 0.6338  loss_rpn_cls: 0.4107  loss_rpn_loc: 0.1803  time: 1.7507  data_time: 1.0256  lr: 0.0056943  max_mem: 6593M
[10/31 16:04:53] d2.utils.events INFO:  eta: 1 day, 6:01:46  iter: 399  total_loss: 1.993  loss_cls: 0.5614  loss_box_reg: 0.2088  loss_mask: 0.6291  loss_rpn_cls: 0.3995  loss_rpn_loc: 0.1569  time: 1.7256  data_time: 0.5107  lr: 0.005994  max_mem: 6593M
[10/31 16:05:25] d2.utils.events INFO:  eta: 1 day, 6:11:53  iter: 419  total_loss: 1.922  loss_cls: 0.525  loss_box_reg: 0.1912  loss_mask: 0.6337  loss_rpn_cls: 0.3854  loss_rpn_loc: 0.1699  time: 1.7193  data_time: 0.8510  lr: 0.0062937  max_mem: 6593M
[10/31 16:05:51] d2.utils.events INFO:  eta: 1 day, 5:52:18  iter: 439  total_loss: 1.866  loss_cls: 0.4935  loss_box_reg: 0.1984  loss_mask: 0.6267  loss_rpn_cls: 0.3733  loss_rpn_loc: 0.1625  time: 1.7002  data_time: 0.6132  lr: 0.0065934  max_mem: 6593M
[10/31 16:06:17] d2.utils.events INFO:  eta: 1 day, 5:08:02  iter: 459  total_loss: 1.879  loss_cls: 0.5061  loss_box_reg: 0.207  loss_mask: 0.6286  loss_rpn_cls: 0.398  loss_rpn_loc: 0.1749  time: 1.6817  data_time: 0.5404  lr: 0.0068931  max_mem: 6593M
[10/31 16:06:50] d2.utils.events INFO:  eta: 1 day, 5:35:04  iter: 479  total_loss: 1.975  loss_cls: 0.4965  loss_box_reg: 0.1984  loss_mask: 0.6301  loss_rpn_cls: 0.4317  loss_rpn_loc: 0.1559  time: 1.6793  data_time: 0.8999  lr: 0.0071928  max_mem: 6593M
[10/31 16:07:17] d2.utils.events INFO:  eta: 1 day, 5:07:15  iter: 499  total_loss: 1.853  loss_cls: 0.4403  loss_box_reg: 0.1726  loss_mask: 0.6372  loss_rpn_cls: 0.4591  loss_rpn_loc: 0.1351  time: 1.6654  data_time: 0.6581  lr: 0.0074925  max_mem: 6593M
[10/31 16:07:43] d2.utils.events INFO:  eta: 1 day, 4:47:22  iter: 519  total_loss: 2.03  loss_cls: 0.4646  loss_box_reg: 0.1893  loss_mask: 0.6579  loss_rpn_cls: 0.5164  loss_rpn_loc: 0.1522  time: 1.6514  data_time: 0.5512  lr: 0.0077922  max_mem: 6593M
[10/31 16:08:11] d2.utils.events INFO:  eta: 1 day, 4:46:59  iter: 539  total_loss: 1.882  loss_cls: 0.4416  loss_box_reg: 0.1728  loss_mask: 0.6384  loss_rpn_cls: 0.47  loss_rpn_loc: 0.1645  time: 1.6434  data_time: 0.7293  lr: 0.0080919  max_mem: 6593M
[10/31 16:08:42] d2.utils.events INFO:  eta: 1 day, 4:54:54  iter: 559  total_loss: 1.772  loss_cls: 0.4314  loss_box_reg: 0.1758  loss_mask: 0.6306  loss_rpn_cls: 0.4215  loss_rpn_loc: 0.1257  time: 1.6375  data_time: 0.7518  lr: 0.0083916  max_mem: 6593M
[10/31 16:09:05] d2.utils.events INFO:  eta: 1 day, 4:14:48  iter: 579  total_loss: 1.815  loss_cls: 0.4357  loss_box_reg: 0.1646  loss_mask: 0.6236  loss_rpn_cls: 0.4135  loss_rpn_loc: 0.1525  time: 1.6194  data_time: 0.3541  lr: 0.0086913  max_mem: 6593M
[10/31 16:09:26] d2.utils.events INFO:  eta: 1 day, 3:42:40  iter: 599  total_loss: 1.788  loss_cls: 0.4602  loss_box_reg: 0.1732  loss_mask: 0.625  loss_rpn_cls: 0.3838  loss_rpn_loc: 0.1402  time: 1.6012  data_time: 0.3729  lr: 0.008991  max_mem: 6593M
[10/31 16:09:49] d2.utils.events INFO:  eta: 1 day, 3:29:46  iter: 619  total_loss: 1.704  loss_cls: 0.3951  loss_box_reg: 0.1594  loss_mask: 0.6164  loss_rpn_cls: 0.3657  loss_rpn_loc: 0.1353  time: 1.5856  data_time: 0.3996  lr: 0.0092907  max_mem: 6593M
[10/31 16:10:12] d2.utils.events INFO:  eta: 1 day, 3:16:39  iter: 639  total_loss: 1.785  loss_cls: 0.4652  loss_box_reg: 0.1971  loss_mask: 0.6189  loss_rpn_cls: 0.356  loss_rpn_loc: 0.135  time: 1.5710  data_time: 0.3277  lr: 0.0095904  max_mem: 6593M
[10/31 16:10:37] d2.utils.events INFO:  eta: 1 day, 2:44:59  iter: 659  total_loss: 1.73  loss_cls: 0.4478  loss_box_reg: 0.184  loss_mask: 0.6253  loss_rpn_cls: 0.3414  loss_rpn_loc: 0.1263  time: 1.5614  data_time: 0.5483  lr: 0.0098901  max_mem: 6593M
[10/31 16:11:07] d2.utils.events INFO:  eta: 1 day, 2:40:25  iter: 679  total_loss: 1.777  loss_cls: 0.4711  loss_box_reg: 0.1974  loss_mask: 0.6127  loss_rpn_cls: 0.3483  loss_rpn_loc: 0.1295  time: 1.5604  data_time: 0.7475  lr: 0.01019  max_mem: 6593M
[10/31 16:11:37] d2.utils.events INFO:  eta: 1 day, 2:32:15  iter: 699  total_loss: 1.776  loss_cls: 0.4859  loss_box_reg: 0.2076  loss_mask: 0.609  loss_rpn_cls: 0.3335  loss_rpn_loc: 0.1445  time: 1.5585  data_time: 0.7627  lr: 0.01049  max_mem: 6593M
[10/31 16:12:05] d2.utils.events INFO:  eta: 1 day, 2:24:38  iter: 719  total_loss: 1.787  loss_cls: 0.4673  loss_box_reg: 0.1914  loss_mask: 0.6152  loss_rpn_cls: 0.329  loss_rpn_loc: 0.1382  time: 1.5537  data_time: 0.6183  lr: 0.010789  max_mem: 6593M
[10/31 16:12:32] d2.utils.events INFO:  eta: 1 day, 2:24:17  iter: 739  total_loss: 1.786  loss_cls: 0.4912  loss_box_reg: 0.2154  loss_mask: 0.6156  loss_rpn_cls: 0.3309  loss_rpn_loc: 0.1342  time: 1.5467  data_time: 0.5834  lr: 0.011089  max_mem: 6593M
[10/31 16:13:01] d2.utils.events INFO:  eta: 1 day, 2:20:02  iter: 759  total_loss: 1.833  loss_cls: 0.4913  loss_box_reg: 0.215  loss_mask: 0.6203  loss_rpn_cls: 0.3414  loss_rpn_loc: 0.1409  time: 1.5451  data_time: 0.7512  lr: 0.011389  max_mem: 6593M
[10/31 16:13:29] d2.utils.events INFO:  eta: 1 day, 2:15:11  iter: 779  total_loss: 1.769  loss_cls: 0.4625  loss_box_reg: 0.1882  loss_mask: 0.6119  loss_rpn_cls: 0.341  loss_rpn_loc: 0.1565  time: 1.5407  data_time: 0.6450  lr: 0.011688  max_mem: 6593M
[10/31 16:13:52] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 134, in train
    self.run_step()
  File "/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 423, in run_step
    self._trainer.run_step()
  File "/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 228, in run_step
    loss_dict = self.model(data)
  File "/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 511, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 160, in forward
    proposals, proposal_losses = self.proposal_generator(images, features, gt_instances)
  File "/cfarhomes/shishira/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/modeling/proposal_generator/rpn.py", line 448, in forward
    proposals = self.predict_proposals(
  File "/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/modeling/proposal_generator/rpn.py", line 474, in predict_proposals
    return find_top_rpn_proposals(
  File "/cfarhomes/shishira/.local/lib/python3.8/site-packages/detectron2/modeling/proposal_generator/proposal_utils.py", line 91, in find_top_rpn_proposals
    raise FloatingPointError(
FloatingPointError: Predicted boxes or scores contain Inf/NaN. Training has diverged.
[10/31 16:13:52] d2.engine.hooks INFO: Overall training speed: 795 iterations in 0:20:19 (1.5344 s / it)
[10/31 16:13:52] d2.engine.hooks INFO: Total training time: 0:20:37 (0:00:17 on hooks)
[10/31 16:13:52] d2.utils.events INFO:  eta: 1 day, 1:52:38  iter: 797  total_loss: 1.795  loss_cls: 0.4467  loss_box_reg: 0.1801  loss_mask: 0.6533  loss_rpn_cls: 0.354  loss_rpn_loc: 0.1651  time: 1.5335  data_time: 0.4255  lr: 0.011943  max_mem: 6593M
